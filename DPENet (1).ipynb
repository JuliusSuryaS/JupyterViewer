{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPENet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from time import sleep\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Input and GT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GT format  \n",
    "- ['ego_global_x', 'ego_global_y']  \n",
    "\n",
    "Input format  \n",
    "- ['ego_vx', 'yaw_rate', 'sas_angle', 'long_acc', 'lat_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_folder = './DDPE_GT_adaf_1/'\n",
    "in_folder = './DDPE_input_adaf_1/'\n",
    "gt_file_list = glob.glob(gt_folder+'/*_gt.json')\n",
    "gt_file_list.sort()\n",
    "in_train, gt_train = [], []\n",
    "in_test, gt_test = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e46cfc891c7408cb5b6f80e4a9244ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "folder progress:   0%|          | 0/533745 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the json log.\n",
    "for k in trange(len(gt_file_list),desc='folder progress'):\n",
    "    # Load the GT\n",
    "    file_name_gt = gt_file_list[k]\n",
    "    with open(file_name_gt, 'r', encoding='utf-8-sig') as data_file_gt:\n",
    "        try:\n",
    "            data_gt = json.load(data_file_gt)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Load the input\n",
    "    file_name_in = in_folder + gt_file_list[k][17:26] + '_in.json'\n",
    "    with open(file_name_in, 'r', encoding='utf-8-sig') as data_file_in:\n",
    "        try:\n",
    "            data_in = json.load(data_file_in)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    if len(data_gt['ego_global_y']) < 60:\n",
    "        continue\n",
    "    if data_in['ego_vx'] == 0.0:\n",
    "        continue\n",
    "        \n",
    "    if k % 10 == 0:\n",
    "        gt_test.append([data_gt['ego_global_x'], data_gt['ego_global_y']])\n",
    "        in_test.append([data_in['ego_vx'],data_in['yaw_rate'],data_in['sas_angle'],data_in['long_acc'],data_in['lat_acc']])\n",
    "    else:\n",
    "        gt_train.append([data_gt['ego_global_x'], data_gt['ego_global_y']])\n",
    "        in_train.append([data_in['ego_vx'],data_in['yaw_rate'],data_in['sas_angle'],data_in['long_acc'],data_in['lat_acc']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list to tensor\n",
    "gt_train = torch.tensor(gt_train,requires_grad=False).cuda()\n",
    "in_train = torch.tensor(in_train,requires_grad=False).cuda()\n",
    "gt_test = torch.tensor(gt_test,requires_grad=False).cuda()\n",
    "in_test = torch.tensor(in_test,requires_grad=False).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the input dataset\n",
    "# Vehicle CAN Information on SV Test/Demo Vehicles\n",
    "# https://docs.stradvision.com/pages/viewpage.action?pageId=106747304\n",
    "# ego_vx : 0 to 511.96875 (km/h)\n",
    "# yaw_rate : -40.95 to 40.96(˚/s)\n",
    "# sas_angle : -3276.8 to 3276.7 (deg)\n",
    "# long_acc : -10.23 to 10.24 (m/s^2)\n",
    "# lat_acc : -10.23 to 10.24 (m/s^2)\n",
    "\n",
    "# train dataset max\n",
    "# [120.7656,  24.1400, 305.8000,   4.1700,   4.9700]\n",
    "# train dataset min\n",
    "# [0.0000,  -31.4100, -473.7000,   -7.7000,   -5.3800]\n",
    "# train dataset mean\n",
    "# [57.9791, -0.1554, -0.5714,  0.0685,  0.0752]\n",
    "# train datset std\n",
    "# [35.1523,  2.0590, 18.6299,  0.5983,  0.4319]\n",
    "\n",
    "# normalize\n",
    "# (in - mean) / std\n",
    "in_train = (in_train - torch.mean(in_train,0)) / torch.std(in_train,0)\n",
    "in_test = (in_test - torch.mean(in_train,0)) / torch.std(in_train,0)\n",
    "# range [0 1]\n",
    "# minus min -> divide by max\n",
    "in_train = in_train - torch.min(in_train,0).values\n",
    "in_train = in_train / torch.max(in_train,0).values\n",
    "in_test = in_test - torch.min(in_train,0).values\n",
    "in_test = in_test / torch.max(in_train,0).values\n",
    "\n",
    "# range [-1 1]\n",
    "# minus min -> divide by max/2 -> minus 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to use the dataloader\n",
    "class DPEDataset(data.Dataset):\n",
    "    def __len__(self):\n",
    "        return len(in_train)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        in_tmp = in_train[idx]\n",
    "        gt_tmp = gt_train[idx]\n",
    "        return {'in':in_tmp,'gt':gt_tmp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPETestset(data.Dataset):\n",
    "    def __len__(self):\n",
    "        return len(in_test)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        in_tm = in_test[idx]\n",
    "        gt_tm = gt_test[idx]\n",
    "        return {'in':in_tm,'gt':gt_tm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpe_dataset = DPEDataset()\n",
    "dpe_testset = DPETestset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(425054, 425054, 47231, 47231)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gt_train), len(in_train), len(gt_test), len(in_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(in_train.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize distribution\n",
    "ls_ego_vx, ls_yaw_rate, ls_sas_angle, ls_long_acc, ls_lat_acc = [], [], [], [], []\n",
    "for i in range(len(in_train)):\n",
    "    ls_ego_vx.append(in_train[i][0])\n",
    "    ls_yaw_rate.append(in_train[i][1])\n",
    "    ls_sas_angle.append(in_train[i][2])\n",
    "    ls_long_acc.append(in_train[i][3])\n",
    "    ls_lat_acc.append(in_train[i][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(ls_ego_vx), max(ls_ego_vx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the ego_vx distribution\n",
    "ls_ego_vx_cat = np.arange(121)\n",
    "ls_ego_vx_count = np.zeros(121)\n",
    "for i in range(len(ls_ego_vx)):\n",
    "    k = int(ls_ego_vx[i])\n",
    "    ls_ego_vx_count[k] += 1\n",
    "plt.title('ego_vx')\n",
    "plt.bar(ls_ego_vx_cat, ls_ego_vx_count, width = 0.7, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(ls_yaw_rate), max(ls_yaw_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the yaw_rate distribution\n",
    "# -31 24 -> 56\n",
    "ls_yaw_rate_cat = np.arange(56)\n",
    "ls_yaw_rate_count = np.zeros(56)\n",
    "for i in range(len(ls_yaw_rate)):\n",
    "    k = int(ls_yaw_rate[i])\n",
    "    ls_yaw_rate_count[k] += 1\n",
    "for i in range(len(ls_yaw_rate_cat)):\n",
    "    ls_yaw_rate_cat[i] = ls_yaw_rate_cat[i]-31\n",
    "plt.title('yaw_rate')\n",
    "plt.bar(ls_yaw_rate_cat, ls_yaw_rate_count, width = 0.7, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(ls_sas_angle), max(ls_sas_angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(ls_long_acc), max(ls_long_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(ls_lat_acc), max(ls_lat_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get lateral point from the polynomial equation\n",
    "# input \n",
    "# : longitudinal distance, C1, C2, C3\n",
    "# output \n",
    "# : lateral point x\n",
    "\n",
    "def get_x_polynomial(long_dist, C1, C2, C3):\n",
    "    mean = 0.0\n",
    "    inv_sigma = 1.0\n",
    "    y_ = (long_dist-mean) * inv_sigma\n",
    "    \n",
    "    x = C1 * (y_**1) + C2 * (y_**2) + C3 * (y_**3)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSELoss from lateral point\n",
    "# input\n",
    "# : [C1, C2, C3], gt['ego_global_x', 'ego_global_y']\n",
    "# output\n",
    "# : sum, variance, standard deviation of MSELoss\n",
    "\n",
    "def lp_loss(output, gt):\n",
    "    criterion = nn.MSELoss()\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    mse_total = []\n",
    "    mse_mean, mse_var, mse_std = 0.0, 0.0, 0.0\n",
    "    for i in range(len(gt[0][1])):\n",
    "        x_tmp = get_x_polynomial(gt[0][1][i],output[0][0],output[0][1],output[0][2])\n",
    "        # make to tensor if needed\n",
    "        x_tmp_torch = x_tmp\n",
    "        gt_tmp_torch = gt[0][0][i]\n",
    "        \n",
    "        mse_tmp = criterion(x_tmp_torch,gt_tmp_torch)\n",
    "        mse_total.append(mse_tmp)\n",
    "    mse_mean = torch.mean(torch.tensor(mse_total))\n",
    "    mse_var = torch.var(torch.tensor(mse_total))\n",
    "    mse_std = torch.std(torch.tensor(mse_total))\n",
    "    \n",
    "    return mse_mean, mse_var, mse_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DPENet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPENet\n",
    "# input\n",
    "# : CAN information (ego_vx, yaw_rate, sas_angle, long_acc, lat_acc)\n",
    "# output\n",
    "# : variable for polynomial equation (C1, C2, C3)\n",
    "\n",
    "# construct model on cuda if available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "class DPENet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DPENet,self).__init__()\n",
    "        # fullyconncted layer\n",
    "        fc1 = nn.Linear(5,64,bias=True)\n",
    "        bn1 = nn.BatchNorm1d(64)\n",
    "        fc2 = nn.Linear(64,128,bias=True)\n",
    "        bn2 = nn.BatchNorm1d(128)\n",
    "        fc3 = nn.Linear(128,256,bias=True)\n",
    "        bn3 = nn.BatchNorm1d(256)\n",
    "        fc4 = nn.Linear(256,512,bias=True)\n",
    "        bn4 = nn.BatchNorm1d(512)\n",
    "        fc5 = nn.Linear(512,1024,bias=True)\n",
    "        bn5 = nn.BatchNorm1d(1024)\n",
    "        fc6 = nn.Linear(1024,1024,bias=True)\n",
    "        bn6 = nn.BatchNorm1d(1024)\n",
    "        fc7 = nn.Linear(1024,512,bias=True)\n",
    "        bn7 = nn.BatchNorm1d(512)\n",
    "        fc8 = nn.Linear(512,256,bias=True)\n",
    "        bn8 = nn.BatchNorm1d(256)\n",
    "        fc9 = nn.Linear(256,128,bias=True)\n",
    "        bn9 = nn.BatchNorm1d(128)\n",
    "        fc10 = nn.Linear(128,64,bias=True)\n",
    "        bn10 = nn.BatchNorm1d(64)\n",
    "        fc11 = nn.Linear(64,3,bias=True)\n",
    "        bn11 = nn.BatchNorm1d(3)\n",
    "        tanh = nn.Tanh()\n",
    "        dropout = nn.Dropout(0.5)\n",
    "        #self.relu = nn.ReLU()\n",
    "        \n",
    "        # order should be\n",
    "        # convolution -> batch normalization -> activation -> dropout\n",
    "        # bn이 dropout을 대체할 수 있다는 의견도 있으나 대체로 같이 사용하였을 때 더 좋은 성능 가져옴\n",
    "        self.fc_module = nn.Sequential(\n",
    "            fc1,\n",
    "            bn1,\n",
    "            fc2,\n",
    "            bn2,\n",
    "            fc3,\n",
    "            bn3,\n",
    "            fc4,\n",
    "            bn4,\n",
    "            fc5,\n",
    "            bn5,\n",
    "            fc6,\n",
    "            bn6,\n",
    "            fc7,\n",
    "            bn7,\n",
    "            fc8,\n",
    "            bn8,\n",
    "            fc9,\n",
    "            bn9,\n",
    "            fc10,\n",
    "            bn10,\n",
    "            tanh,\n",
    "            dropout,\n",
    "            fc11\n",
    "        )\n",
    "        # use gpu\n",
    "        if use_cuda:\n",
    "            self_fc_module = self.fc_module.cuda()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.fc_module(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPENet(\n",
      "  (fc_module): Sequential(\n",
      "    (0): Linear(in_features=5, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (7): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (9): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (11): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (13): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (15): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (17): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (19): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (20): Tanh()\n",
      "    (21): Dropout(p=0.5, inplace=False)\n",
      "    (22): Linear(in_features=64, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(DPENet())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xavier Normal initialization\n",
    "def weight_init(m): \n",
    "    if isinstance(m, nn.Linear):\n",
    "        size = m.weight.size()\n",
    "        fan_out = size[0] # number of rows\n",
    "        fan_in = size[1] # number of columns\n",
    "        variance = np.sqrt(2.0/(fan_in + fan_out))\n",
    "        m.weight.data.normal_(0.0, variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argument\n",
    "# learning rate and beta1,2 for optimizer\n",
    "dpenet_learning_rate = 0.001 # 0.01(x) 0.001 0.0002\n",
    "dpenet_momentum = 0.9\n",
    "dpenet_beta1 = 0.5\n",
    "dpenet_beta2 = 0.999\n",
    "# lambda for loss\n",
    "lambda_mean = 10**-7\n",
    "lambda_var = 10**-10\n",
    "lambda_std = 10**-7\n",
    "# number of epoch and log steo\n",
    "num_epochs = 1000\n",
    "log_step = 100\n",
    "dpenet_batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], BatchStep[100/6642]\n",
      "Loss: 1465.9196, lp_mean: 1465.9196, lp_var: 61888733184.0000, lp_std: 2487.7446\n",
      "Epoch [1/1000], BatchStep[200/6642]\n",
      "Loss: 100.7890, lp_mean: 100.7890, lp_var: 251179344.0000, lp_std: 158.4864\n",
      "Epoch [1/1000], BatchStep[300/6642]\n",
      "Loss: 466.2022, lp_mean: 466.2022, lp_var: 6301857792.0000, lp_std: 793.8425\n",
      "Epoch [1/1000], BatchStep[400/6642]\n",
      "Loss: 12325.1504, lp_mean: 12325.1504, lp_var: 4308859617280.0000, lp_std: 20757.7930\n",
      "Epoch [1/1000], BatchStep[500/6642]\n",
      "Loss: 69462.5000, lp_mean: 69462.5000, lp_var: 136787569672192.0000, lp_std: 116956.2188\n",
      "Epoch [1/1000], BatchStep[600/6642]\n",
      "Loss: 6671.3252, lp_mean: 6671.3252, lp_var: 1353330065408.0000, lp_std: 11633.2715\n",
      "Epoch [1/1000], BatchStep[700/6642]\n",
      "Loss: 305.5999, lp_mean: 305.5999, lp_var: 2610588416.0000, lp_std: 510.9392\n",
      "Epoch [1/1000], BatchStep[800/6642]\n",
      "Loss: 16247.9180, lp_mean: 16247.9180, lp_var: 7321226838016.0000, lp_std: 27057.7676\n",
      "Epoch [1/1000], BatchStep[900/6642]\n",
      "Loss: 7122.2446, lp_mean: 7122.2446, lp_var: 1339921661952.0000, lp_std: 11575.4980\n",
      "Epoch [1/1000], BatchStep[1000/6642]\n",
      "Loss: 13496.5146, lp_mean: 13496.5146, lp_var: 4986529382400.0000, lp_std: 22330.5391\n",
      "Epoch [1/1000], BatchStep[1100/6642]\n",
      "Loss: 24120.8438, lp_mean: 24120.8438, lp_var: 16284960948224.0000, lp_std: 40354.6289\n",
      "Epoch [1/1000], BatchStep[1200/6642]\n",
      "Loss: 876.7625, lp_mean: 876.7625, lp_var: 20373172224.0000, lp_std: 1427.3462\n",
      "Epoch [1/1000], BatchStep[1300/6642]\n",
      "Loss: 5111.6011, lp_mean: 5111.6011, lp_var: 732747005952.0000, lp_std: 8560.0645\n",
      "Epoch [1/1000], BatchStep[1400/6642]\n",
      "Loss: 3.2273, lp_mean: 3.2273, lp_var: 182162.1406, lp_std: 4.2680\n",
      "Epoch [1/1000], BatchStep[1500/6642]\n",
      "Loss: 32359.1738, lp_mean: 32359.1738, lp_var: 30552218402816.0000, lp_std: 55274.0586\n",
      "Epoch [1/1000], BatchStep[1600/6642]\n",
      "Loss: 44081.5078, lp_mean: 44081.5078, lp_var: 54896371957760.0000, lp_std: 74092.0859\n",
      "Epoch [1/1000], BatchStep[1700/6642]\n",
      "Loss: 2.3979, lp_mean: 2.3979, lp_var: 246113.9531, lp_std: 4.9610\n",
      "Epoch [1/1000], BatchStep[1800/6642]\n",
      "Loss: 188.2154, lp_mean: 188.2154, lp_var: 1010099008.0000, lp_std: 317.8206\n",
      "Epoch [1/1000], BatchStep[1900/6642]\n",
      "Loss: 149.8359, lp_mean: 149.8359, lp_var: 935629120.0000, lp_std: 305.8806\n",
      "Epoch [1/1000], BatchStep[2000/6642]\n",
      "Loss: 12299.9746, lp_mean: 12299.9746, lp_var: 3892940898304.0000, lp_std: 19730.5371\n",
      "Epoch [1/1000], BatchStep[2100/6642]\n",
      "Loss: 1225.6709, lp_mean: 1225.6709, lp_var: 35357368320.0000, lp_std: 1880.3555\n",
      "Epoch [1/1000], BatchStep[2200/6642]\n",
      "Loss: 0.0000, lp_mean: 0.0000, lp_var: 0.0000, lp_std: 0.0000\n",
      "Epoch [1/1000], BatchStep[2300/6642]\n",
      "Loss: 82.5335, lp_mean: 82.5335, lp_var: 226797344.0000, lp_std: 150.5979\n",
      "Epoch [1/1000], BatchStep[2400/6642]\n",
      "Loss: 12162.4414, lp_mean: 12162.4414, lp_var: 4113435721728.0000, lp_std: 20281.6055\n",
      "Epoch [1/1000], BatchStep[2500/6642]\n",
      "Loss: 19.9549, lp_mean: 19.9549, lp_var: 9827841.0000, lp_std: 31.3494\n",
      "Epoch [1/1000], BatchStep[2600/6642]\n",
      "Loss: 11193.4414, lp_mean: 11193.4414, lp_var: 3644181970944.0000, lp_std: 19089.7402\n",
      "Epoch [1/1000], BatchStep[2700/6642]\n",
      "Loss: 1118.0659, lp_mean: 1118.0659, lp_var: 34558251008.0000, lp_std: 1858.9850\n",
      "Epoch [1/1000], BatchStep[2800/6642]\n",
      "Loss: 1.6144, lp_mean: 1.6144, lp_var: 99901.1172, lp_std: 3.1607\n",
      "Epoch [1/1000], BatchStep[2900/6642]\n",
      "Loss: 0.0258, lp_mean: 0.0258, lp_var: 19.0925, lp_std: 0.0437\n",
      "Epoch [1/1000], BatchStep[3000/6642]\n",
      "Loss: 731.9387, lp_mean: 731.9387, lp_var: 14275245056.0000, lp_std: 1194.7906\n",
      "Epoch [1/1000], BatchStep[3100/6642]\n",
      "Loss: 611.1312, lp_mean: 611.1312, lp_var: 9923695616.0000, lp_std: 996.1776\n",
      "Epoch [1/1000], BatchStep[3200/6642]\n",
      "Loss: 201.4076, lp_mean: 201.4076, lp_var: 1208300032.0000, lp_std: 347.6061\n",
      "Epoch [1/1000], BatchStep[3300/6642]\n",
      "Loss: 22802.2012, lp_mean: 22802.2012, lp_var: 15557580554240.0000, lp_std: 39443.1016\n",
      "Epoch [1/1000], BatchStep[3400/6642]\n",
      "Loss: 3274.7700, lp_mean: 3274.7700, lp_var: 298569400320.0000, lp_std: 5464.1504\n",
      "Epoch [1/1000], BatchStep[3500/6642]\n",
      "Loss: 143.2272, lp_mean: 143.2272, lp_var: 591508864.0000, lp_std: 243.2095\n",
      "Epoch [1/1000], BatchStep[3600/6642]\n",
      "Loss: 373.8873, lp_mean: 373.8873, lp_var: 3863702016.0000, lp_std: 621.5869\n",
      "Epoch [1/1000], BatchStep[3700/6642]\n",
      "Loss: 2654.2515, lp_mean: 2654.2515, lp_var: 206517583872.0000, lp_std: 4544.4209\n",
      "Epoch [1/1000], BatchStep[3800/6642]\n",
      "Loss: 88.8797, lp_mean: 88.8797, lp_var: 225125632.0000, lp_std: 150.0419\n",
      "Epoch [1/1000], BatchStep[3900/6642]\n",
      "Loss: 6180.1328, lp_mean: 6180.1328, lp_var: 1087354372096.0000, lp_std: 10427.6289\n",
      "Epoch [1/1000], BatchStep[4000/6642]\n",
      "Loss: 1.8721, lp_mean: 1.8721, lp_var: 123939.8750, lp_std: 3.5205\n",
      "Epoch [1/1000], BatchStep[4100/6642]\n",
      "Loss: 3.9328, lp_mean: 3.9328, lp_var: 490954.8125, lp_std: 7.0068\n",
      "Epoch [1/1000], BatchStep[4200/6642]\n",
      "Loss: 52526.1562, lp_mean: 52526.1562, lp_var: 77753437126656.0000, lp_std: 88177.9062\n",
      "Epoch [1/1000], BatchStep[4300/6642]\n",
      "Loss: 2520.9604, lp_mean: 2520.9604, lp_var: 191514542080.0000, lp_std: 4376.2378\n",
      "Epoch [1/1000], BatchStep[4400/6642]\n",
      "Loss: 2377.1956, lp_mean: 2377.1956, lp_var: 163687759872.0000, lp_std: 4045.8347\n",
      "Epoch [1/1000], BatchStep[4500/6642]\n",
      "Loss: 3731.8899, lp_mean: 3731.8899, lp_var: 396255592448.0000, lp_std: 6294.8838\n",
      "Epoch [1/1000], BatchStep[4600/6642]\n",
      "Loss: 934.8772, lp_mean: 934.8772, lp_var: 24811253760.0000, lp_std: 1575.1589\n",
      "Epoch [1/1000], BatchStep[4700/6642]\n",
      "Loss: 361.3467, lp_mean: 361.3467, lp_var: 3654288896.0000, lp_std: 604.5071\n",
      "Epoch [1/1000], BatchStep[4800/6642]\n",
      "Loss: 10389.0400, lp_mean: 10389.0400, lp_var: 3003231502336.0000, lp_std: 17329.8340\n",
      "Epoch [1/1000], BatchStep[4900/6642]\n",
      "Loss: 24251.3398, lp_mean: 24251.3398, lp_var: 16946061901824.0000, lp_std: 41165.5977\n",
      "Epoch [1/1000], BatchStep[5000/6642]\n",
      "Loss: 84311.8828, lp_mean: 84311.8828, lp_var: 198405628362752.0000, lp_std: 140856.5469\n",
      "Epoch [1/1000], BatchStep[5100/6642]\n",
      "Loss: 9000.5635, lp_mean: 9000.5635, lp_var: 2262855843840.0000, lp_std: 15042.7910\n",
      "Epoch [1/1000], BatchStep[5200/6642]\n",
      "Loss: 1998.0104, lp_mean: 1998.0104, lp_var: 113511178240.0000, lp_std: 3369.1418\n",
      "Epoch [1/1000], BatchStep[5300/6642]\n",
      "Loss: 14096.3428, lp_mean: 14096.3428, lp_var: 5645353877504.0000, lp_std: 23759.9531\n",
      "Epoch [1/1000], BatchStep[5400/6642]\n",
      "Loss: 1662.5687, lp_mean: 1662.5687, lp_var: 79551152128.0000, lp_std: 2820.4812\n",
      "Epoch [1/1000], BatchStep[5500/6642]\n",
      "Loss: 14022.0781, lp_mean: 14022.0781, lp_var: 5439517884416.0000, lp_std: 23322.7754\n",
      "Epoch [1/1000], BatchStep[5600/6642]\n",
      "Loss: 32073.6211, lp_mean: 32073.6211, lp_var: 27913864871936.0000, lp_std: 52833.5742\n",
      "Epoch [1/1000], BatchStep[5700/6642]\n",
      "Loss: 45648.2578, lp_mean: 45648.2578, lp_var: 57286693224448.0000, lp_std: 75687.9766\n",
      "Epoch [1/1000], BatchStep[5800/6642]\n",
      "Loss: 1875.1484, lp_mean: 1875.1484, lp_var: 97588559872.0000, lp_std: 3123.9167\n",
      "Epoch [1/1000], BatchStep[5900/6642]\n",
      "Loss: 2369.3926, lp_mean: 2369.3926, lp_var: 143235121152.0000, lp_std: 3784.6414\n",
      "Epoch [1/1000], BatchStep[6000/6642]\n",
      "Loss: 0.0481, lp_mean: 0.0481, lp_var: 75.7452, lp_std: 0.0870\n",
      "Epoch [1/1000], BatchStep[6100/6642]\n",
      "Loss: 366.9414, lp_mean: 366.9414, lp_var: 2947657984.0000, lp_std: 542.9234\n",
      "Epoch [1/1000], BatchStep[6200/6642]\n",
      "Loss: 313533.6562, lp_mean: 313533.6562, lp_var: 2720278740205568.0000, lp_std: 521562.9062\n",
      "Epoch [1/1000], BatchStep[6300/6642]\n",
      "Loss: 139604.1562, lp_mean: 139604.1562, lp_var: 544182187851776.0000, lp_std: 233277.1250\n",
      "Epoch [1/1000], BatchStep[6400/6642]\n",
      "Loss: 26993.4922, lp_mean: 26993.4922, lp_var: 20463671574528.0000, lp_std: 45236.7891\n",
      "Epoch [1/1000], BatchStep[6500/6642]\n",
      "Loss: 93580.9844, lp_mean: 93580.9844, lp_var: 246507852791808.0000, lp_std: 157005.6875\n",
      "Epoch [1/1000], BatchStep[6600/6642]\n",
      "Loss: 9673.4033, lp_mean: 9673.4033, lp_var: 2577961320448.0000, lp_std: 16056.0322\n",
      "Epoch [1/1000]\n",
      "Train Loss : 560.574768, Test Loss: 10625042432.0000\n",
      "Epoch [2/1000], BatchStep[100/6642]\n",
      "Loss: 2.8106, lp_mean: 2.8106, lp_var: 187652.2031, lp_std: 4.3319\n",
      "Epoch [2/1000], BatchStep[200/6642]\n",
      "Loss: 869.7250, lp_mean: 869.7250, lp_var: 22308098048.0000, lp_std: 1493.5895\n",
      "Epoch [2/1000], BatchStep[300/6642]\n",
      "Loss: 362063.1250, lp_mean: 362063.1250, lp_var: 3724477259055104.0000, lp_std: 610285.0000\n",
      "Epoch [2/1000], BatchStep[400/6642]\n",
      "Loss: 15232.7783, lp_mean: 15232.7783, lp_var: 6635334402048.0000, lp_std: 25759.1426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/1000], BatchStep[500/6642]\n",
      "Loss: 1441.8594, lp_mean: 1441.8594, lp_var: 62449348608.0000, lp_std: 2498.9866\n",
      "Epoch [2/1000], BatchStep[600/6642]\n",
      "Loss: 2.7045, lp_mean: 2.7045, lp_var: 118127.3359, lp_std: 3.4370\n",
      "Epoch [2/1000], BatchStep[700/6642]\n",
      "Loss: 1976.7640, lp_mean: 1976.7640, lp_var: 124046245888.0000, lp_std: 3522.0203\n",
      "Epoch [2/1000], BatchStep[800/6642]\n",
      "Loss: 50.0910, lp_mean: 50.0910, lp_var: 66338500.0000, lp_std: 81.4484\n",
      "Epoch [2/1000], BatchStep[900/6642]\n",
      "Loss: 129.5639, lp_mean: 129.5639, lp_var: 487714784.0000, lp_std: 220.8427\n",
      "Epoch [2/1000], BatchStep[1000/6642]\n",
      "Loss: 42045.3516, lp_mean: 42045.3516, lp_var: 48521575137280.0000, lp_std: 69657.4297\n",
      "Epoch [2/1000], BatchStep[1100/6642]\n",
      "Loss: 977.0900, lp_mean: 977.0900, lp_var: 27018045440.0000, lp_std: 1643.7167\n",
      "Epoch [2/1000], BatchStep[1200/6642]\n",
      "Loss: 37219.6914, lp_mean: 37219.6914, lp_var: 38902115074048.0000, lp_std: 62371.5625\n",
      "Epoch [2/1000], BatchStep[1300/6642]\n",
      "Loss: 126722.8125, lp_mean: 126722.8125, lp_var: 434555362213888.0000, lp_std: 208459.9062\n",
      "Epoch [2/1000], BatchStep[1400/6642]\n",
      "Loss: 29027.6250, lp_mean: 29027.6250, lp_var: 22974335811584.0000, lp_std: 47931.5508\n",
      "Epoch [2/1000], BatchStep[1500/6642]\n",
      "Loss: 0.0714, lp_mean: 0.0714, lp_var: 177.9485, lp_std: 0.1334\n",
      "Epoch [2/1000], BatchStep[1600/6642]\n",
      "Loss: 66.1973, lp_mean: 66.1973, lp_var: 133156272.0000, lp_std: 115.3934\n",
      "Epoch [2/1000], BatchStep[1700/6642]\n",
      "Loss: 0.0074, lp_mean: 0.0074, lp_var: 2.9610, lp_std: 0.0172\n",
      "Epoch [2/1000], BatchStep[1800/6642]\n",
      "Loss: 0.6469, lp_mean: 0.6469, lp_var: 14489.2520, lp_std: 1.2037\n",
      "Epoch [2/1000], BatchStep[1900/6642]\n",
      "Loss: 331223.1562, lp_mean: 331223.1562, lp_var: 3033793099202560.0000, lp_std: 550798.8125\n",
      "Epoch [2/1000], BatchStep[2000/6642]\n",
      "Loss: 1184.1439, lp_mean: 1184.1439, lp_var: 38361161728.0000, lp_std: 1958.6005\n",
      "Epoch [2/1000], BatchStep[2100/6642]\n",
      "Loss: 4676.1753, lp_mean: 4676.1753, lp_var: 623081160704.0000, lp_std: 7893.5493\n",
      "Epoch [2/1000], BatchStep[2200/6642]\n",
      "Loss: 420.4421, lp_mean: 420.4421, lp_var: 4927762944.0000, lp_std: 701.9803\n",
      "Epoch [2/1000], BatchStep[2300/6642]\n",
      "Loss: 5434.6152, lp_mean: 5434.6152, lp_var: 860050882560.0000, lp_std: 9273.8926\n",
      "Epoch [2/1000], BatchStep[2400/6642]\n",
      "Loss: 373.0872, lp_mean: 373.0872, lp_var: 3670052096.0000, lp_std: 605.8096\n",
      "Epoch [2/1000], BatchStep[2500/6642]\n",
      "Loss: 8588.6133, lp_mean: 8588.6133, lp_var: 2161174773760.0000, lp_std: 14700.9346\n",
      "Epoch [2/1000], BatchStep[2600/6642]\n",
      "Loss: 0.0000, lp_mean: 0.0000, lp_var: 0.0000, lp_std: 0.0000\n",
      "Epoch [2/1000], BatchStep[2700/6642]\n",
      "Loss: 11541.6406, lp_mean: 11541.6406, lp_var: 3541986705408.0000, lp_std: 18820.1660\n",
      "Epoch [2/1000], BatchStep[2800/6642]\n",
      "Loss: 7023.0908, lp_mean: 7023.0908, lp_var: 1432398987264.0000, lp_std: 11968.2871\n",
      "Epoch [2/1000], BatchStep[2900/6642]\n",
      "Loss: 9094.6719, lp_mean: 9094.6719, lp_var: 2346644406272.0000, lp_std: 15318.7598\n",
      "Epoch [2/1000], BatchStep[3000/6642]\n",
      "Loss: 5718.0620, lp_mean: 5718.0620, lp_var: 928447725568.0000, lp_std: 9635.5996\n",
      "Epoch [2/1000], BatchStep[3100/6642]\n",
      "Loss: 970.0588, lp_mean: 970.0588, lp_var: 24712134656.0000, lp_std: 1572.0094\n",
      "Epoch [2/1000], BatchStep[3200/6642]\n",
      "Loss: 95.8672, lp_mean: 95.8672, lp_var: 245364384.0000, lp_std: 156.6411\n",
      "Epoch [2/1000], BatchStep[3300/6642]\n",
      "Loss: 0.0000, lp_mean: 0.0000, lp_var: 0.0000, lp_std: 0.0000\n",
      "Epoch [2/1000], BatchStep[3400/6642]\n",
      "Loss: 724925.8750, lp_mean: 724925.8750, lp_var: 15102383839248384.0000, lp_std: 1228917.5000\n",
      "Epoch [2/1000], BatchStep[3500/6642]\n",
      "Loss: 161607.5312, lp_mean: 161607.5312, lp_var: 743102625087488.0000, lp_std: 272599.0938\n",
      "Epoch [2/1000], BatchStep[3600/6642]\n",
      "Loss: 251226.9062, lp_mean: 251226.9062, lp_var: 1742966716628992.0000, lp_std: 417488.5312\n",
      "Epoch [2/1000], BatchStep[3700/6642]\n",
      "Loss: 3046.4031, lp_mean: 3046.4031, lp_var: 253164896256.0000, lp_std: 5031.5493\n",
      "Epoch [2/1000], BatchStep[3800/6642]\n",
      "Loss: 249.2755, lp_mean: 249.2755, lp_var: 1696992384.0000, lp_std: 411.9457\n",
      "Epoch [2/1000], BatchStep[3900/6642]\n",
      "Loss: 119.8550, lp_mean: 119.8550, lp_var: 408268672.0000, lp_std: 202.0566\n",
      "Epoch [2/1000], BatchStep[4000/6642]\n",
      "Loss: 2485.6626, lp_mean: 2485.6626, lp_var: 183524900864.0000, lp_std: 4283.9805\n",
      "Epoch [2/1000], BatchStep[4100/6642]\n",
      "Loss: 190.5204, lp_mean: 190.5204, lp_var: 1021328896.0000, lp_std: 319.5824\n",
      "Epoch [2/1000], BatchStep[4200/6642]\n",
      "Loss: 202.2238, lp_mean: 202.2238, lp_var: 1147908608.0000, lp_std: 338.8080\n",
      "Epoch [2/1000], BatchStep[4300/6642]\n",
      "Loss: 18889.6191, lp_mean: 18889.6191, lp_var: 9989867438080.0000, lp_std: 31606.7520\n",
      "Epoch [2/1000], BatchStep[4400/6642]\n",
      "Loss: 407.3303, lp_mean: 407.3303, lp_var: 3898697216.0000, lp_std: 624.3954\n",
      "Epoch [2/1000], BatchStep[4500/6642]\n",
      "Loss: 10.5540, lp_mean: 10.5540, lp_var: 3401429.2500, lp_std: 18.4430\n",
      "Epoch [2/1000], BatchStep[4600/6642]\n",
      "Loss: 40931.1445, lp_mean: 40931.1445, lp_var: 46577980801024.0000, lp_std: 68248.0625\n",
      "Epoch [2/1000], BatchStep[4700/6642]\n",
      "Loss: 49083.5742, lp_mean: 49083.5742, lp_var: 65596003713024.0000, lp_std: 80991.3594\n",
      "Epoch [2/1000], BatchStep[4800/6642]\n",
      "Loss: 1268.6665, lp_mean: 1268.6665, lp_var: 45085937664.0000, lp_std: 2123.3450\n",
      "Epoch [2/1000], BatchStep[4900/6642]\n",
      "Loss: 2276.4788, lp_mean: 2276.4788, lp_var: 141965033472.0000, lp_std: 3767.8250\n",
      "Epoch [2/1000], BatchStep[5000/6642]\n",
      "Loss: 26394.1934, lp_mean: 26394.1934, lp_var: 17689823150080.0000, lp_std: 42059.2734\n",
      "Epoch [2/1000], BatchStep[5100/6642]\n",
      "Loss: 4676.5137, lp_mean: 4676.5137, lp_var: 633496272896.0000, lp_std: 7959.2480\n",
      "Epoch [2/1000], BatchStep[5200/6642]\n",
      "Loss: 21209.3340, lp_mean: 21209.3340, lp_var: 12564709769216.0000, lp_std: 35446.7344\n",
      "Epoch [2/1000], BatchStep[5300/6642]\n",
      "Loss: 3972.3484, lp_mean: 3972.3484, lp_var: 414026498048.0000, lp_std: 6434.4893\n",
      "Epoch [2/1000], BatchStep[5400/6642]\n",
      "Loss: 1200.0691, lp_mean: 1200.0691, lp_var: 36939923456.0000, lp_std: 1921.9762\n",
      "Epoch [2/1000], BatchStep[5500/6642]\n",
      "Loss: 33025.9492, lp_mean: 33025.9492, lp_var: 30666974560256.0000, lp_std: 55377.7695\n",
      "Epoch [2/1000], BatchStep[5600/6642]\n",
      "Loss: 26.0327, lp_mean: 26.0327, lp_var: 15497129.0000, lp_std: 39.3664\n",
      "Epoch [2/1000], BatchStep[5700/6642]\n",
      "Loss: 1201.5223, lp_mean: 1201.5223, lp_var: 40250466304.0000, lp_std: 2006.2517\n",
      "Epoch [2/1000], BatchStep[5800/6642]\n",
      "Loss: 8833.6387, lp_mean: 8833.6387, lp_var: 2347385225216.0000, lp_std: 15321.1787\n",
      "Epoch [2/1000], BatchStep[5900/6642]\n",
      "Loss: 828.5309, lp_mean: 828.5309, lp_var: 21434245120.0000, lp_std: 1464.0438\n",
      "Epoch [2/1000], BatchStep[6000/6642]\n",
      "Loss: 1483.3243, lp_mean: 1483.3243, lp_var: 56576401408.0000, lp_std: 2378.5793\n",
      "Epoch [2/1000], BatchStep[6100/6642]\n",
      "Loss: 11960.0635, lp_mean: 11960.0635, lp_var: 3968419758080.0000, lp_std: 19920.8926\n",
      "Epoch [2/1000], BatchStep[6200/6642]\n",
      "Loss: 42.1388, lp_mean: 42.1388, lp_var: 49810692.0000, lp_std: 70.5767\n",
      "Epoch [2/1000], BatchStep[6300/6642]\n",
      "Loss: 50772.0508, lp_mean: 50772.0508, lp_var: 71890555109376.0000, lp_std: 84788.3047\n",
      "Epoch [2/1000], BatchStep[6400/6642]\n",
      "Loss: 147.4804, lp_mean: 147.4804, lp_var: 611799936.0000, lp_std: 247.3459\n",
      "Epoch [2/1000], BatchStep[6500/6642]\n",
      "Loss: 4202.7510, lp_mean: 4202.7510, lp_var: 491558731776.0000, lp_std: 7011.1250\n",
      "Epoch [2/1000], BatchStep[6600/6642]\n",
      "Loss: 14325.7695, lp_mean: 14325.7695, lp_var: 6042784628736.0000, lp_std: 24582.0762\n",
      "Epoch [2/1000]\n",
      "Train Loss : 564.531067, Test Loss: 9672336384.0000\n",
      "Epoch [3/1000], BatchStep[100/6642]\n",
      "Loss: 11969.1035, lp_mean: 11969.1035, lp_var: 3854269415424.0000, lp_std: 19632.2930\n",
      "Epoch [3/1000], BatchStep[200/6642]\n",
      "Loss: 1178.6045, lp_mean: 1178.6045, lp_var: 36134526976.0000, lp_std: 1900.9083\n",
      "Epoch [3/1000], BatchStep[300/6642]\n",
      "Loss: 4.9712, lp_mean: 4.9712, lp_var: 621974.2500, lp_std: 7.8865\n",
      "Epoch [3/1000], BatchStep[400/6642]\n",
      "Loss: 0.0633, lp_mean: 0.0633, lp_var: 71.8814, lp_std: 0.0848\n",
      "Epoch [3/1000], BatchStep[500/6642]\n",
      "Loss: 402.0989, lp_mean: 402.0989, lp_var: 5124749824.0000, lp_std: 715.8736\n",
      "Epoch [3/1000], BatchStep[600/6642]\n",
      "Loss: 7678.8130, lp_mean: 7678.8130, lp_var: 1629387751424.0000, lp_std: 12764.7471\n",
      "Epoch [3/1000], BatchStep[700/6642]\n",
      "Loss: 0.0139, lp_mean: 0.0139, lp_var: 6.4757, lp_std: 0.0254\n",
      "Epoch [3/1000], BatchStep[800/6642]\n",
      "Loss: 22.7425, lp_mean: 22.7425, lp_var: 13580584.0000, lp_std: 36.8518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/1000], BatchStep[900/6642]\n",
      "Loss: 3.6515, lp_mean: 3.6515, lp_var: 333594.8750, lp_std: 5.7758\n",
      "Epoch [3/1000], BatchStep[1000/6642]\n",
      "Loss: 4921.6445, lp_mean: 4921.6445, lp_var: 676501848064.0000, lp_std: 8224.9736\n",
      "Epoch [3/1000], BatchStep[1100/6642]\n",
      "Loss: 6584.3716, lp_mean: 6584.3716, lp_var: 1224556675072.0000, lp_std: 11065.9688\n",
      "Epoch [3/1000], BatchStep[1200/6642]\n",
      "Loss: 0.0227, lp_mean: 0.0227, lp_var: 6.3168, lp_std: 0.0251\n",
      "Epoch [3/1000], BatchStep[1300/6642]\n",
      "Loss: 5681.3042, lp_mean: 5681.3042, lp_var: 882726600704.0000, lp_std: 9395.3535\n",
      "Epoch [3/1000], BatchStep[1400/6642]\n",
      "Loss: 91288.6797, lp_mean: 91288.6797, lp_var: 237485166690304.0000, lp_std: 154105.5312\n",
      "Epoch [3/1000], BatchStep[1500/6642]\n",
      "Loss: 89.5338, lp_mean: 89.5338, lp_var: 134629616.0000, lp_std: 116.0300\n",
      "Epoch [3/1000], BatchStep[1600/6642]\n",
      "Loss: 31.5613, lp_mean: 31.5613, lp_var: 28346392.0000, lp_std: 53.2413\n",
      "Epoch [3/1000], BatchStep[1700/6642]\n",
      "Loss: 144464.0781, lp_mean: 144464.0781, lp_var: 581303623745536.0000, lp_std: 241102.3906\n",
      "Epoch [3/1000], BatchStep[1800/6642]\n",
      "Loss: 105974.4688, lp_mean: 105974.4688, lp_var: 315961315950592.0000, lp_std: 177753.0000\n",
      "Epoch [3/1000], BatchStep[1900/6642]\n",
      "Loss: 0.0000, lp_mean: 0.0000, lp_var: 0.0000, lp_std: 0.0000\n",
      "Epoch [3/1000], BatchStep[2000/6642]\n",
      "Loss: 0.7525, lp_mean: 0.7525, lp_var: 14364.0303, lp_std: 1.1985\n",
      "Epoch [3/1000], BatchStep[2100/6642]\n",
      "Loss: 14.4447, lp_mean: 14.4447, lp_var: 4482212.5000, lp_std: 21.1712\n",
      "Epoch [3/1000], BatchStep[2200/6642]\n",
      "Loss: 361.6075, lp_mean: 361.6075, lp_var: 3888441856.0000, lp_std: 623.5737\n",
      "Epoch [3/1000], BatchStep[2300/6642]\n",
      "Loss: 74494.2969, lp_mean: 74494.2969, lp_var: 152919726882816.0000, lp_std: 123660.7188\n",
      "Epoch [3/1000], BatchStep[2400/6642]\n",
      "Loss: 0.8197, lp_mean: 0.8197, lp_var: 19339.6660, lp_std: 1.3907\n",
      "Epoch [3/1000], BatchStep[2500/6642]\n",
      "Loss: 11932.2285, lp_mean: 11932.2285, lp_var: 3929621659648.0000, lp_std: 19823.2734\n",
      "Epoch [3/1000], BatchStep[2600/6642]\n",
      "Loss: 4046.8320, lp_mean: 4046.8320, lp_var: 452374855680.0000, lp_std: 6725.8818\n",
      "Epoch [3/1000], BatchStep[2700/6642]\n",
      "Loss: 291269.0000, lp_mean: 291269.0000, lp_var: 2383119512502272.0000, lp_std: 488172.0625\n",
      "Epoch [3/1000], BatchStep[2800/6642]\n",
      "Loss: 2.8415, lp_mean: 2.8415, lp_var: 117520.3359, lp_std: 3.4281\n",
      "Epoch [3/1000], BatchStep[2900/6642]\n",
      "Loss: 1.0360, lp_mean: 1.0360, lp_var: 13737.7119, lp_std: 1.1721\n",
      "Epoch [3/1000], BatchStep[3000/6642]\n",
      "Loss: 22640.6914, lp_mean: 22640.6914, lp_var: 14060853657600.0000, lp_std: 37497.8047\n",
      "Epoch [3/1000], BatchStep[3100/6642]\n",
      "Loss: 10501.0820, lp_mean: 10501.0820, lp_var: 3185175953408.0000, lp_std: 17847.0625\n",
      "Epoch [3/1000], BatchStep[3200/6642]\n",
      "Loss: 529501.7500, lp_mean: 529501.7500, lp_var: 7974575300149248.0000, lp_std: 893004.7500\n",
      "Epoch [3/1000], BatchStep[3300/6642]\n",
      "Loss: 1876.4847, lp_mean: 1876.4847, lp_var: 100038688768.0000, lp_std: 3162.8894\n",
      "Epoch [3/1000], BatchStep[3400/6642]\n",
      "Loss: 162.3273, lp_mean: 162.3273, lp_var: 763517120.0000, lp_std: 276.3181\n",
      "Epoch [3/1000], BatchStep[3500/6642]\n",
      "Loss: 111.2956, lp_mean: 111.2956, lp_var: 406117888.0000, lp_std: 201.5237\n",
      "Epoch [3/1000], BatchStep[3600/6642]\n",
      "Loss: 654.8414, lp_mean: 654.8414, lp_var: 10956316672.0000, lp_std: 1046.7244\n",
      "Epoch [3/1000], BatchStep[3700/6642]\n",
      "Loss: 616.9691, lp_mean: 616.9691, lp_var: 11323787264.0000, lp_std: 1064.1328\n",
      "Epoch [3/1000], BatchStep[3800/6642]\n",
      "Loss: 451.1880, lp_mean: 451.1880, lp_var: 5591944192.0000, lp_std: 747.7930\n",
      "Epoch [3/1000], BatchStep[3900/6642]\n",
      "Loss: 727.8477, lp_mean: 727.8477, lp_var: 14301614080.0000, lp_std: 1195.8936\n",
      "Epoch [3/1000], BatchStep[4000/6642]\n",
      "Loss: 8499.5498, lp_mean: 8499.5498, lp_var: 1963677712384.0000, lp_std: 14013.1289\n",
      "Epoch [3/1000], BatchStep[4100/6642]\n",
      "Loss: 1980.9298, lp_mean: 1980.9298, lp_var: 96087793664.0000, lp_std: 3099.8030\n",
      "Epoch [3/1000], BatchStep[4200/6642]\n",
      "Loss: 103408.8984, lp_mean: 103408.8984, lp_var: 291082315235328.0000, lp_std: 170611.3594\n",
      "Epoch [3/1000], BatchStep[4300/6642]\n",
      "Loss: 1577.1951, lp_mean: 1577.1951, lp_var: 66939154432.0000, lp_std: 2587.2603\n",
      "Epoch [3/1000], BatchStep[4400/6642]\n",
      "Loss: 0.0056, lp_mean: 0.0056, lp_var: 1.3301, lp_std: 0.0115\n",
      "Epoch [3/1000], BatchStep[4500/6642]\n",
      "Loss: 336.2735, lp_mean: 336.2735, lp_var: 3007436800.0000, lp_std: 548.4011\n",
      "Epoch [3/1000], BatchStep[4600/6642]\n",
      "Loss: 1.3258, lp_mean: 1.3258, lp_var: 16940.7598, lp_std: 1.3016\n",
      "Epoch [3/1000], BatchStep[4700/6642]\n",
      "Loss: 55.3274, lp_mean: 55.3274, lp_var: 69194928.0000, lp_std: 83.1835\n",
      "Epoch [3/1000], BatchStep[4800/6642]\n",
      "Loss: 160982.9062, lp_mean: 160982.9062, lp_var: 742583001153536.0000, lp_std: 272503.7812\n",
      "Epoch [3/1000], BatchStep[4900/6642]\n",
      "Loss: 65954.8125, lp_mean: 65954.8125, lp_var: 123076289232896.0000, lp_std: 110939.7578\n",
      "Epoch [3/1000], BatchStep[5000/6642]\n",
      "Loss: 205969.1719, lp_mean: 205969.1719, lp_var: 1219069155999744.0000, lp_std: 349151.6875\n",
      "Epoch [3/1000], BatchStep[5100/6642]\n",
      "Loss: 20.9040, lp_mean: 20.9040, lp_var: 13281919.0000, lp_std: 36.4444\n",
      "Epoch [3/1000], BatchStep[5200/6642]\n",
      "Loss: 16007.2285, lp_mean: 16007.2285, lp_var: 7175356284928.0000, lp_std: 26786.8555\n",
      "Epoch [3/1000], BatchStep[5300/6642]\n",
      "Loss: 8547.9326, lp_mean: 8547.9326, lp_var: 2165055815680.0000, lp_std: 14714.1279\n",
      "Epoch [3/1000], BatchStep[5400/6642]\n",
      "Loss: 861.8703, lp_mean: 861.8703, lp_var: 21865953280.0000, lp_std: 1478.7141\n",
      "Epoch [3/1000], BatchStep[5500/6642]\n",
      "Loss: 0.5347, lp_mean: 0.5347, lp_var: 7043.1768, lp_std: 0.8392\n",
      "Epoch [3/1000], BatchStep[5600/6642]\n",
      "Loss: 2644.2471, lp_mean: 2644.2471, lp_var: 178875809792.0000, lp_std: 4229.3711\n",
      "Epoch [3/1000], BatchStep[5700/6642]\n",
      "Loss: 428.8480, lp_mean: 428.8480, lp_var: 5193986048.0000, lp_std: 720.6932\n",
      "Epoch [3/1000], BatchStep[5800/6642]\n",
      "Loss: 122280.3750, lp_mean: 122280.3750, lp_var: 422617097961472.0000, lp_std: 205576.5312\n",
      "Epoch [3/1000], BatchStep[5900/6642]\n",
      "Loss: 11306.2988, lp_mean: 11306.2988, lp_var: 3563956207616.0000, lp_std: 18878.4434\n",
      "Epoch [3/1000], BatchStep[6000/6642]\n",
      "Loss: 11885.5811, lp_mean: 11885.5811, lp_var: 2986594009088.0000, lp_std: 17281.7656\n",
      "Epoch [3/1000], BatchStep[6100/6642]\n",
      "Loss: 99.1920, lp_mean: 99.1920, lp_var: 276378560.0000, lp_std: 166.2464\n",
      "Epoch [3/1000], BatchStep[6200/6642]\n",
      "Loss: 16201.3496, lp_mean: 16201.3496, lp_var: 7314508611584.0000, lp_std: 27045.3496\n",
      "Epoch [3/1000], BatchStep[6300/6642]\n",
      "Loss: 964.3408, lp_mean: 964.3408, lp_var: 23714801664.0000, lp_std: 1539.9612\n",
      "Epoch [3/1000], BatchStep[6400/6642]\n",
      "Loss: 15340.1641, lp_mean: 15340.1641, lp_var: 6511210266624.0000, lp_std: 25517.0723\n",
      "Epoch [3/1000], BatchStep[6500/6642]\n",
      "Loss: 14.3523, lp_mean: 14.3523, lp_var: 5950332.0000, lp_std: 24.3933\n",
      "Epoch [3/1000], BatchStep[6600/6642]\n",
      "Loss: 732.9706, lp_mean: 732.9706, lp_var: 15938599936.0000, lp_std: 1262.4816\n",
      "Epoch [3/1000]\n",
      "Train Loss : 534.254761, Test Loss: 9264115712.0000\n",
      "Epoch [4/1000], BatchStep[100/6642]\n",
      "Loss: 38733.6094, lp_mean: 38733.6094, lp_var: 38501873614848.0000, lp_std: 62049.8789\n",
      "Epoch [4/1000], BatchStep[200/6642]\n",
      "Loss: 0.0336, lp_mean: 0.0336, lp_var: 29.6278, lp_std: 0.0544\n",
      "Epoch [4/1000], BatchStep[300/6642]\n",
      "Loss: 2315.4980, lp_mean: 2315.4980, lp_var: 147699695616.0000, lp_std: 3843.1716\n",
      "Epoch [4/1000], BatchStep[400/6642]\n",
      "Loss: 28627.5566, lp_mean: 28627.5566, lp_var: 23400344977408.0000, lp_std: 48373.9062\n",
      "Epoch [4/1000], BatchStep[500/6642]\n",
      "Loss: 22054.5957, lp_mean: 22054.5957, lp_var: 14700390645760.0000, lp_std: 38341.0859\n",
      "Epoch [4/1000], BatchStep[600/6642]\n",
      "Loss: 5.3286, lp_mean: 5.3286, lp_var: 592888.3750, lp_std: 7.6999\n",
      "Epoch [4/1000], BatchStep[700/6642]\n",
      "Loss: 217.1836, lp_mean: 217.1836, lp_var: 1502340992.0000, lp_std: 387.6004\n",
      "Epoch [4/1000], BatchStep[800/6642]\n",
      "Loss: 0.0290, lp_mean: 0.0290, lp_var: 41.4447, lp_std: 0.0644\n",
      "Epoch [4/1000], BatchStep[900/6642]\n",
      "Loss: 4468.2471, lp_mean: 4468.2471, lp_var: 550517342208.0000, lp_std: 7419.6855\n",
      "Epoch [4/1000], BatchStep[1000/6642]\n",
      "Loss: 833.2725, lp_mean: 833.2725, lp_var: 18128797696.0000, lp_std: 1346.4323\n",
      "Epoch [4/1000], BatchStep[1100/6642]\n",
      "Loss: 13816.2705, lp_mean: 13816.2705, lp_var: 5469053124608.0000, lp_std: 23386.0059\n",
      "Epoch [4/1000], BatchStep[1200/6642]\n",
      "Loss: 12.0556, lp_mean: 12.0556, lp_var: 4097638.0000, lp_std: 20.2426\n",
      "Epoch [4/1000], BatchStep[1300/6642]\n",
      "Loss: 1937.1437, lp_mean: 1937.1437, lp_var: 109062987776.0000, lp_std: 3302.4685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/1000], BatchStep[1400/6642]\n",
      "Loss: 62382.5508, lp_mean: 62382.5508, lp_var: 110140460105728.0000, lp_std: 104947.8281\n",
      "Epoch [4/1000], BatchStep[1500/6642]\n",
      "Loss: 13.7114, lp_mean: 13.7114, lp_var: 6086109.0000, lp_std: 24.6700\n",
      "Epoch [4/1000], BatchStep[1600/6642]\n",
      "Loss: 130821.7734, lp_mean: 130821.7734, lp_var: 480556139675648.0000, lp_std: 219215.9062\n",
      "Epoch [4/1000], BatchStep[1700/6642]\n",
      "Loss: 20592.4316, lp_mean: 20592.4316, lp_var: 12012719439872.0000, lp_std: 34659.3711\n",
      "Epoch [4/1000], BatchStep[1800/6642]\n",
      "Loss: 8061.3687, lp_mean: 8061.3687, lp_var: 1758078959616.0000, lp_std: 13259.2578\n",
      "Epoch [4/1000], BatchStep[1900/6642]\n",
      "Loss: 0.7975, lp_mean: 0.7975, lp_var: 15965.5850, lp_std: 1.2635\n",
      "Epoch [4/1000], BatchStep[2000/6642]\n",
      "Loss: 2314.7993, lp_mean: 2314.7993, lp_var: 148081410048.0000, lp_std: 3848.1348\n",
      "Epoch [4/1000], BatchStep[2100/6642]\n",
      "Loss: 0.0000, lp_mean: 0.0000, lp_var: 0.0000, lp_std: 0.0000\n",
      "Epoch [4/1000], BatchStep[2200/6642]\n",
      "Loss: 61198.7188, lp_mean: 61198.7188, lp_var: 105210055753728.0000, lp_std: 102571.9531\n",
      "Epoch [4/1000], BatchStep[2300/6642]\n",
      "Loss: 0.0020, lp_mean: 0.0020, lp_var: 0.3182, lp_std: 0.0056\n",
      "Epoch [4/1000], BatchStep[2400/6642]\n",
      "Loss: 0.2654, lp_mean: 0.2654, lp_var: 3661.9539, lp_std: 0.6051\n",
      "Epoch [4/1000], BatchStep[2500/6642]\n",
      "Loss: 78.7992, lp_mean: 78.7992, lp_var: 183241520.0000, lp_std: 135.3667\n",
      "Epoch [4/1000], BatchStep[2600/6642]\n",
      "Loss: 41.8297, lp_mean: 41.8297, lp_var: 63244884.0000, lp_std: 79.5266\n",
      "Epoch [4/1000], BatchStep[2700/6642]\n",
      "Loss: 666743.8125, lp_mean: 666743.8125, lp_var: 12675501831225344.0000, lp_std: 1125855.3750\n",
      "Epoch [4/1000], BatchStep[2800/6642]\n",
      "Loss: 712.5275, lp_mean: 712.5275, lp_var: 14447047680.0000, lp_std: 1201.9587\n",
      "Epoch [4/1000], BatchStep[2900/6642]\n",
      "Loss: 1732.9069, lp_mean: 1732.9069, lp_var: 84115046400.0000, lp_std: 2900.2595\n",
      "Epoch [4/1000], BatchStep[3000/6642]\n",
      "Loss: 12575.0498, lp_mean: 12575.0498, lp_var: 4379624079360.0000, lp_std: 20927.5508\n",
      "Epoch [4/1000], BatchStep[3100/6642]\n",
      "Loss: 0.0192, lp_mean: 0.0192, lp_var: 7.1670, lp_std: 0.0268\n",
      "Epoch [4/1000], BatchStep[3200/6642]\n",
      "Loss: 57347.2266, lp_mean: 57347.2266, lp_var: 90574409433088.0000, lp_std: 95170.5859\n",
      "Epoch [4/1000], BatchStep[3300/6642]\n",
      "Loss: 639.9627, lp_mean: 639.9627, lp_var: 10333734912.0000, lp_std: 1016.5498\n",
      "Epoch [4/1000], BatchStep[3400/6642]\n",
      "Loss: 461.1599, lp_mean: 461.1599, lp_var: 5641168384.0000, lp_std: 751.0771\n",
      "Epoch [4/1000], BatchStep[3500/6642]\n",
      "Loss: 31772.5449, lp_mean: 31772.5449, lp_var: 28758088089600.0000, lp_std: 53626.5703\n",
      "Epoch [4/1000], BatchStep[3600/6642]\n",
      "Loss: 163143.4219, lp_mean: 163143.4219, lp_var: 709583156805632.0000, lp_std: 266380.0000\n",
      "Epoch [4/1000], BatchStep[3700/6642]\n",
      "Loss: 6018.5562, lp_mean: 6018.5562, lp_var: 1033118220288.0000, lp_std: 10164.2422\n",
      "Epoch [4/1000], BatchStep[3800/6642]\n",
      "Loss: 96353.5781, lp_mean: 96353.5781, lp_var: 265008491528192.0000, lp_std: 162790.8125\n",
      "Epoch [4/1000], BatchStep[3900/6642]\n",
      "Loss: 587.5686, lp_mean: 587.5686, lp_var: 9423912960.0000, lp_std: 970.7684\n",
      "Epoch [4/1000], BatchStep[4000/6642]\n",
      "Loss: 21.7504, lp_mean: 21.7504, lp_var: 15323040.0000, lp_std: 39.1447\n",
      "Epoch [4/1000], BatchStep[4100/6642]\n",
      "Loss: 737.7313, lp_mean: 737.7313, lp_var: 18642855936.0000, lp_std: 1365.3884\n",
      "Epoch [4/1000], BatchStep[4200/6642]\n",
      "Loss: 343389.7812, lp_mean: 343389.7812, lp_var: 3297188947951616.0000, lp_std: 574211.5625\n",
      "Epoch [4/1000], BatchStep[4300/6642]\n",
      "Loss: 610.3193, lp_mean: 610.3193, lp_var: 12889482240.0000, lp_std: 1135.3185\n",
      "Epoch [4/1000], BatchStep[4400/6642]\n",
      "Loss: 8.6760, lp_mean: 8.6760, lp_var: 1721872.7500, lp_std: 13.1220\n",
      "Epoch [4/1000], BatchStep[4500/6642]\n",
      "Loss: 16852.6562, lp_mean: 16852.6562, lp_var: 7957142044672.0000, lp_std: 28208.4043\n",
      "Epoch [4/1000], BatchStep[4600/6642]\n",
      "Loss: 20877.6934, lp_mean: 20877.6934, lp_var: 12126269734912.0000, lp_std: 34822.7969\n",
      "Epoch [4/1000], BatchStep[4700/6642]\n",
      "Loss: 21822.5469, lp_mean: 21822.5469, lp_var: 13202936037376.0000, lp_std: 36335.8438\n",
      "Epoch [4/1000], BatchStep[4800/6642]\n",
      "Loss: 38242.1680, lp_mean: 38242.1680, lp_var: 40507438792704.0000, lp_std: 63645.4570\n",
      "Epoch [4/1000], BatchStep[4900/6642]\n",
      "Loss: 5479.5039, lp_mean: 5479.5039, lp_var: 833375764480.0000, lp_std: 9128.9424\n",
      "Epoch [4/1000], BatchStep[5000/6642]\n",
      "Loss: 238893.4531, lp_mean: 238893.4531, lp_var: 1605337576636416.0000, lp_std: 400666.6250\n",
      "Epoch [4/1000], BatchStep[5100/6642]\n",
      "Loss: 40098.5664, lp_mean: 40098.5664, lp_var: 45053762338816.0000, lp_std: 67122.0938\n",
      "Epoch [4/1000], BatchStep[5200/6642]\n",
      "Loss: 323665.3438, lp_mean: 323665.3438, lp_var: 2953689274777600.0000, lp_std: 543478.5000\n",
      "Epoch [4/1000], BatchStep[5300/6642]\n",
      "Loss: 44465.7031, lp_mean: 44465.7031, lp_var: 56213907701760.0000, lp_std: 74975.9297\n",
      "Epoch [4/1000], BatchStep[5400/6642]\n",
      "Loss: 67581.2734, lp_mean: 67581.2734, lp_var: 133868988399616.0000, lp_std: 115701.7656\n",
      "Epoch [4/1000], BatchStep[5500/6642]\n",
      "Loss: 91.2912, lp_mean: 91.2912, lp_var: 183027344.0000, lp_std: 135.2876\n",
      "Epoch [4/1000], BatchStep[5600/6642]\n",
      "Loss: 19.1669, lp_mean: 19.1669, lp_var: 10236892.0000, lp_std: 31.9951\n",
      "Epoch [4/1000], BatchStep[5700/6642]\n",
      "Loss: 2478.8208, lp_mean: 2478.8208, lp_var: 171133648896.0000, lp_std: 4136.8301\n",
      "Epoch [4/1000], BatchStep[5800/6642]\n",
      "Loss: 7.1155, lp_mean: 7.1155, lp_var: 1761537.5000, lp_std: 13.2723\n",
      "Epoch [4/1000], BatchStep[5900/6642]\n",
      "Loss: 451.5071, lp_mean: 451.5071, lp_var: 5842335744.0000, lp_std: 764.3517\n",
      "Epoch [4/1000], BatchStep[6000/6642]\n",
      "Loss: 8094.3394, lp_mean: 8094.3394, lp_var: 1787170783232.0000, lp_std: 13368.5107\n",
      "Epoch [4/1000], BatchStep[6100/6642]\n",
      "Loss: 187791.1094, lp_mean: 187791.1094, lp_var: 993072204742656.0000, lp_std: 315130.5000\n",
      "Epoch [4/1000], BatchStep[6200/6642]\n",
      "Loss: 93286.6797, lp_mean: 93286.6797, lp_var: 239798828662784.0000, lp_std: 154854.3906\n",
      "Epoch [4/1000], BatchStep[6300/6642]\n",
      "Loss: 3.0429, lp_mean: 3.0429, lp_var: 170535.2031, lp_std: 4.1296\n",
      "Epoch [4/1000], BatchStep[6400/6642]\n",
      "Loss: 0.2088, lp_mean: 0.2088, lp_var: 2203.3396, lp_std: 0.4694\n",
      "Epoch [4/1000], BatchStep[6500/6642]\n",
      "Loss: 12393.6094, lp_mean: 12393.6094, lp_var: 4150286352384.0000, lp_std: 20372.2500\n",
      "Epoch [4/1000], BatchStep[6600/6642]\n",
      "Loss: 56529.2852, lp_mean: 56529.2852, lp_var: 89131526914048.0000, lp_std: 94409.4922\n",
      "Epoch [4/1000]\n",
      "Train Loss : 573.600098, Test Loss: 10626154496.0000\n",
      "Epoch [5/1000], BatchStep[100/6642]\n",
      "Loss: 20461.5508, lp_mean: 20461.5508, lp_var: 11845446402048.0000, lp_std: 34417.2148\n",
      "Epoch [5/1000], BatchStep[200/6642]\n",
      "Loss: 0.0000, lp_mean: 0.0000, lp_var: 0.0000, lp_std: 0.0000\n",
      "Epoch [5/1000], BatchStep[300/6642]\n",
      "Loss: 9962.3154, lp_mean: 9962.3154, lp_var: 2836989214720.0000, lp_std: 16843.3652\n",
      "Epoch [5/1000], BatchStep[400/6642]\n",
      "Loss: 1363.1375, lp_mean: 1363.1375, lp_var: 50593800192.0000, lp_std: 2249.3066\n",
      "Epoch [5/1000], BatchStep[500/6642]\n",
      "Loss: 4223.9131, lp_mean: 4223.9131, lp_var: 460450824192.0000, lp_std: 6785.6528\n",
      "Epoch [5/1000], BatchStep[600/6642]\n",
      "Loss: 0.4429, lp_mean: 0.4429, lp_var: 7339.2686, lp_std: 0.8567\n",
      "Epoch [5/1000], BatchStep[700/6642]\n",
      "Loss: 128294.7969, lp_mean: 128294.7969, lp_var: 452669084794880.0000, lp_std: 212760.2188\n",
      "Epoch [5/1000], BatchStep[800/6642]\n",
      "Loss: 318.9811, lp_mean: 318.9811, lp_var: 2831417856.0000, lp_std: 532.1107\n",
      "Epoch [5/1000], BatchStep[900/6642]\n",
      "Loss: 89.1404, lp_mean: 89.1404, lp_var: 209429360.0000, lp_std: 144.7168\n",
      "Epoch [5/1000], BatchStep[1000/6642]\n",
      "Loss: 3664.3186, lp_mean: 3664.3186, lp_var: 374110224384.0000, lp_std: 6116.4551\n",
      "Epoch [5/1000], BatchStep[1100/6642]\n",
      "Loss: 41.3456, lp_mean: 41.3456, lp_var: 48187812.0000, lp_std: 69.4174\n",
      "Epoch [5/1000], BatchStep[1200/6642]\n",
      "Loss: 2585.0022, lp_mean: 2585.0022, lp_var: 190454300672.0000, lp_std: 4364.1069\n",
      "Epoch [5/1000], BatchStep[1300/6642]\n",
      "Loss: 111.3556, lp_mean: 111.3556, lp_var: 337609152.0000, lp_std: 183.7414\n",
      "Epoch [5/1000], BatchStep[1400/6642]\n",
      "Loss: 247656.5625, lp_mean: 247656.5625, lp_var: 1681020403318784.0000, lp_std: 410002.5000\n",
      "Epoch [5/1000], BatchStep[1500/6642]\n",
      "Loss: 0.0042, lp_mean: 0.0042, lp_var: 0.1960, lp_std: 0.0044\n",
      "Epoch [5/1000], BatchStep[1600/6642]\n",
      "Loss: 2878.2009, lp_mean: 2878.2009, lp_var: 234364190720.0000, lp_std: 4841.1177\n",
      "Epoch [5/1000], BatchStep[1700/6642]\n",
      "Loss: 649.9122, lp_mean: 649.9122, lp_var: 12644238336.0000, lp_std: 1124.4659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/1000], BatchStep[1800/6642]\n",
      "Loss: 5883.4727, lp_mean: 5883.4727, lp_var: 992461783040.0000, lp_std: 9962.2383\n",
      "Epoch [5/1000], BatchStep[1900/6642]\n",
      "Loss: 1972.0753, lp_mean: 1972.0753, lp_var: 118830448640.0000, lp_std: 3447.1792\n",
      "Epoch [5/1000], BatchStep[2000/6642]\n",
      "Loss: 4.3939, lp_mean: 4.3939, lp_var: 799987.7500, lp_std: 8.9442\n",
      "Epoch [5/1000], BatchStep[2100/6642]\n",
      "Loss: 109005.9219, lp_mean: 109005.9219, lp_var: 340653384925184.0000, lp_std: 184567.9844\n",
      "Epoch [5/1000], BatchStep[2200/6642]\n",
      "Loss: 5.9508, lp_mean: 5.9508, lp_var: 856769.8750, lp_std: 9.2562\n",
      "Epoch [5/1000], BatchStep[2300/6642]\n",
      "Loss: 6.4438, lp_mean: 6.4438, lp_var: 1236853.2500, lp_std: 11.1214\n",
      "Epoch [5/1000], BatchStep[2400/6642]\n",
      "Loss: 2400.1077, lp_mean: 2400.1077, lp_var: 163007021056.0000, lp_std: 4037.4128\n",
      "Epoch [5/1000], BatchStep[2500/6642]\n",
      "Loss: 2111.2822, lp_mean: 2111.2822, lp_var: 122043850752.0000, lp_std: 3493.4775\n",
      "Epoch [5/1000], BatchStep[2600/6642]\n",
      "Loss: 309.7025, lp_mean: 309.7025, lp_var: 2951909888.0000, lp_std: 543.3148\n",
      "Epoch [5/1000], BatchStep[2700/6642]\n",
      "Loss: 59673.6211, lp_mean: 59673.6211, lp_var: 97628960325632.0000, lp_std: 98807.3672\n",
      "Epoch [5/1000], BatchStep[2800/6642]\n",
      "Loss: 16194.5127, lp_mean: 16194.5127, lp_var: 7287396630528.0000, lp_std: 26995.1797\n",
      "Epoch [5/1000], BatchStep[2900/6642]\n",
      "Loss: 53288.8008, lp_mean: 53288.8008, lp_var: 77867278925824.0000, lp_std: 88242.4453\n",
      "Epoch [5/1000], BatchStep[3000/6642]\n",
      "Loss: 24607.0137, lp_mean: 24607.0137, lp_var: 17218003795968.0000, lp_std: 41494.5820\n",
      "Epoch [5/1000], BatchStep[3100/6642]\n",
      "Loss: 7477.7979, lp_mean: 7477.7979, lp_var: 1510797606912.0000, lp_std: 12291.4502\n",
      "Epoch [5/1000], BatchStep[3200/6642]\n",
      "Loss: 4.4438, lp_mean: 4.4438, lp_var: 672633.0000, lp_std: 8.2014\n",
      "Epoch [5/1000], BatchStep[3300/6642]\n",
      "Loss: 12950.0049, lp_mean: 12950.0049, lp_var: 4723209404416.0000, lp_std: 21732.9453\n",
      "Epoch [5/1000], BatchStep[3400/6642]\n",
      "Loss: 306.9546, lp_mean: 306.9546, lp_var: 3007838464.0000, lp_std: 548.4376\n",
      "Epoch [5/1000], BatchStep[3500/6642]\n",
      "Loss: 0.0607, lp_mean: 0.0607, lp_var: 173.4454, lp_std: 0.1317\n",
      "Epoch [5/1000], BatchStep[3600/6642]\n",
      "Loss: 2218.7917, lp_mean: 2218.7917, lp_var: 132365271040.0000, lp_std: 3638.2041\n",
      "Epoch [5/1000], BatchStep[3700/6642]\n",
      "Loss: 1388.4926, lp_mean: 1388.4926, lp_var: 51927425024.0000, lp_std: 2278.7590\n",
      "Epoch [5/1000], BatchStep[3800/6642]\n",
      "Loss: 232.6796, lp_mean: 232.6796, lp_var: 999504896.0000, lp_std: 316.1495\n",
      "Epoch [5/1000], BatchStep[3900/6642]\n",
      "Loss: 886.7447, lp_mean: 886.7447, lp_var: 22282080256.0000, lp_std: 1492.7183\n",
      "Epoch [5/1000], BatchStep[4000/6642]\n",
      "Loss: 2557.8396, lp_mean: 2557.8396, lp_var: 188789506048.0000, lp_std: 4344.9912\n",
      "Epoch [5/1000], BatchStep[4100/6642]\n",
      "Loss: 9432.3623, lp_mean: 9432.3623, lp_var: 2386467225600.0000, lp_std: 15448.1934\n",
      "Epoch [5/1000], BatchStep[4200/6642]\n",
      "Loss: 3.1776, lp_mean: 3.1776, lp_var: 374938.5312, lp_std: 6.1232\n",
      "Epoch [5/1000], BatchStep[4300/6642]\n",
      "Loss: 1148.2048, lp_mean: 1148.2048, lp_var: 35888832512.0000, lp_std: 1894.4348\n",
      "Epoch [5/1000], BatchStep[4400/6642]\n",
      "Loss: 195619.7500, lp_mean: 195619.7500, lp_var: 1076485838340096.0000, lp_std: 328098.4375\n",
      "Epoch [5/1000], BatchStep[4500/6642]\n",
      "Loss: 1445.7847, lp_mean: 1445.7847, lp_var: 59300151296.0000, lp_std: 2435.1621\n",
      "Epoch [5/1000], BatchStep[4600/6642]\n",
      "Loss: 1029.0245, lp_mean: 1029.0245, lp_var: 30199418880.0000, lp_std: 1737.7981\n",
      "Epoch [5/1000], BatchStep[4700/6642]\n",
      "Loss: 77.1494, lp_mean: 77.1494, lp_var: 168906832.0000, lp_std: 129.9642\n",
      "Epoch [5/1000], BatchStep[4800/6642]\n",
      "Loss: 43.2526, lp_mean: 43.2526, lp_var: 50750944.0000, lp_std: 71.2397\n",
      "Epoch [5/1000], BatchStep[4900/6642]\n",
      "Loss: 0.0000, lp_mean: 0.0000, lp_var: 0.0000, lp_std: 0.0000\n",
      "Epoch [5/1000], BatchStep[5000/6642]\n",
      "Loss: 82.8181, lp_mean: 82.8181, lp_var: 136666112.0000, lp_std: 116.9043\n",
      "Epoch [5/1000], BatchStep[5100/6642]\n",
      "Loss: 10.2728, lp_mean: 10.2728, lp_var: 3368385.7500, lp_std: 18.3532\n",
      "Epoch [5/1000], BatchStep[5200/6642]\n",
      "Loss: 0.0000, lp_mean: 0.0000, lp_var: 0.0000, lp_std: 0.0000\n",
      "Epoch [5/1000], BatchStep[5300/6642]\n",
      "Loss: 174153.5312, lp_mean: 174153.5312, lp_var: 848321606320128.0000, lp_std: 291259.5938\n",
      "Epoch [5/1000], BatchStep[5400/6642]\n",
      "Loss: 42.2880, lp_mean: 42.2880, lp_var: 47322624.0000, lp_std: 68.7914\n",
      "Epoch [5/1000], BatchStep[5500/6642]\n",
      "Loss: 7.4690, lp_mean: 7.4690, lp_var: 1830379.3750, lp_std: 13.5292\n",
      "Epoch [5/1000], BatchStep[5600/6642]\n",
      "Loss: 26454.4414, lp_mean: 26454.4414, lp_var: 18934256369664.0000, lp_std: 43513.5078\n",
      "Epoch [5/1000], BatchStep[5700/6642]\n",
      "Loss: 131.0171, lp_mean: 131.0171, lp_var: 428874304.0000, lp_std: 207.0928\n",
      "Epoch [5/1000], BatchStep[5800/6642]\n",
      "Loss: 21494.5371, lp_mean: 21494.5371, lp_var: 12863408177152.0000, lp_std: 35865.5938\n",
      "Epoch [5/1000], BatchStep[5900/6642]\n",
      "Loss: 1.1662, lp_mean: 1.1662, lp_var: 26985.1230, lp_std: 1.6427\n",
      "Epoch [5/1000], BatchStep[6000/6642]\n",
      "Loss: 10130.6299, lp_mean: 10130.6299, lp_var: 2839712366592.0000, lp_std: 16851.4453\n",
      "Epoch [5/1000], BatchStep[6100/6642]\n",
      "Loss: 4351.6143, lp_mean: 4351.6143, lp_var: 532917813248.0000, lp_std: 7300.1221\n",
      "Epoch [5/1000], BatchStep[6200/6642]\n",
      "Loss: 23399.0762, lp_mean: 23399.0762, lp_var: 15483157872640.0000, lp_std: 39348.6445\n",
      "Epoch [5/1000], BatchStep[6300/6642]\n",
      "Loss: 7509.3779, lp_mean: 7509.3779, lp_var: 1539349676032.0000, lp_std: 12407.0537\n",
      "Epoch [5/1000], BatchStep[6400/6642]\n",
      "Loss: 4591.6333, lp_mean: 4591.6333, lp_var: 596451917824.0000, lp_std: 7723.0293\n",
      "Epoch [5/1000], BatchStep[6500/6642]\n",
      "Loss: 13219.8545, lp_mean: 13219.8545, lp_var: 4961054228480.0000, lp_std: 22273.4238\n",
      "Epoch [5/1000], BatchStep[6600/6642]\n",
      "Loss: 386296.6875, lp_mean: 386296.6875, lp_var: 4135119887532032.0000, lp_std: 643049.0000\n",
      "Epoch [5/1000]\n",
      "Train Loss : 572.919128, Test Loss: 9816774656.0000\n",
      "Epoch [6/1000], BatchStep[100/6642]\n",
      "Loss: 85860.2188, lp_mean: 85860.2188, lp_var: 203523182559232.0000, lp_std: 142661.5469\n",
      "Epoch [6/1000], BatchStep[200/6642]\n",
      "Loss: 42633.2422, lp_mean: 42633.2422, lp_var: 51651368976384.0000, lp_std: 71868.8828\n",
      "Epoch [6/1000], BatchStep[300/6642]\n",
      "Loss: 415.9705, lp_mean: 415.9705, lp_var: 4749264384.0000, lp_std: 689.1491\n",
      "Epoch [6/1000], BatchStep[400/6642]\n",
      "Loss: 0.1617, lp_mean: 0.1617, lp_var: 947.6253, lp_std: 0.3078\n",
      "Epoch [6/1000], BatchStep[500/6642]\n",
      "Loss: 2008.5718, lp_mean: 2008.5718, lp_var: 112920961024.0000, lp_std: 3360.3716\n",
      "Epoch [6/1000], BatchStep[600/6642]\n",
      "Loss: 2.9273, lp_mean: 2.9273, lp_var: 266045.1875, lp_std: 5.1580\n",
      "Epoch [6/1000], BatchStep[700/6642]\n",
      "Loss: 90.6317, lp_mean: 90.6317, lp_var: 233196608.0000, lp_std: 152.7078\n",
      "Epoch [6/1000], BatchStep[800/6642]\n",
      "Loss: 93444.9766, lp_mean: 93444.9766, lp_var: 236977152589824.0000, lp_std: 153940.6250\n",
      "Epoch [6/1000], BatchStep[900/6642]\n",
      "Loss: 5205.2710, lp_mean: 5205.2710, lp_var: 792686886912.0000, lp_std: 8903.2969\n",
      "Epoch [6/1000], BatchStep[1000/6642]\n",
      "Loss: 333226.8125, lp_mean: 333226.8125, lp_var: 3075866061963264.0000, lp_std: 554604.9375\n",
      "Epoch [6/1000], BatchStep[1100/6642]\n",
      "Loss: 36.1036, lp_mean: 36.1036, lp_var: 35246744.0000, lp_std: 59.3690\n",
      "Epoch [6/1000], BatchStep[1200/6642]\n",
      "Loss: 14681.8223, lp_mean: 14681.8223, lp_var: 5996047499264.0000, lp_std: 24486.8281\n",
      "Epoch [6/1000], BatchStep[1300/6642]\n",
      "Loss: 120.6017, lp_mean: 120.6017, lp_var: 356631584.0000, lp_std: 188.8469\n",
      "Epoch [6/1000], BatchStep[1400/6642]\n",
      "Loss: 59303.5273, lp_mean: 59303.5273, lp_var: 98902460071936.0000, lp_std: 99449.7109\n",
      "Epoch [6/1000], BatchStep[1500/6642]\n",
      "Loss: 1.4874, lp_mean: 1.4874, lp_var: 82351.9766, lp_std: 2.8697\n",
      "Epoch [6/1000], BatchStep[1600/6642]\n",
      "Loss: 1450.9674, lp_mean: 1450.9674, lp_var: 56031449088.0000, lp_std: 2367.0962\n",
      "Epoch [6/1000], BatchStep[1700/6642]\n",
      "Loss: 355.2648, lp_mean: 355.2648, lp_var: 3547868928.0000, lp_std: 595.6399\n",
      "Epoch [6/1000], BatchStep[1800/6642]\n",
      "Loss: 51766.4141, lp_mean: 51766.4141, lp_var: 78653157277696.0000, lp_std: 88686.6094\n",
      "Epoch [6/1000], BatchStep[1900/6642]\n",
      "Loss: 0.2443, lp_mean: 0.2443, lp_var: 2506.4517, lp_std: 0.5006\n",
      "Epoch [6/1000], BatchStep[2000/6642]\n",
      "Loss: 356415.8438, lp_mean: 356415.8438, lp_var: 3451550609440768.0000, lp_std: 587499.0000\n",
      "Epoch [6/1000], BatchStep[2100/6642]\n",
      "Loss: 20.6208, lp_mean: 20.6208, lp_var: 12561654.0000, lp_std: 35.4424\n",
      "Epoch [6/1000], BatchStep[2200/6642]\n",
      "Loss: 0.1440, lp_mean: 0.1440, lp_var: 237.7698, lp_std: 0.1542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/1000], BatchStep[2300/6642]\n",
      "Loss: 72.6892, lp_mean: 72.6892, lp_var: 84349960.0000, lp_std: 91.8422\n",
      "Epoch [6/1000], BatchStep[2400/6642]\n",
      "Loss: 541.2911, lp_mean: 541.2911, lp_var: 8817639424.0000, lp_std: 939.0229\n",
      "Epoch [6/1000], BatchStep[2500/6642]\n",
      "Loss: 2.0376, lp_mean: 2.0376, lp_var: 71362.5859, lp_std: 2.6714\n",
      "Epoch [6/1000], BatchStep[2600/6642]\n",
      "Loss: 0.1386, lp_mean: 0.1386, lp_var: 664.6891, lp_std: 0.2578\n",
      "Epoch [6/1000], BatchStep[2700/6642]\n",
      "Loss: 57880.5039, lp_mean: 57880.5039, lp_var: 93860403347456.0000, lp_std: 96881.5781\n",
      "Epoch [6/1000], BatchStep[2800/6642]\n",
      "Loss: 33.2113, lp_mean: 33.2113, lp_var: 34520640.0000, lp_std: 58.7543\n",
      "Epoch [6/1000], BatchStep[2900/6642]\n",
      "Loss: 696.0555, lp_mean: 696.0555, lp_var: 13837093888.0000, lp_std: 1176.3118\n",
      "Epoch [6/1000], BatchStep[3000/6642]\n",
      "Loss: 1.8367, lp_mean: 1.8367, lp_var: 72040.9062, lp_std: 2.6840\n",
      "Epoch [6/1000], BatchStep[3100/6642]\n",
      "Loss: 16.8265, lp_mean: 16.8265, lp_var: 8585592.0000, lp_std: 29.3012\n",
      "Epoch [6/1000], BatchStep[3200/6642]\n",
      "Loss: 79210.8203, lp_mean: 79210.8203, lp_var: 175898355564544.0000, lp_std: 132626.6719\n",
      "Epoch [6/1000], BatchStep[3300/6642]\n",
      "Loss: 4037.0364, lp_mean: 4037.0364, lp_var: 459328618496.0000, lp_std: 6777.3784\n",
      "Epoch [6/1000], BatchStep[3400/6642]\n",
      "Loss: 73963.1016, lp_mean: 73963.1016, lp_var: 152757390540800.0000, lp_std: 123595.0625\n",
      "Epoch [6/1000], BatchStep[3500/6642]\n",
      "Loss: 22499.0098, lp_mean: 22499.0098, lp_var: 14302038720512.0000, lp_std: 37818.0391\n",
      "Epoch [6/1000], BatchStep[3600/6642]\n",
      "Loss: 0.0064, lp_mean: 0.0064, lp_var: 1.6078, lp_std: 0.0127\n",
      "Epoch [6/1000], BatchStep[3700/6642]\n",
      "Loss: 0.3916, lp_mean: 0.3916, lp_var: 2574.9973, lp_std: 0.5074\n",
      "Epoch [6/1000], BatchStep[3800/6642]\n",
      "Loss: 892.1173, lp_mean: 892.1173, lp_var: 23075516416.0000, lp_std: 1519.0627\n",
      "Epoch [6/1000], BatchStep[3900/6642]\n",
      "Loss: 77479.6953, lp_mean: 77479.6953, lp_var: 168241720721408.0000, lp_std: 129708.0234\n",
      "Epoch [6/1000], BatchStep[4000/6642]\n",
      "Loss: 6.4870, lp_mean: 6.4870, lp_var: 1593566.3750, lp_std: 12.6237\n",
      "Epoch [6/1000], BatchStep[4100/6642]\n",
      "Loss: 43918.3945, lp_mean: 43918.3945, lp_var: 52768341491712.0000, lp_std: 72641.8203\n",
      "Epoch [6/1000], BatchStep[4200/6642]\n",
      "Loss: 0.9846, lp_mean: 0.9846, lp_var: 28160.2695, lp_std: 1.6781\n",
      "Epoch [6/1000], BatchStep[4300/6642]\n",
      "Loss: 52.6431, lp_mean: 52.6431, lp_var: 77244624.0000, lp_std: 87.8889\n",
      "Epoch [6/1000], BatchStep[4400/6642]\n",
      "Loss: 4564.1431, lp_mean: 4564.1431, lp_var: 548583079936.0000, lp_std: 7406.6396\n",
      "Epoch [6/1000], BatchStep[4500/6642]\n",
      "Loss: 0.2144, lp_mean: 0.2144, lp_var: 1200.7435, lp_std: 0.3465\n",
      "Epoch [6/1000], BatchStep[4600/6642]\n",
      "Loss: 204747.3438, lp_mean: 204747.3438, lp_var: 1150544563404800.0000, lp_std: 339196.7812\n",
      "Epoch [6/1000], BatchStep[4700/6642]\n",
      "Loss: 2595.6968, lp_mean: 2595.6968, lp_var: 183589339136.0000, lp_std: 4284.7329\n",
      "Epoch [6/1000], BatchStep[4800/6642]\n",
      "Loss: 19950.3887, lp_mean: 19950.3887, lp_var: 11548555739136.0000, lp_std: 33983.1641\n",
      "Epoch [6/1000], BatchStep[4900/6642]\n",
      "Loss: 70865.5625, lp_mean: 70865.5625, lp_var: 141388092014592.0000, lp_std: 118906.7266\n",
      "Epoch [6/1000], BatchStep[5000/6642]\n",
      "Loss: 60.0836, lp_mean: 60.0836, lp_var: 108920360.0000, lp_std: 104.3649\n",
      "Epoch [6/1000], BatchStep[5100/6642]\n",
      "Loss: 24.1881, lp_mean: 24.1881, lp_var: 18403024.0000, lp_std: 42.8987\n",
      "Epoch [6/1000], BatchStep[5200/6642]\n",
      "Loss: 5.5484, lp_mean: 5.5484, lp_var: 671606.4375, lp_std: 8.1952\n",
      "Epoch [6/1000], BatchStep[5300/6642]\n",
      "Loss: 87474.4609, lp_mean: 87474.4609, lp_var: 215192759697408.0000, lp_std: 146694.5000\n",
      "Epoch [6/1000], BatchStep[5400/6642]\n",
      "Loss: 0.0137, lp_mean: 0.0137, lp_var: 6.1622, lp_std: 0.0248\n",
      "Epoch [6/1000], BatchStep[5500/6642]\n",
      "Loss: 875.5605, lp_mean: 875.5605, lp_var: 22481586176.0000, lp_std: 1499.3861\n",
      "Epoch [6/1000], BatchStep[5600/6642]\n",
      "Loss: 585.6750, lp_mean: 585.6750, lp_var: 7914287616.0000, lp_std: 889.6229\n",
      "Epoch [6/1000], BatchStep[5700/6642]\n",
      "Loss: 6465.5889, lp_mean: 6465.5889, lp_var: 1178866286592.0000, lp_std: 10857.5615\n",
      "Epoch [6/1000], BatchStep[5800/6642]\n",
      "Loss: 115703.4297, lp_mean: 115703.4297, lp_var: 358018944335872.0000, lp_std: 189213.8906\n",
      "Epoch [6/1000], BatchStep[5900/6642]\n",
      "Loss: 480800.6875, lp_mean: 480800.6875, lp_var: 6587373877985280.0000, lp_std: 811626.4375\n",
      "Epoch [6/1000], BatchStep[6000/6642]\n",
      "Loss: 0.0054, lp_mean: 0.0054, lp_var: 0.7048, lp_std: 0.0084\n",
      "Epoch [6/1000], BatchStep[6100/6642]\n",
      "Loss: 308.4827, lp_mean: 308.4827, lp_var: 2367949056.0000, lp_std: 486.6158\n",
      "Epoch [6/1000], BatchStep[6200/6642]\n",
      "Loss: 65539.3203, lp_mean: 65539.3203, lp_var: 120087059103744.0000, lp_std: 109584.2422\n",
      "Epoch [6/1000], BatchStep[6300/6642]\n",
      "Loss: 199.2033, lp_mean: 199.2033, lp_var: 954298176.0000, lp_std: 308.9171\n",
      "Epoch [6/1000], BatchStep[6400/6642]\n",
      "Loss: 241755.6719, lp_mean: 241755.6719, lp_var: 1663599076442112.0000, lp_std: 407872.4375\n",
      "Epoch [6/1000], BatchStep[6500/6642]\n",
      "Loss: 24036.6035, lp_mean: 24036.6035, lp_var: 16452934434816.0000, lp_std: 40562.2148\n",
      "Epoch [6/1000], BatchStep[6600/6642]\n",
      "Loss: 1474.3618, lp_mean: 1474.3618, lp_var: 58483752960.0000, lp_std: 2418.3416\n",
      "Epoch [6/1000]\n",
      "Train Loss : 546.418335, Test Loss: 9880971264.0000\n",
      "Epoch [7/1000], BatchStep[100/6642]\n",
      "Loss: 333.5300, lp_mean: 333.5300, lp_var: 3456200448.0000, lp_std: 587.8946\n",
      "Epoch [7/1000], BatchStep[200/6642]\n",
      "Loss: 10787.5020, lp_mean: 10787.5020, lp_var: 3250597658624.0000, lp_std: 18029.4141\n",
      "Epoch [7/1000], BatchStep[300/6642]\n",
      "Loss: 0.0000, lp_mean: 0.0000, lp_var: 0.0000, lp_std: 0.0000\n",
      "Epoch [7/1000], BatchStep[400/6642]\n",
      "Loss: 0.0705, lp_mean: 0.0705, lp_var: 124.7631, lp_std: 0.1117\n",
      "Epoch [7/1000], BatchStep[500/6642]\n",
      "Loss: 236138.0000, lp_mean: 236138.0000, lp_var: 1581094096863232.0000, lp_std: 397629.7500\n",
      "Epoch [7/1000], BatchStep[600/6642]\n",
      "Loss: 4143.1904, lp_mean: 4143.1904, lp_var: 469863661568.0000, lp_std: 6854.6602\n",
      "Epoch [7/1000], BatchStep[700/6642]\n",
      "Loss: 551.3287, lp_mean: 551.3287, lp_var: 8303148544.0000, lp_std: 911.2161\n",
      "Epoch [7/1000], BatchStep[800/6642]\n",
      "Loss: 93985.7266, lp_mean: 93985.7266, lp_var: 241328843653120.0000, lp_std: 155347.6250\n",
      "Epoch [7/1000], BatchStep[900/6642]\n",
      "Loss: 519950.2188, lp_mean: 519950.2188, lp_var: 7373425370726400.0000, lp_std: 858686.5000\n",
      "Epoch [7/1000], BatchStep[1000/6642]\n",
      "Loss: 17815.6621, lp_mean: 17815.6621, lp_var: 9630782586880.0000, lp_std: 31033.5059\n",
      "Epoch [7/1000], BatchStep[1100/6642]\n",
      "Loss: 20700.9473, lp_mean: 20700.9473, lp_var: 12026069909504.0000, lp_std: 34678.6250\n",
      "Epoch [7/1000], BatchStep[1200/6642]\n",
      "Loss: 0.0335, lp_mean: 0.0335, lp_var: 16.8696, lp_std: 0.0411\n",
      "Epoch [7/1000], BatchStep[1300/6642]\n",
      "Loss: 3467.9854, lp_mean: 3467.9854, lp_var: 323421798400.0000, lp_std: 5687.0186\n",
      "Epoch [7/1000], BatchStep[1400/6642]\n",
      "Loss: 14068.1592, lp_mean: 14068.1592, lp_var: 5522819907584.0000, lp_std: 23500.6816\n",
      "Epoch [7/1000], BatchStep[1500/6642]\n",
      "Loss: 0.9676, lp_mean: 0.9676, lp_var: 8924.1963, lp_std: 0.9447\n",
      "Epoch [7/1000], BatchStep[1600/6642]\n",
      "Loss: 35061.1328, lp_mean: 35061.1328, lp_var: 34154590765056.0000, lp_std: 58441.9336\n",
      "Epoch [7/1000], BatchStep[1700/6642]\n",
      "Loss: 0.0004, lp_mean: 0.0004, lp_var: 0.0031, lp_std: 0.0006\n",
      "Epoch [7/1000], BatchStep[1800/6642]\n",
      "Loss: 42.4360, lp_mean: 42.4360, lp_var: 38524320.0000, lp_std: 62.0680\n",
      "Epoch [7/1000], BatchStep[1900/6642]\n",
      "Loss: 9.5948, lp_mean: 9.5948, lp_var: 2417920.5000, lp_std: 15.5497\n",
      "Epoch [7/1000], BatchStep[2000/6642]\n",
      "Loss: 5907.8730, lp_mean: 5907.8730, lp_var: 1003502108672.0000, lp_std: 10017.4951\n",
      "Epoch [7/1000], BatchStep[2100/6642]\n",
      "Loss: 9730.6309, lp_mean: 9730.6309, lp_var: 2737362436096.0000, lp_std: 16544.9766\n",
      "Epoch [7/1000], BatchStep[2200/6642]\n",
      "Loss: 2.2548, lp_mean: 2.2548, lp_var: 128359.9609, lp_std: 3.5827\n",
      "Epoch [7/1000], BatchStep[2300/6642]\n",
      "Loss: 227500.9688, lp_mean: 227500.9688, lp_var: 1433290179346432.0000, lp_std: 378588.1875\n",
      "Epoch [7/1000], BatchStep[2400/6642]\n",
      "Loss: 9.7975, lp_mean: 9.7975, lp_var: 2793027.5000, lp_std: 16.7124\n",
      "Epoch [7/1000], BatchStep[2500/6642]\n",
      "Loss: 48932.6797, lp_mean: 48932.6797, lp_var: 61121989967872.0000, lp_std: 78180.5469\n",
      "Epoch [7/1000], BatchStep[2600/6642]\n",
      "Loss: 2417.1433, lp_mean: 2417.1433, lp_var: 163872145408.0000, lp_std: 4048.1125\n",
      "Epoch [7/1000], BatchStep[2700/6642]\n",
      "Loss: 15.7352, lp_mean: 15.7352, lp_var: 7798487.5000, lp_std: 27.9258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/1000], BatchStep[2800/6642]\n",
      "Loss: 19215.9629, lp_mean: 19215.9629, lp_var: 10508298092544.0000, lp_std: 32416.5039\n",
      "Epoch [7/1000], BatchStep[2900/6642]\n",
      "Loss: 896.7275, lp_mean: 896.7275, lp_var: 19749351424.0000, lp_std: 1405.3239\n",
      "Epoch [7/1000], BatchStep[3000/6642]\n",
      "Loss: 5.0611, lp_mean: 5.0611, lp_var: 648523.9375, lp_std: 8.0531\n",
      "Epoch [7/1000], BatchStep[3100/6642]\n",
      "Loss: 12017.5088, lp_mean: 12017.5088, lp_var: 3959938613248.0000, lp_std: 19899.5938\n",
      "Epoch [7/1000], BatchStep[3200/6642]\n",
      "Loss: 16.5253, lp_mean: 16.5253, lp_var: 7566417.5000, lp_std: 27.5071\n",
      "Epoch [7/1000], BatchStep[3300/6642]\n",
      "Loss: 12.9270, lp_mean: 12.9270, lp_var: 5892545.5000, lp_std: 24.2746\n",
      "Epoch [7/1000], BatchStep[3400/6642]\n",
      "Loss: 4373.1040, lp_mean: 4373.1040, lp_var: 543935922176.0000, lp_std: 7375.2012\n",
      "Epoch [7/1000], BatchStep[3500/6642]\n",
      "Loss: 10043.0352, lp_mean: 10043.0352, lp_var: 2866850299904.0000, lp_std: 16931.7754\n",
      "Epoch [7/1000], BatchStep[3600/6642]\n",
      "Loss: 9172.7451, lp_mean: 9172.7451, lp_var: 2296047206400.0000, lp_std: 15152.7129\n",
      "Epoch [7/1000], BatchStep[3700/6642]\n",
      "Loss: 101531.3438, lp_mean: 101531.3438, lp_var: 291052451790848.0000, lp_std: 170602.5781\n",
      "Epoch [7/1000], BatchStep[3800/6642]\n",
      "Loss: 45.6495, lp_mean: 45.6495, lp_var: 58207472.0000, lp_std: 76.2938\n",
      "Epoch [7/1000], BatchStep[3900/6642]\n",
      "Loss: 426979.6250, lp_mean: 426979.6250, lp_var: 5081608809873408.0000, lp_std: 712854.0000\n",
      "Epoch [7/1000], BatchStep[4000/6642]\n",
      "Loss: 24.4175, lp_mean: 24.4175, lp_var: 18437246.0000, lp_std: 42.9386\n",
      "Epoch [7/1000], BatchStep[4100/6642]\n",
      "Loss: 93173.1484, lp_mean: 93173.1484, lp_var: 240712616509440.0000, lp_std: 155149.1562\n",
      "Epoch [7/1000], BatchStep[4200/6642]\n",
      "Loss: 0.0078, lp_mean: 0.0078, lp_var: 1.3501, lp_std: 0.0116\n",
      "Epoch [7/1000], BatchStep[4300/6642]\n",
      "Loss: 291.5305, lp_mean: 291.5305, lp_var: 2409132544.0000, lp_std: 490.8292\n",
      "Epoch [7/1000], BatchStep[4400/6642]\n",
      "Loss: 413.9018, lp_mean: 413.9018, lp_var: 4918536704.0000, lp_std: 701.3228\n",
      "Epoch [7/1000], BatchStep[4500/6642]\n",
      "Loss: 650.9535, lp_mean: 650.9535, lp_var: 11734219776.0000, lp_std: 1083.2460\n",
      "Epoch [7/1000], BatchStep[4600/6642]\n",
      "Loss: 3466.7246, lp_mean: 3466.7246, lp_var: 333415743488.0000, lp_std: 5774.2158\n",
      "Epoch [7/1000], BatchStep[4700/6642]\n",
      "Loss: 400.2297, lp_mean: 400.2297, lp_var: 5035888640.0000, lp_std: 709.6400\n",
      "Epoch [7/1000], BatchStep[4800/6642]\n",
      "Loss: 178.4963, lp_mean: 178.4963, lp_var: 841778880.0000, lp_std: 290.1342\n",
      "Epoch [7/1000], BatchStep[4900/6642]\n",
      "Loss: 68.3459, lp_mean: 68.3459, lp_var: 137535568.0000, lp_std: 117.2756\n",
      "Epoch [7/1000], BatchStep[5000/6642]\n",
      "Loss: 41523.0234, lp_mean: 41523.0234, lp_var: 48955417165824.0000, lp_std: 69968.1484\n",
      "Epoch [7/1000], BatchStep[5100/6642]\n",
      "Loss: 1996.6511, lp_mean: 1996.6511, lp_var: 112589455360.0000, lp_std: 3355.4353\n",
      "Epoch [7/1000], BatchStep[5200/6642]\n",
      "Loss: 12218.4863, lp_mean: 12218.4863, lp_var: 4236023693312.0000, lp_std: 20581.6016\n",
      "Epoch [7/1000], BatchStep[5300/6642]\n",
      "Loss: 13528.1045, lp_mean: 13528.1045, lp_var: 5090346795008.0000, lp_std: 22561.7949\n",
      "Epoch [7/1000], BatchStep[5400/6642]\n",
      "Loss: 822.9869, lp_mean: 822.9869, lp_var: 18751870976.0000, lp_std: 1369.3748\n",
      "Epoch [7/1000], BatchStep[5500/6642]\n",
      "Loss: 1838.4669, lp_mean: 1838.4669, lp_var: 90051878912.0000, lp_std: 3000.8645\n",
      "Epoch [7/1000], BatchStep[5600/6642]\n",
      "Loss: 3436.0391, lp_mean: 3436.0391, lp_var: 334991687680.0000, lp_std: 5787.8467\n",
      "Epoch [7/1000], BatchStep[5700/6642]\n",
      "Loss: 142.1141, lp_mean: 142.1141, lp_var: 599702080.0000, lp_std: 244.8881\n",
      "Epoch [7/1000], BatchStep[5800/6642]\n",
      "Loss: 120455.1797, lp_mean: 120455.1797, lp_var: 410881066270720.0000, lp_std: 202702.0156\n",
      "Epoch [7/1000], BatchStep[5900/6642]\n",
      "Loss: 45418.6406, lp_mean: 45418.6406, lp_var: 60064190693376.0000, lp_std: 77501.0938\n",
      "Epoch [7/1000], BatchStep[6000/6642]\n",
      "Loss: 0.0000, lp_mean: 0.0000, lp_var: 0.0000, lp_std: 0.0000\n",
      "Epoch [7/1000], BatchStep[6100/6642]\n",
      "Loss: 21.3926, lp_mean: 21.3926, lp_var: 16502555.0000, lp_std: 40.6233\n",
      "Epoch [7/1000], BatchStep[6200/6642]\n",
      "Loss: 38358.8633, lp_mean: 38358.8633, lp_var: 41128518746112.0000, lp_std: 64131.5195\n",
      "Epoch [7/1000], BatchStep[6300/6642]\n",
      "Loss: 363.6293, lp_mean: 363.6293, lp_var: 3248567296.0000, lp_std: 569.9620\n",
      "Epoch [7/1000], BatchStep[6400/6642]\n",
      "Loss: 610.6562, lp_mean: 610.6562, lp_var: 8385848832.0000, lp_std: 915.7429\n",
      "Epoch [7/1000], BatchStep[6500/6642]\n",
      "Loss: 3113.0066, lp_mean: 3113.0066, lp_var: 270659338240.0000, lp_std: 5202.4932\n",
      "Epoch [7/1000], BatchStep[6600/6642]\n",
      "Loss: 5.6177, lp_mean: 5.6177, lp_var: 1104254.6250, lp_std: 10.5084\n",
      "Epoch [7/1000]\n",
      "Train Loss : 594.124146, Test Loss: 8596324352.0000\n",
      "Epoch [8/1000], BatchStep[100/6642]\n",
      "Loss: 0.0097, lp_mean: 0.0097, lp_var: 1.6764, lp_std: 0.0129\n",
      "Epoch [8/1000], BatchStep[200/6642]\n",
      "Loss: 4221.3784, lp_mean: 4221.3784, lp_var: 490682482688.0000, lp_std: 7004.8735\n",
      "Epoch [8/1000], BatchStep[300/6642]\n",
      "Loss: 2320.5623, lp_mean: 2320.5623, lp_var: 159086215168.0000, lp_std: 3988.5610\n",
      "Epoch [8/1000], BatchStep[400/6642]\n",
      "Loss: 21.4184, lp_mean: 21.4184, lp_var: 18595040.0000, lp_std: 43.1220\n",
      "Epoch [8/1000], BatchStep[500/6642]\n",
      "Loss: 0.6507, lp_mean: 0.6507, lp_var: 20751.2031, lp_std: 1.4405\n",
      "Epoch [8/1000], BatchStep[600/6642]\n",
      "Loss: 201378.0312, lp_mean: 201378.0312, lp_var: 1115500247515136.0000, lp_std: 333991.0312\n",
      "Epoch [8/1000], BatchStep[700/6642]\n",
      "Loss: 0.2835, lp_mean: 0.2835, lp_var: 1300.3533, lp_std: 0.3606\n",
      "Epoch [8/1000], BatchStep[800/6642]\n",
      "Loss: 2963.3120, lp_mean: 2963.3120, lp_var: 247824646144.0000, lp_std: 4978.1992\n",
      "Epoch [8/1000], BatchStep[900/6642]\n",
      "Loss: 87.4203, lp_mean: 87.4203, lp_var: 180696272.0000, lp_std: 134.4233\n",
      "Epoch [8/1000], BatchStep[1000/6642]\n",
      "Loss: 30024.0566, lp_mean: 30024.0566, lp_var: 25281830060032.0000, lp_std: 50281.0430\n",
      "Epoch [8/1000], BatchStep[1100/6642]\n",
      "Loss: 347.4760, lp_mean: 347.4760, lp_var: 3351313408.0000, lp_std: 578.9053\n",
      "Epoch [8/1000], BatchStep[1200/6642]\n",
      "Loss: 2.2969, lp_mean: 2.2969, lp_var: 223830.7812, lp_std: 4.7311\n",
      "Epoch [8/1000], BatchStep[1300/6642]\n",
      "Loss: 1877.7465, lp_mean: 1877.7465, lp_var: 93727850496.0000, lp_std: 3061.5002\n",
      "Epoch [8/1000], BatchStep[1400/6642]\n",
      "Loss: 2526.8674, lp_mean: 2526.8674, lp_var: 180797030400.0000, lp_std: 4252.0234\n",
      "Epoch [8/1000], BatchStep[1500/6642]\n",
      "Loss: 39015.0586, lp_mean: 39015.0586, lp_var: 41480265662464.0000, lp_std: 64405.1758\n",
      "Epoch [8/1000], BatchStep[1600/6642]\n",
      "Loss: 0.4354, lp_mean: 0.4354, lp_var: 6970.7231, lp_std: 0.8349\n",
      "Epoch [8/1000], BatchStep[1700/6642]\n",
      "Loss: 36296.7852, lp_mean: 36296.7852, lp_var: 38316602818560.0000, lp_std: 61900.4023\n",
      "Epoch [8/1000], BatchStep[1800/6642]\n",
      "Loss: 35499.1719, lp_mean: 35499.1719, lp_var: 35349577334784.0000, lp_std: 59455.5117\n",
      "Epoch [8/1000], BatchStep[1900/6642]\n",
      "Loss: 144863.6250, lp_mean: 144863.6250, lp_var: 592585764634624.0000, lp_std: 243430.8281\n",
      "Epoch [8/1000], BatchStep[2000/6642]\n",
      "Loss: 37.1196, lp_mean: 37.1196, lp_var: 51430396.0000, lp_std: 71.7150\n",
      "Epoch [8/1000], BatchStep[2100/6642]\n",
      "Loss: 11829.6826, lp_mean: 11829.6826, lp_var: 3845547098112.0000, lp_std: 19610.0664\n",
      "Epoch [8/1000], BatchStep[2200/6642]\n",
      "Loss: 52991.8906, lp_mean: 52991.8906, lp_var: 77078548447232.0000, lp_std: 87794.3906\n",
      "Epoch [8/1000], BatchStep[2300/6642]\n",
      "Loss: 45416.5977, lp_mean: 45416.5977, lp_var: 56101793955840.0000, lp_std: 74901.1328\n",
      "Epoch [8/1000], BatchStep[2400/6642]\n",
      "Loss: 0.5443, lp_mean: 0.5443, lp_var: 8273.7363, lp_std: 0.9096\n",
      "Epoch [8/1000], BatchStep[2500/6642]\n",
      "Loss: 0.0239, lp_mean: 0.0239, lp_var: 19.4806, lp_std: 0.0441\n",
      "Epoch [8/1000], BatchStep[2600/6642]\n",
      "Loss: 0.0002, lp_mean: 0.0002, lp_var: 0.0009, lp_std: 0.0003\n",
      "Epoch [8/1000], BatchStep[2700/6642]\n",
      "Loss: 360.3917, lp_mean: 360.3917, lp_var: 3986557952.0000, lp_std: 631.3920\n",
      "Epoch [8/1000], BatchStep[2800/6642]\n",
      "Loss: 7184.9409, lp_mean: 7184.9409, lp_var: 1449974956032.0000, lp_std: 12041.4902\n",
      "Epoch [8/1000], BatchStep[2900/6642]\n",
      "Loss: 2.7572, lp_mean: 2.7572, lp_var: 195947.2500, lp_std: 4.4266\n",
      "Epoch [8/1000], BatchStep[3000/6642]\n",
      "Loss: 9889.5518, lp_mean: 9889.5518, lp_var: 2735980412928.0000, lp_std: 16540.8008\n",
      "Epoch [8/1000], BatchStep[3100/6642]\n",
      "Loss: 335.9966, lp_mean: 335.9966, lp_var: 3317414656.0000, lp_std: 575.9700\n",
      "Epoch [8/1000], BatchStep[3200/6642]\n",
      "Loss: 0.0000, lp_mean: 0.0000, lp_var: 0.0000, lp_std: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/1000], BatchStep[3300/6642]\n",
      "Loss: 285420.7188, lp_mean: 285420.7188, lp_var: 2256626551619584.0000, lp_std: 475039.6562\n",
      "Epoch [8/1000], BatchStep[3400/6642]\n",
      "Loss: 0.0000, lp_mean: 0.0000, lp_var: 0.0000, lp_std: 0.0000\n",
      "Epoch [8/1000], BatchStep[3500/6642]\n",
      "Loss: 75794.2578, lp_mean: 75794.2578, lp_var: 156644587601920.0000, lp_std: 125157.7422\n",
      "Epoch [8/1000], BatchStep[3600/6642]\n",
      "Loss: 438817.6562, lp_mean: 438817.6562, lp_var: 5421658550566912.0000, lp_std: 736319.1250\n",
      "Epoch [8/1000], BatchStep[3700/6642]\n",
      "Loss: 3593.2598, lp_mean: 3593.2598, lp_var: 364850577408.0000, lp_std: 6040.2866\n",
      "Epoch [8/1000], BatchStep[3800/6642]\n",
      "Loss: 839.3382, lp_mean: 839.3382, lp_var: 22145447936.0000, lp_std: 1488.1346\n",
      "Epoch [8/1000], BatchStep[3900/6642]\n",
      "Loss: 271307.9062, lp_mean: 271307.9062, lp_var: 2056355850485760.0000, lp_std: 453470.5938\n",
      "Epoch [8/1000], BatchStep[4000/6642]\n",
      "Loss: 85.7875, lp_mean: 85.7875, lp_var: 182747632.0000, lp_std: 135.1842\n",
      "Epoch [8/1000], BatchStep[4100/6642]\n",
      "Loss: 6540.1118, lp_mean: 6540.1118, lp_var: 1233768415232.0000, lp_std: 11107.5137\n",
      "Epoch [8/1000], BatchStep[4200/6642]\n",
      "Loss: 322.4870, lp_mean: 322.4870, lp_var: 2959769344.0000, lp_std: 544.0377\n",
      "Epoch [8/1000], BatchStep[4300/6642]\n",
      "Loss: 703.4019, lp_mean: 703.4019, lp_var: 14752277504.0000, lp_std: 1214.5896\n",
      "Epoch [8/1000], BatchStep[4400/6642]\n",
      "Loss: 94.9952, lp_mean: 94.9952, lp_var: 240140736.0000, lp_std: 154.9648\n",
      "Epoch [8/1000], BatchStep[4500/6642]\n",
      "Loss: 0.3204, lp_mean: 0.3204, lp_var: 3063.7747, lp_std: 0.5535\n",
      "Epoch [8/1000], BatchStep[4600/6642]\n",
      "Loss: 61444.1484, lp_mean: 61444.1484, lp_var: 102853838372864.0000, lp_std: 101416.8828\n",
      "Epoch [8/1000], BatchStep[4700/6642]\n",
      "Loss: 43199.6289, lp_mean: 43199.6289, lp_var: 52188604792832.0000, lp_std: 72241.6875\n",
      "Epoch [8/1000], BatchStep[4800/6642]\n",
      "Loss: 119.5115, lp_mean: 119.5115, lp_var: 370107776.0000, lp_std: 192.3819\n",
      "Epoch [8/1000], BatchStep[4900/6642]\n",
      "Loss: 103725.7891, lp_mean: 103725.7891, lp_var: 295704136253440.0000, lp_std: 171960.5000\n",
      "Epoch [8/1000], BatchStep[5000/6642]\n",
      "Loss: 220.5703, lp_mean: 220.5703, lp_var: 1545205504.0000, lp_std: 393.0910\n",
      "Epoch [8/1000], BatchStep[5100/6642]\n",
      "Loss: 25679.0508, lp_mean: 25679.0508, lp_var: 17620430487552.0000, lp_std: 41976.6953\n",
      "Epoch [8/1000], BatchStep[5200/6642]\n",
      "Loss: 5596.6108, lp_mean: 5596.6108, lp_var: 942608678912.0000, lp_std: 9708.8037\n",
      "Epoch [8/1000], BatchStep[5300/6642]\n",
      "Loss: 16334.9385, lp_mean: 16334.9385, lp_var: 7230618337280.0000, lp_std: 26889.8086\n",
      "Epoch [8/1000], BatchStep[5400/6642]\n",
      "Loss: 142598.7031, lp_mean: 142598.7031, lp_var: 575087061237760.0000, lp_std: 239809.7344\n",
      "Epoch [8/1000], BatchStep[5500/6642]\n",
      "Loss: 254843.7188, lp_mean: 254843.7188, lp_var: 1800622357610496.0000, lp_std: 424337.4062\n",
      "Epoch [8/1000], BatchStep[5600/6642]\n",
      "Loss: 0.1205, lp_mean: 0.1205, lp_var: 107.8607, lp_std: 0.1039\n",
      "Epoch [8/1000], BatchStep[5700/6642]\n",
      "Loss: 19150.8711, lp_mean: 19150.8711, lp_var: 9571421650944.0000, lp_std: 30937.7148\n",
      "Epoch [8/1000], BatchStep[5800/6642]\n",
      "Loss: 168.8387, lp_mean: 168.8387, lp_var: 695258624.0000, lp_std: 263.6776\n",
      "Epoch [8/1000], BatchStep[5900/6642]\n",
      "Loss: 32.9641, lp_mean: 32.9641, lp_var: 32771316.0000, lp_std: 57.2462\n",
      "Epoch [8/1000], BatchStep[6000/6642]\n",
      "Loss: 9.8218, lp_mean: 9.8218, lp_var: 3486407.5000, lp_std: 18.6719\n",
      "Epoch [8/1000], BatchStep[6100/6642]\n",
      "Loss: 88450.6406, lp_mean: 88450.6406, lp_var: 213265426677760.0000, lp_std: 146036.0938\n",
      "Epoch [8/1000], BatchStep[6200/6642]\n",
      "Loss: 1379.9473, lp_mean: 1379.9473, lp_var: 47970660352.0000, lp_std: 2190.2207\n",
      "Epoch [8/1000], BatchStep[6300/6642]\n",
      "Loss: 74674.3438, lp_mean: 74674.3438, lp_var: 156119662067712.0000, lp_std: 124947.8594\n",
      "Epoch [8/1000], BatchStep[6400/6642]\n",
      "Loss: 17311.8125, lp_mean: 17311.8125, lp_var: 8416053952512.0000, lp_std: 29010.4375\n",
      "Epoch [8/1000], BatchStep[6500/6642]\n",
      "Loss: 1234.9072, lp_mean: 1234.9072, lp_var: 41528115200.0000, lp_std: 2037.8448\n",
      "Epoch [8/1000], BatchStep[6600/6642]\n",
      "Loss: 608.4110, lp_mean: 608.4110, lp_var: 10195756032.0000, lp_std: 1009.7404\n",
      "Epoch [8/1000]\n",
      "Train Loss : 540.507141, Test Loss: 9774597120.0000\n",
      "Epoch [9/1000], BatchStep[100/6642]\n",
      "Loss: 0.0122, lp_mean: 0.0122, lp_var: 0.7758, lp_std: 0.0088\n",
      "Epoch [9/1000], BatchStep[200/6642]\n",
      "Loss: 83681.6562, lp_mean: 83681.6562, lp_var: 191376461398016.0000, lp_std: 138338.8750\n",
      "Epoch [9/1000], BatchStep[300/6642]\n",
      "Loss: 32068.2363, lp_mean: 32068.2363, lp_var: 28849848975360.0000, lp_std: 53712.0547\n",
      "Epoch [9/1000], BatchStep[400/6642]\n",
      "Loss: 1.9179, lp_mean: 1.9179, lp_var: 74629.7031, lp_std: 2.7318\n",
      "Epoch [9/1000], BatchStep[500/6642]\n",
      "Loss: 51679.2734, lp_mean: 51679.2734, lp_var: 76947543556096.0000, lp_std: 87719.7500\n",
      "Epoch [9/1000], BatchStep[600/6642]\n",
      "Loss: 75660.6719, lp_mean: 75660.6719, lp_var: 160230784630784.0000, lp_std: 126582.2969\n",
      "Epoch [9/1000], BatchStep[700/6642]\n",
      "Loss: 11728.2598, lp_mean: 11728.2598, lp_var: 3815690993664.0000, lp_std: 19533.7949\n",
      "Epoch [9/1000], BatchStep[800/6642]\n",
      "Loss: 370.0381, lp_mean: 370.0381, lp_var: 4093483520.0000, lp_std: 639.8034\n",
      "Epoch [9/1000], BatchStep[900/6642]\n",
      "Loss: 2.3339, lp_mean: 2.3339, lp_var: 123864.5391, lp_std: 3.5194\n",
      "Epoch [9/1000], BatchStep[1000/6642]\n",
      "Loss: 69.3324, lp_mean: 69.3324, lp_var: 119679160.0000, lp_std: 109.3980\n",
      "Epoch [9/1000], BatchStep[1100/6642]\n",
      "Loss: 342.6232, lp_mean: 342.6232, lp_var: 3301380352.0000, lp_std: 574.5764\n",
      "Epoch [9/1000], BatchStep[1200/6642]\n",
      "Loss: 0.0000, lp_mean: 0.0000, lp_var: 0.0000, lp_std: 0.0000\n",
      "Epoch [9/1000], BatchStep[1300/6642]\n",
      "Loss: 9045.1650, lp_mean: 9045.1650, lp_var: 2407071219712.0000, lp_std: 15514.7393\n",
      "Epoch [9/1000], BatchStep[1400/6642]\n",
      "Loss: 349.2304, lp_mean: 349.2304, lp_var: 3357576192.0000, lp_std: 579.4460\n",
      "Epoch [9/1000], BatchStep[1500/6642]\n",
      "Loss: 8182.8340, lp_mean: 8182.8340, lp_var: 2044432482304.0000, lp_std: 14298.3662\n",
      "Epoch [9/1000], BatchStep[1600/6642]\n",
      "Loss: 6.8986, lp_mean: 6.8986, lp_var: 1309457.6250, lp_std: 11.4432\n",
      "Epoch [9/1000], BatchStep[1700/6642]\n",
      "Loss: 149068.7188, lp_mean: 149068.7188, lp_var: 628675972169728.0000, lp_std: 250734.1406\n",
      "Epoch [9/1000], BatchStep[1800/6642]\n",
      "Loss: 594.9030, lp_mean: 594.9030, lp_var: 10883316736.0000, lp_std: 1043.2313\n",
      "Epoch [9/1000], BatchStep[1900/6642]\n",
      "Loss: 1644.3281, lp_mean: 1644.3281, lp_var: 71730331648.0000, lp_std: 2678.2520\n",
      "Epoch [9/1000], BatchStep[2000/6642]\n",
      "Loss: 2941.4231, lp_mean: 2941.4231, lp_var: 242411749376.0000, lp_std: 4923.5332\n",
      "Epoch [9/1000], BatchStep[2100/6642]\n",
      "Loss: 35.4495, lp_mean: 35.4495, lp_var: 31245990.0000, lp_std: 55.8981\n",
      "Epoch [9/1000], BatchStep[2200/6642]\n",
      "Loss: 514.9225, lp_mean: 514.9225, lp_var: 7289953280.0000, lp_std: 853.8123\n",
      "Epoch [9/1000], BatchStep[2300/6642]\n",
      "Loss: 1147.8213, lp_mean: 1147.8213, lp_var: 38541385728.0000, lp_std: 1963.1960\n",
      "Epoch [9/1000], BatchStep[2400/6642]\n",
      "Loss: 402.7545, lp_mean: 402.7545, lp_var: 4361341440.0000, lp_std: 660.4045\n",
      "Epoch [9/1000], BatchStep[2500/6642]\n",
      "Loss: 3169.3230, lp_mean: 3169.3230, lp_var: 274914574336.0000, lp_std: 5243.2295\n",
      "Epoch [9/1000], BatchStep[2600/6642]\n",
      "Loss: 123502.1484, lp_mean: 123502.1484, lp_var: 452480508887040.0000, lp_std: 212715.9062\n",
      "Epoch [9/1000], BatchStep[2700/6642]\n",
      "Loss: 354498.7500, lp_mean: 354498.7500, lp_var: 3511258707918848.0000, lp_std: 592558.7500\n",
      "Epoch [9/1000], BatchStep[2800/6642]\n",
      "Loss: 8585.7471, lp_mean: 8585.7471, lp_var: 2031448883200.0000, lp_std: 14252.8906\n",
      "Epoch [9/1000], BatchStep[2900/6642]\n",
      "Loss: 4.0681, lp_mean: 4.0681, lp_var: 515611.5312, lp_std: 7.1806\n",
      "Epoch [9/1000], BatchStep[3000/6642]\n",
      "Loss: 8395.9502, lp_mean: 8395.9502, lp_var: 1941407531008.0000, lp_std: 13933.4404\n",
      "Epoch [9/1000], BatchStep[3100/6642]\n",
      "Loss: 23481.5820, lp_mean: 23481.5820, lp_var: 15363522691072.0000, lp_std: 39196.3281\n",
      "Epoch [9/1000], BatchStep[3200/6642]\n",
      "Loss: 0.3830, lp_mean: 0.3830, lp_var: 4176.6875, lp_std: 0.6463\n",
      "Epoch [9/1000], BatchStep[3300/6642]\n",
      "Loss: 261.2088, lp_mean: 261.2088, lp_var: 1848003328.0000, lp_std: 429.8841\n",
      "Epoch [9/1000], BatchStep[3400/6642]\n",
      "Loss: 141.2095, lp_mean: 141.2095, lp_var: 596492352.0000, lp_std: 244.2319\n",
      "Epoch [9/1000], BatchStep[3500/6642]\n",
      "Loss: 182.9966, lp_mean: 182.9966, lp_var: 960586048.0000, lp_std: 309.9332\n",
      "Epoch [9/1000], BatchStep[3600/6642]\n",
      "Loss: 0.2728, lp_mean: 0.2728, lp_var: 2151.3069, lp_std: 0.4638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/1000], BatchStep[3700/6642]\n",
      "Loss: 0.0098, lp_mean: 0.0098, lp_var: 0.6668, lp_std: 0.0082\n",
      "Epoch [9/1000], BatchStep[3800/6642]\n",
      "Loss: 17.7850, lp_mean: 17.7850, lp_var: 2985267.2500, lp_std: 17.2779\n",
      "Epoch [9/1000], BatchStep[3900/6642]\n",
      "Loss: 0.8640, lp_mean: 0.8640, lp_var: 18368.6934, lp_std: 1.3553\n",
      "Epoch [9/1000], BatchStep[4000/6642]\n",
      "Loss: 10.7488, lp_mean: 10.7488, lp_var: 3244149.7500, lp_std: 18.0115\n",
      "Epoch [9/1000], BatchStep[4100/6642]\n",
      "Loss: 410645.2812, lp_mean: 410645.2812, lp_var: 4877014284632064.0000, lp_std: 698356.2500\n",
      "Epoch [9/1000], BatchStep[4200/6642]\n",
      "Loss: 1.7890, lp_mean: 1.7890, lp_var: 94048.0938, lp_std: 3.0667\n",
      "Epoch [9/1000], BatchStep[4300/6642]\n",
      "Loss: 2514.0427, lp_mean: 2514.0427, lp_var: 163573891072.0000, lp_std: 4044.4270\n",
      "Epoch [9/1000], BatchStep[4400/6642]\n",
      "Loss: 1028.5900, lp_mean: 1028.5900, lp_var: 27800770560.0000, lp_std: 1667.3563\n",
      "Epoch [9/1000], BatchStep[4500/6642]\n",
      "Loss: 184732.4531, lp_mean: 184732.4531, lp_var: 987281179541504.0000, lp_std: 314210.3125\n",
      "Epoch [9/1000], BatchStep[4600/6642]\n",
      "Loss: 74825.3984, lp_mean: 74825.3984, lp_var: 152024326864896.0000, lp_std: 123298.1484\n",
      "Epoch [9/1000], BatchStep[4700/6642]\n",
      "Loss: 39341.4336, lp_mean: 39341.4336, lp_var: 44524537643008.0000, lp_std: 66726.7031\n",
      "Epoch [9/1000], BatchStep[4800/6642]\n",
      "Loss: 459.5319, lp_mean: 459.5319, lp_var: 5499671040.0000, lp_std: 741.5977\n",
      "Epoch [9/1000], BatchStep[4900/6642]\n",
      "Loss: 1459.1874, lp_mean: 1459.1874, lp_var: 59543343104.0000, lp_std: 2440.1504\n",
      "Epoch [9/1000], BatchStep[5000/6642]\n",
      "Loss: 1140.1796, lp_mean: 1140.1796, lp_var: 38811897856.0000, lp_std: 1970.0736\n",
      "Epoch [9/1000], BatchStep[5100/6642]\n",
      "Loss: 0.3742, lp_mean: 0.3742, lp_var: 3383.6284, lp_std: 0.5817\n",
      "Epoch [9/1000], BatchStep[5200/6642]\n",
      "Loss: 10210.3867, lp_mean: 10210.3867, lp_var: 2255375826944.0000, lp_std: 15017.9092\n",
      "Epoch [9/1000], BatchStep[5300/6642]\n",
      "Loss: 346.2362, lp_mean: 346.2362, lp_var: 2823325696.0000, lp_std: 531.3498\n",
      "Epoch [9/1000], BatchStep[5400/6642]\n",
      "Loss: 21123.2090, lp_mean: 21123.2090, lp_var: 12913097048064.0000, lp_std: 35934.7969\n",
      "Epoch [9/1000], BatchStep[5500/6642]\n",
      "Loss: 38.2800, lp_mean: 38.2800, lp_var: 36233456.0000, lp_std: 60.1942\n",
      "Epoch [9/1000], BatchStep[5600/6642]\n",
      "Loss: 1581.6002, lp_mean: 1581.6002, lp_var: 68127907840.0000, lp_std: 2610.1323\n",
      "Epoch [9/1000], BatchStep[5700/6642]\n",
      "Loss: 91059.9062, lp_mean: 91059.9062, lp_var: 234559555764224.0000, lp_std: 153153.3750\n",
      "Epoch [9/1000], BatchStep[5800/6642]\n",
      "Loss: 10898.8691, lp_mean: 10898.8691, lp_var: 3290365689856.0000, lp_std: 18139.3652\n",
      "Epoch [9/1000], BatchStep[5900/6642]\n",
      "Loss: 57116.7344, lp_mean: 57116.7344, lp_var: 90750108827648.0000, lp_std: 95262.8516\n",
      "Epoch [9/1000], BatchStep[6000/6642]\n",
      "Loss: 2.6536, lp_mean: 2.6536, lp_var: 257149.5000, lp_std: 5.0710\n",
      "Epoch [9/1000], BatchStep[6100/6642]\n",
      "Loss: 5.0334, lp_mean: 5.0334, lp_var: 635785.9375, lp_std: 7.9736\n",
      "Epoch [9/1000], BatchStep[6200/6642]\n",
      "Loss: 18712.8711, lp_mean: 18712.8711, lp_var: 9789973200896.0000, lp_std: 31288.9355\n",
      "Epoch [9/1000], BatchStep[6300/6642]\n",
      "Loss: 22328.0449, lp_mean: 22328.0449, lp_var: 14085507776512.0000, lp_std: 37530.6641\n",
      "Epoch [9/1000], BatchStep[6400/6642]\n",
      "Loss: 0.0349, lp_mean: 0.0349, lp_var: 71.4196, lp_std: 0.0845\n",
      "Epoch [9/1000], BatchStep[6500/6642]\n",
      "Loss: 6823.8369, lp_mean: 6823.8369, lp_var: 1256498659328.0000, lp_std: 11209.3652\n",
      "Epoch [9/1000], BatchStep[6600/6642]\n",
      "Loss: 93106.8359, lp_mean: 93106.8359, lp_var: 242445350600704.0000, lp_std: 155706.5625\n",
      "Epoch [9/1000]\n",
      "Train Loss : 563.902161, Test Loss: 11376804864.0000\n",
      "Epoch [10/1000], BatchStep[100/6642]\n",
      "Loss: 175.3303, lp_mean: 175.3303, lp_var: 946643264.0000, lp_std: 307.6757\n",
      "Epoch [10/1000], BatchStep[200/6642]\n",
      "Loss: 59661.0898, lp_mean: 59661.0898, lp_var: 98614395273216.0000, lp_std: 99304.7812\n",
      "Epoch [10/1000], BatchStep[300/6642]\n",
      "Loss: 57.7469, lp_mean: 57.7469, lp_var: 93093344.0000, lp_std: 96.4849\n",
      "Epoch [10/1000], BatchStep[400/6642]\n",
      "Loss: 3308.3711, lp_mean: 3308.3711, lp_var: 308400521216.0000, lp_std: 5553.3818\n",
      "Epoch [10/1000], BatchStep[500/6642]\n",
      "Loss: 315362.5000, lp_mean: 315362.5000, lp_var: 2798169582731264.0000, lp_std: 528977.3125\n",
      "Epoch [10/1000], BatchStep[600/6642]\n",
      "Loss: 191568.8281, lp_mean: 191568.8281, lp_var: 1029414204735488.0000, lp_std: 320844.8750\n",
      "Epoch [10/1000], BatchStep[700/6642]\n",
      "Loss: 4483.9971, lp_mean: 4483.9971, lp_var: 576509313024.0000, lp_std: 7592.8208\n",
      "Epoch [10/1000], BatchStep[800/6642]\n",
      "Loss: 16004.6611, lp_mean: 16004.6611, lp_var: 7211971510272.0000, lp_std: 26855.1152\n",
      "Epoch [10/1000], BatchStep[900/6642]\n",
      "Loss: 673.0504, lp_mean: 673.0504, lp_var: 9395543040.0000, lp_std: 969.3062\n",
      "Epoch [10/1000], BatchStep[1000/6642]\n",
      "Loss: 1603.7898, lp_mean: 1603.7898, lp_var: 79136448512.0000, lp_std: 2813.1201\n",
      "Epoch [10/1000], BatchStep[1100/6642]\n",
      "Loss: 4276.4526, lp_mean: 4276.4526, lp_var: 514251751424.0000, lp_std: 7171.1353\n",
      "Epoch [10/1000], BatchStep[1200/6642]\n",
      "Loss: 2347.0215, lp_mean: 2347.0215, lp_var: 148505313280.0000, lp_std: 3853.6384\n",
      "Epoch [10/1000], BatchStep[1300/6642]\n",
      "Loss: 27702.3008, lp_mean: 27702.3008, lp_var: 21272801050624.0000, lp_std: 46122.4492\n",
      "Epoch [10/1000], BatchStep[1400/6642]\n",
      "Loss: 32230.9746, lp_mean: 32230.9746, lp_var: 27557112053760.0000, lp_std: 52494.8672\n",
      "Epoch [10/1000], BatchStep[1500/6642]\n",
      "Loss: 128806.4844, lp_mean: 128806.4844, lp_var: 456609213972480.0000, lp_std: 213684.1562\n",
      "Epoch [10/1000], BatchStep[1600/6642]\n",
      "Loss: 161.6309, lp_mean: 161.6309, lp_var: 587280320.0000, lp_std: 242.3387\n",
      "Epoch [10/1000], BatchStep[1700/6642]\n",
      "Loss: 37228.1758, lp_mean: 37228.1758, lp_var: 38844133015552.0000, lp_std: 62325.0625\n",
      "Epoch [10/1000], BatchStep[1800/6642]\n",
      "Loss: 2814.1545, lp_mean: 2814.1545, lp_var: 227355803648.0000, lp_std: 4768.1846\n",
      "Epoch [10/1000], BatchStep[1900/6642]\n",
      "Loss: 726398.0625, lp_mean: 726398.0625, lp_var: 14293972209893376.0000, lp_std: 1195574.0000\n",
      "Epoch [10/1000], BatchStep[2000/6642]\n",
      "Loss: 55.2750, lp_mean: 55.2750, lp_var: 85437808.0000, lp_std: 92.4326\n",
      "Epoch [10/1000], BatchStep[2100/6642]\n",
      "Loss: 185.4096, lp_mean: 185.4096, lp_var: 1011561600.0000, lp_std: 318.0506\n",
      "Epoch [10/1000], BatchStep[2200/6642]\n",
      "Loss: 16345.2305, lp_mean: 16345.2305, lp_var: 6556661317632.0000, lp_std: 25605.9766\n",
      "Epoch [10/1000], BatchStep[2300/6642]\n",
      "Loss: 16.6038, lp_mean: 16.6038, lp_var: 7624721.5000, lp_std: 27.6129\n",
      "Epoch [10/1000], BatchStep[2400/6642]\n",
      "Loss: 846.9240, lp_mean: 846.9240, lp_var: 18446243840.0000, lp_std: 1358.1696\n",
      "Epoch [10/1000], BatchStep[2500/6642]\n",
      "Loss: 6.7473, lp_mean: 6.7473, lp_var: 1238492.2500, lp_std: 11.1288\n",
      "Epoch [10/1000], BatchStep[2600/6642]\n",
      "Loss: 1751478.8750, lp_mean: 1751478.8750, lp_var: 86096106961764352.0000, lp_std: 2934214.0000\n",
      "Epoch [10/1000], BatchStep[2700/6642]\n",
      "Loss: 163592.8750, lp_mean: 163592.8750, lp_var: 742402746744832.0000, lp_std: 272470.6875\n",
      "Epoch [10/1000], BatchStep[2800/6642]\n",
      "Loss: 0.0669, lp_mean: 0.0669, lp_var: 124.1743, lp_std: 0.1114\n",
      "Epoch [10/1000], BatchStep[2900/6642]\n",
      "Loss: 10514.3311, lp_mean: 10514.3311, lp_var: 3000106221568.0000, lp_std: 17320.8145\n",
      "Epoch [10/1000], BatchStep[3000/6642]\n",
      "Loss: 2369.1509, lp_mean: 2369.1509, lp_var: 169328623616.0000, lp_std: 4114.9561\n",
      "Epoch [10/1000], BatchStep[3100/6642]\n",
      "Loss: 3046.1880, lp_mean: 3046.1880, lp_var: 252296019968.0000, lp_std: 5022.9077\n",
      "Epoch [10/1000], BatchStep[3200/6642]\n",
      "Loss: 4822.3525, lp_mean: 4822.3525, lp_var: 790526492672.0000, lp_std: 8891.1553\n",
      "Epoch [10/1000], BatchStep[3300/6642]\n",
      "Loss: 25742.3262, lp_mean: 25742.3262, lp_var: 19306899308544.0000, lp_std: 43939.6172\n",
      "Epoch [10/1000], BatchStep[3400/6642]\n",
      "Loss: 63.8034, lp_mean: 63.8034, lp_var: 73203056.0000, lp_std: 85.5588\n",
      "Epoch [10/1000], BatchStep[3500/6642]\n",
      "Loss: 1.0153, lp_mean: 1.0153, lp_var: 31345.4160, lp_std: 1.7705\n",
      "Epoch [10/1000], BatchStep[3600/6642]\n",
      "Loss: 180.1009, lp_mean: 180.1009, lp_var: 910151296.0000, lp_std: 301.6871\n",
      "Epoch [10/1000], BatchStep[3700/6642]\n",
      "Loss: 308.5711, lp_mean: 308.5711, lp_var: 2882845440.0000, lp_std: 536.9213\n",
      "Epoch [10/1000], BatchStep[3800/6642]\n",
      "Loss: 1075.4694, lp_mean: 1075.4694, lp_var: 31408803840.0000, lp_std: 1772.2531\n",
      "Epoch [10/1000], BatchStep[3900/6642]\n",
      "Loss: 840.5172, lp_mean: 840.5172, lp_var: 19747954688.0000, lp_std: 1405.2742\n",
      "Epoch [10/1000], BatchStep[4000/6642]\n",
      "Loss: 72.5685, lp_mean: 72.5685, lp_var: 147046096.0000, lp_std: 121.2626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], BatchStep[4100/6642]\n",
      "Loss: 91806.7422, lp_mean: 91806.7422, lp_var: 246515754860544.0000, lp_std: 157008.2031\n",
      "Epoch [10/1000], BatchStep[4200/6642]\n",
      "Loss: 421790.2188, lp_mean: 421790.2188, lp_var: 4940896017580032.0000, lp_std: 702915.0625\n",
      "Epoch [10/1000], BatchStep[4300/6642]\n",
      "Loss: 155.7516, lp_mean: 155.7516, lp_var: 644906048.0000, lp_std: 253.9500\n",
      "Epoch [10/1000], BatchStep[4400/6642]\n",
      "Loss: 1593.3109, lp_mean: 1593.3109, lp_var: 73377193984.0000, lp_std: 2708.8225\n",
      "Epoch [10/1000], BatchStep[4500/6642]\n",
      "Loss: 0.0333, lp_mean: 0.0333, lp_var: 7.7228, lp_std: 0.0278\n",
      "Epoch [10/1000], BatchStep[4600/6642]\n",
      "Loss: 1078.7091, lp_mean: 1078.7091, lp_var: 32883593216.0000, lp_std: 1813.3834\n",
      "Epoch [10/1000], BatchStep[4700/6642]\n",
      "Loss: 88.2188, lp_mean: 88.2188, lp_var: 183638848.0000, lp_std: 135.5134\n",
      "Epoch [10/1000], BatchStep[4800/6642]\n",
      "Loss: 0.7568, lp_mean: 0.7568, lp_var: 15713.3867, lp_std: 1.2535\n",
      "Epoch [10/1000], BatchStep[4900/6642]\n",
      "Loss: 117768.3672, lp_mean: 117768.3672, lp_var: 396151543037952.0000, lp_std: 199035.5781\n",
      "Epoch [10/1000], BatchStep[5000/6642]\n",
      "Loss: 140.7682, lp_mean: 140.7682, lp_var: 572431104.0000, lp_std: 239.2553\n",
      "Epoch [10/1000], BatchStep[5100/6642]\n",
      "Loss: 31.8173, lp_mean: 31.8173, lp_var: 30699052.0000, lp_std: 55.4067\n",
      "Epoch [10/1000], BatchStep[5200/6642]\n",
      "Loss: 4082.3984, lp_mean: 4082.3984, lp_var: 469707227136.0000, lp_std: 6853.5190\n",
      "Epoch [10/1000], BatchStep[5300/6642]\n",
      "Loss: 201069.2031, lp_mean: 201069.2031, lp_var: 1118496658292736.0000, lp_std: 334439.3438\n",
      "Epoch [10/1000], BatchStep[5400/6642]\n",
      "Loss: 51034.0938, lp_mean: 51034.0938, lp_var: 71212386484224.0000, lp_std: 84387.4297\n",
      "Epoch [10/1000], BatchStep[5500/6642]\n",
      "Loss: 0.0000, lp_mean: 0.0000, lp_var: 0.0000, lp_std: 0.0000\n",
      "Epoch [10/1000], BatchStep[5600/6642]\n",
      "Loss: 247.1297, lp_mean: 247.1297, lp_var: 1496705792.0000, lp_std: 386.8728\n",
      "Epoch [10/1000], BatchStep[5700/6642]\n",
      "Loss: 21721.8105, lp_mean: 21721.8105, lp_var: 13418461396992.0000, lp_std: 36631.2188\n",
      "Epoch [10/1000], BatchStep[5800/6642]\n",
      "Loss: 7347.4146, lp_mean: 7347.4146, lp_var: 1323299110912.0000, lp_std: 11503.4736\n",
      "Epoch [10/1000], BatchStep[5900/6642]\n",
      "Loss: 78.6463, lp_mean: 78.6463, lp_var: 198122448.0000, lp_std: 140.7560\n",
      "Epoch [10/1000], BatchStep[6000/6642]\n",
      "Loss: 4.5772, lp_mean: 4.5772, lp_var: 460391.4062, lp_std: 6.7852\n",
      "Epoch [10/1000], BatchStep[6100/6642]\n",
      "Loss: 925.1094, lp_mean: 925.1094, lp_var: 22502049792.0000, lp_std: 1500.0684\n",
      "Epoch [10/1000], BatchStep[6200/6642]\n",
      "Loss: 2603.0530, lp_mean: 2603.0530, lp_var: 204702433280.0000, lp_std: 4524.4058\n",
      "Epoch [10/1000], BatchStep[6300/6642]\n",
      "Loss: 6599.2051, lp_mean: 6599.2051, lp_var: 1235067338752.0000, lp_std: 11113.3584\n",
      "Epoch [10/1000], BatchStep[6400/6642]\n",
      "Loss: 85.6625, lp_mean: 85.6625, lp_var: 170146928.0000, lp_std: 130.4404\n",
      "Epoch [10/1000], BatchStep[6500/6642]\n",
      "Loss: 324.9527, lp_mean: 324.9527, lp_var: 2930484736.0000, lp_std: 541.3395\n",
      "Epoch [10/1000], BatchStep[6600/6642]\n",
      "Loss: 41436.2852, lp_mean: 41436.2852, lp_var: 49909382250496.0000, lp_std: 70646.5703\n",
      "Epoch [10/1000]\n",
      "Train Loss : 611.150696, Test Loss: 9031717888.0000\n",
      "Epoch [11/1000], BatchStep[100/6642]\n",
      "Loss: 4659.7119, lp_mean: 4659.7119, lp_var: 692047642624.0000, lp_std: 8318.9404\n",
      "Epoch [11/1000], BatchStep[200/6642]\n",
      "Loss: 4431.1260, lp_mean: 4431.1260, lp_var: 535532896256.0000, lp_std: 7318.0112\n",
      "Epoch [11/1000], BatchStep[300/6642]\n",
      "Loss: 6381.1211, lp_mean: 6381.1211, lp_var: 1100841746432.0000, lp_std: 10492.1006\n",
      "Epoch [11/1000], BatchStep[400/6642]\n",
      "Loss: 976.3062, lp_mean: 976.3062, lp_var: 25449680896.0000, lp_std: 1595.2955\n",
      "Epoch [11/1000], BatchStep[500/6642]\n",
      "Loss: 167469.7031, lp_mean: 167469.7031, lp_var: 761350867386368.0000, lp_std: 275925.8750\n",
      "Epoch [11/1000], BatchStep[600/6642]\n",
      "Loss: 389.8536, lp_mean: 389.8536, lp_var: 3644433920.0000, lp_std: 603.6915\n",
      "Epoch [11/1000], BatchStep[700/6642]\n",
      "Loss: 964.0584, lp_mean: 964.0584, lp_var: 25097981952.0000, lp_std: 1584.2343\n",
      "Epoch [11/1000], BatchStep[800/6642]\n",
      "Loss: 2144.5083, lp_mean: 2144.5083, lp_var: 129325801472.0000, lp_std: 3596.1897\n",
      "Epoch [11/1000], BatchStep[900/6642]\n",
      "Loss: 109772.3594, lp_mean: 109772.3594, lp_var: 338500633231360.0000, lp_std: 183983.8750\n",
      "Epoch [11/1000], BatchStep[1000/6642]\n",
      "Loss: 10.7638, lp_mean: 10.7638, lp_var: 2864472.5000, lp_std: 16.9248\n",
      "Epoch [11/1000], BatchStep[1100/6642]\n",
      "Loss: 80981.0312, lp_mean: 80981.0312, lp_var: 183413877243904.0000, lp_std: 135430.3750\n",
      "Epoch [11/1000], BatchStep[1200/6642]\n",
      "Loss: 297.8128, lp_mean: 297.8128, lp_var: 2521473280.0000, lp_std: 502.1428\n",
      "Epoch [11/1000], BatchStep[1300/6642]\n",
      "Loss: 5116.7729, lp_mean: 5116.7729, lp_var: 733019766784.0000, lp_std: 8561.6572\n",
      "Epoch [11/1000], BatchStep[1400/6642]\n",
      "Loss: 365.8856, lp_mean: 365.8856, lp_var: 3754732544.0000, lp_std: 612.7587\n",
      "Epoch [11/1000], BatchStep[1500/6642]\n",
      "Loss: 4685.8252, lp_mean: 4685.8252, lp_var: 572108308480.0000, lp_std: 7563.7842\n",
      "Epoch [11/1000], BatchStep[1600/6642]\n",
      "Loss: 15845.5449, lp_mean: 15845.5449, lp_var: 7272037613568.0000, lp_std: 26966.7168\n",
      "Epoch [11/1000], BatchStep[1700/6642]\n",
      "Loss: 21390.1504, lp_mean: 21390.1504, lp_var: 12721514872832.0000, lp_std: 35667.2305\n",
      "Epoch [11/1000], BatchStep[1800/6642]\n",
      "Loss: 283.2039, lp_mean: 283.2039, lp_var: 2750917120.0000, lp_std: 524.4919\n",
      "Epoch [11/1000], BatchStep[1900/6642]\n",
      "Loss: 558763.6250, lp_mean: 558763.6250, lp_var: 8718275194126336.0000, lp_std: 933717.0625\n",
      "Epoch [11/1000], BatchStep[2000/6642]\n",
      "Loss: 18773.0742, lp_mean: 18773.0742, lp_var: 8572681322496.0000, lp_std: 29279.1426\n",
      "Epoch [11/1000], BatchStep[2100/6642]\n",
      "Loss: 55.2596, lp_mean: 55.2596, lp_var: 103305424.0000, lp_std: 101.6393\n",
      "Epoch [11/1000], BatchStep[2200/6642]\n",
      "Loss: 26340.3809, lp_mean: 26340.3809, lp_var: 18620346269696.0000, lp_std: 43151.2969\n",
      "Epoch [11/1000], BatchStep[2300/6642]\n",
      "Loss: 8.2738, lp_mean: 8.2738, lp_var: 1751100.8750, lp_std: 13.2329\n",
      "Epoch [11/1000], BatchStep[2400/6642]\n",
      "Loss: 243.7547, lp_mean: 243.7547, lp_var: 1041488960.0000, lp_std: 322.7211\n",
      "Epoch [11/1000], BatchStep[2500/6642]\n",
      "Loss: 13227.3828, lp_mean: 13227.3828, lp_var: 4867974234112.0000, lp_std: 22063.4863\n",
      "Epoch [11/1000], BatchStep[2600/6642]\n",
      "Loss: 0.0027, lp_mean: 0.0027, lp_var: 0.0290, lp_std: 0.0017\n",
      "Epoch [11/1000], BatchStep[2700/6642]\n",
      "Loss: 19332.3105, lp_mean: 19332.3105, lp_var: 10948998856704.0000, lp_std: 33089.2695\n",
      "Epoch [11/1000], BatchStep[2800/6642]\n",
      "Loss: 62212.3359, lp_mean: 62212.3359, lp_var: 110352188571648.0000, lp_std: 105048.6484\n",
      "Epoch [11/1000], BatchStep[2900/6642]\n",
      "Loss: 958.5154, lp_mean: 958.5154, lp_var: 26169968640.0000, lp_std: 1617.7135\n",
      "Epoch [11/1000], BatchStep[3000/6642]\n",
      "Loss: 6.3849, lp_mean: 6.3849, lp_var: 1096348.7500, lp_std: 10.4707\n",
      "Epoch [11/1000], BatchStep[3100/6642]\n",
      "Loss: 9267.1855, lp_mean: 9267.1855, lp_var: 2413587070976.0000, lp_std: 15535.7236\n",
      "Epoch [11/1000], BatchStep[3200/6642]\n",
      "Loss: 167185.0000, lp_mean: 167185.0000, lp_var: 773906667405312.0000, lp_std: 278191.7812\n",
      "Epoch [11/1000], BatchStep[3300/6642]\n",
      "Loss: 3676.5176, lp_mean: 3676.5176, lp_var: 397459849216.0000, lp_std: 6304.4414\n",
      "Epoch [11/1000], BatchStep[3400/6642]\n",
      "Loss: 5148.3311, lp_mean: 5148.3311, lp_var: 745632432128.0000, lp_std: 8635.0010\n",
      "Epoch [11/1000], BatchStep[3500/6642]\n",
      "Loss: 39282.3828, lp_mean: 39282.3828, lp_var: 43254716301312.0000, lp_std: 65768.3203\n",
      "Epoch [11/1000], BatchStep[3600/6642]\n",
      "Loss: 1.3642, lp_mean: 1.3642, lp_var: 50813.9766, lp_std: 2.2542\n",
      "Epoch [11/1000], BatchStep[3700/6642]\n",
      "Loss: 288.2417, lp_mean: 288.2417, lp_var: 2634321408.0000, lp_std: 513.2564\n",
      "Epoch [11/1000], BatchStep[3800/6642]\n",
      "Loss: 24278.2188, lp_mean: 24278.2188, lp_var: 17182880694272.0000, lp_std: 41452.2422\n",
      "Epoch [11/1000], BatchStep[3900/6642]\n",
      "Loss: 640.2296, lp_mean: 640.2296, lp_var: 13820015616.0000, lp_std: 1175.5856\n",
      "Epoch [11/1000], BatchStep[4000/6642]\n",
      "Loss: 35781.1758, lp_mean: 35781.1758, lp_var: 35825962188800.0000, lp_std: 59854.7969\n",
      "Epoch [11/1000], BatchStep[4100/6642]\n",
      "Loss: 107980.7812, lp_mean: 107980.7812, lp_var: 316852555218944.0000, lp_std: 178003.5312\n",
      "Epoch [11/1000], BatchStep[4200/6642]\n",
      "Loss: 43507.1602, lp_mean: 43507.1602, lp_var: 52766345003008.0000, lp_std: 72640.4531\n",
      "Epoch [11/1000], BatchStep[4300/6642]\n",
      "Loss: 10289.2109, lp_mean: 10289.2109, lp_var: 2760612511744.0000, lp_std: 16615.0918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/1000], BatchStep[4400/6642]\n",
      "Loss: 1.4615, lp_mean: 1.4615, lp_var: 59341.4570, lp_std: 2.4360\n",
      "Epoch [11/1000], BatchStep[4500/6642]\n",
      "Loss: 2406.9097, lp_mean: 2406.9097, lp_var: 168963997696.0000, lp_std: 4110.5234\n",
      "Epoch [11/1000], BatchStep[4600/6642]\n",
      "Loss: 8209.0869, lp_mean: 8209.0869, lp_var: 1946061897728.0000, lp_std: 13950.1328\n",
      "Epoch [11/1000], BatchStep[4700/6642]\n",
      "Loss: 593.6962, lp_mean: 593.6962, lp_var: 10410595328.0000, lp_std: 1020.3232\n",
      "Epoch [11/1000], BatchStep[4800/6642]\n",
      "Loss: 28.5698, lp_mean: 28.5698, lp_var: 23841380.0000, lp_std: 48.8276\n",
      "Epoch [11/1000], BatchStep[4900/6642]\n",
      "Loss: 9060.5605, lp_mean: 9060.5605, lp_var: 2087550844928.0000, lp_std: 14448.3604\n",
      "Epoch [11/1000], BatchStep[5000/6642]\n",
      "Loss: 13822.2754, lp_mean: 13822.2754, lp_var: 5270864920576.0000, lp_std: 22958.3652\n",
      "Epoch [11/1000], BatchStep[5100/6642]\n",
      "Loss: 47788.5625, lp_mean: 47788.5625, lp_var: 64170443669504.0000, lp_std: 80106.4531\n",
      "Epoch [11/1000], BatchStep[5200/6642]\n",
      "Loss: 32.0705, lp_mean: 32.0705, lp_var: 29041394.0000, lp_std: 53.8901\n",
      "Epoch [11/1000], BatchStep[5300/6642]\n",
      "Loss: 5338.5918, lp_mean: 5338.5918, lp_var: 791319019520.0000, lp_std: 8895.6113\n",
      "Epoch [11/1000], BatchStep[5400/6642]\n",
      "Loss: 135.5008, lp_mean: 135.5008, lp_var: 381720416.0000, lp_std: 195.3767\n",
      "Epoch [11/1000], BatchStep[5500/6642]\n",
      "Loss: 47.4635, lp_mean: 47.4635, lp_var: 58167980.0000, lp_std: 76.2679\n",
      "Epoch [11/1000], BatchStep[5600/6642]\n",
      "Loss: 18903.6562, lp_mean: 18903.6562, lp_var: 10064936042496.0000, lp_std: 31725.2832\n",
      "Epoch [11/1000], BatchStep[5700/6642]\n",
      "Loss: 4955.1499, lp_mean: 4955.1499, lp_var: 650077667328.0000, lp_std: 8062.7393\n",
      "Epoch [11/1000], BatchStep[5800/6642]\n",
      "Loss: 0.0790, lp_mean: 0.0790, lp_var: 268.1795, lp_std: 0.1638\n",
      "Epoch [11/1000], BatchStep[5900/6642]\n",
      "Loss: 52.2527, lp_mean: 52.2527, lp_var: 79875944.0000, lp_std: 89.3733\n",
      "Epoch [11/1000], BatchStep[6000/6642]\n",
      "Loss: 4666.6523, lp_mean: 4666.6523, lp_var: 582365544448.0000, lp_std: 7631.2881\n",
      "Epoch [11/1000], BatchStep[6100/6642]\n",
      "Loss: 13277.3730, lp_mean: 13277.3730, lp_var: 4853922791424.0000, lp_std: 22031.6211\n",
      "Epoch [11/1000], BatchStep[6200/6642]\n",
      "Loss: 14071.2070, lp_mean: 14071.2070, lp_var: 5590444146688.0000, lp_std: 23644.1191\n",
      "Epoch [11/1000], BatchStep[6300/6642]\n",
      "Loss: 144.2773, lp_mean: 144.2773, lp_var: 673279168.0000, lp_std: 259.4762\n",
      "Epoch [11/1000], BatchStep[6400/6642]\n",
      "Loss: 41496.3203, lp_mean: 41496.3203, lp_var: 48231371243520.0000, lp_std: 69448.8125\n",
      "Epoch [11/1000], BatchStep[6500/6642]\n",
      "Loss: 7357.8579, lp_mean: 7357.8579, lp_var: 1467840593920.0000, lp_std: 12115.4473\n",
      "Epoch [11/1000], BatchStep[6600/6642]\n",
      "Loss: 20459.0195, lp_mean: 20459.0195, lp_var: 11799425449984.0000, lp_std: 34350.2930\n",
      "Epoch [11/1000]\n",
      "Train Loss : 561.309448, Test Loss: 10076800000.0000\n",
      "Epoch [12/1000], BatchStep[100/6642]\n",
      "Loss: 11724.8604, lp_mean: 11724.8604, lp_var: 3686186614784.0000, lp_std: 19199.4453\n",
      "Epoch [12/1000], BatchStep[200/6642]\n",
      "Loss: 1.0580, lp_mean: 1.0580, lp_var: 33440.7109, lp_std: 1.8287\n",
      "Epoch [12/1000], BatchStep[300/6642]\n",
      "Loss: 379889.3750, lp_mean: 379889.3750, lp_var: 3970265084067840.0000, lp_std: 630100.3750\n",
      "Epoch [12/1000], BatchStep[400/6642]\n",
      "Loss: 0.0782, lp_mean: 0.0782, lp_var: 213.6812, lp_std: 0.1462\n",
      "Epoch [12/1000], BatchStep[500/6642]\n",
      "Loss: 53118.3203, lp_mean: 53118.3203, lp_var: 77529880723456.0000, lp_std: 88051.0547\n",
      "Epoch [12/1000], BatchStep[600/6642]\n",
      "Loss: 6129.9795, lp_mean: 6129.9795, lp_var: 1150710579200.0000, lp_std: 10727.1182\n",
      "Epoch [12/1000], BatchStep[700/6642]\n",
      "Loss: 0.8184, lp_mean: 0.8184, lp_var: 20863.8633, lp_std: 1.4444\n",
      "Epoch [12/1000], BatchStep[800/6642]\n",
      "Loss: 1556.2228, lp_mean: 1556.2228, lp_var: 70264717312.0000, lp_std: 2650.7493\n",
      "Epoch [12/1000], BatchStep[900/6642]\n",
      "Loss: 675.0574, lp_mean: 675.0574, lp_var: 11994040320.0000, lp_std: 1095.1730\n",
      "Epoch [12/1000], BatchStep[1000/6642]\n",
      "Loss: 13125.2148, lp_mean: 13125.2148, lp_var: 4778427940864.0000, lp_std: 21859.6152\n",
      "Epoch [12/1000], BatchStep[1100/6642]\n",
      "Loss: 58665.4023, lp_mean: 58665.4023, lp_var: 96134412369920.0000, lp_std: 98048.1562\n",
      "Epoch [12/1000], BatchStep[1200/6642]\n",
      "Loss: 631253.0000, lp_mean: 631253.0000, lp_var: 10819859063504896.0000, lp_std: 1040185.5000\n",
      "Epoch [12/1000], BatchStep[1300/6642]\n",
      "Loss: 9949.5254, lp_mean: 9949.5254, lp_var: 2743476682752.0000, lp_std: 16563.4434\n",
      "Epoch [12/1000], BatchStep[1400/6642]\n",
      "Loss: 35324.8086, lp_mean: 35324.8086, lp_var: 34598555746304.0000, lp_std: 58820.5391\n",
      "Epoch [12/1000], BatchStep[1500/6642]\n",
      "Loss: 0.0000, lp_mean: 0.0000, lp_var: 0.0000, lp_std: 0.0000\n",
      "Epoch [12/1000], BatchStep[1600/6642]\n",
      "Loss: 266109.2812, lp_mean: 266109.2812, lp_var: 1913695156305920.0000, lp_std: 437458.0312\n",
      "Epoch [12/1000], BatchStep[1700/6642]\n",
      "Loss: 1735.4879, lp_mean: 1735.4879, lp_var: 82568617984.0000, lp_std: 2873.4756\n",
      "Epoch [12/1000], BatchStep[1800/6642]\n",
      "Loss: 0.0185, lp_mean: 0.0185, lp_var: 6.1564, lp_std: 0.0248\n",
      "Epoch [12/1000], BatchStep[1900/6642]\n",
      "Loss: 5725.6436, lp_mean: 5725.6436, lp_var: 928018726912.0000, lp_std: 9633.3730\n",
      "Epoch [12/1000], BatchStep[2000/6642]\n",
      "Loss: 2798.7429, lp_mean: 2798.7429, lp_var: 214403973120.0000, lp_std: 4630.3779\n",
      "Epoch [12/1000], BatchStep[2100/6642]\n",
      "Loss: 22810.2383, lp_mean: 22810.2383, lp_var: 15711897387008.0000, lp_std: 39638.2344\n",
      "Epoch [12/1000], BatchStep[2200/6642]\n",
      "Loss: 0.1479, lp_mean: 0.1479, lp_var: 1437.4370, lp_std: 0.3791\n",
      "Epoch [12/1000], BatchStep[2300/6642]\n",
      "Loss: 53.1010, lp_mean: 53.1010, lp_var: 82979360.0000, lp_std: 91.0930\n",
      "Epoch [12/1000], BatchStep[2400/6642]\n",
      "Loss: 954.2953, lp_mean: 954.2953, lp_var: 25174175744.0000, lp_std: 1586.6372\n",
      "Epoch [12/1000], BatchStep[2500/6642]\n",
      "Loss: 244821.8750, lp_mean: 244821.8750, lp_var: 1627655434665984.0000, lp_std: 403442.1250\n",
      "Epoch [12/1000], BatchStep[2600/6642]\n",
      "Loss: 0.0000, lp_mean: 0.0000, lp_var: 0.0000, lp_std: 0.0000\n",
      "Epoch [12/1000], BatchStep[2700/6642]\n",
      "Loss: 39291.6211, lp_mean: 39291.6211, lp_var: 44157842227200.0000, lp_std: 66451.3672\n",
      "Epoch [12/1000], BatchStep[2800/6642]\n",
      "Loss: 4050.3284, lp_mean: 4050.3284, lp_var: 475860140032.0000, lp_std: 6898.2622\n",
      "Epoch [12/1000], BatchStep[2900/6642]\n",
      "Loss: 9.1526, lp_mean: 9.1526, lp_var: 2232783.2500, lp_std: 14.9425\n",
      "Epoch [12/1000], BatchStep[3000/6642]\n",
      "Loss: 0.0396, lp_mean: 0.0396, lp_var: 24.8103, lp_std: 0.0498\n",
      "Epoch [12/1000], BatchStep[3100/6642]\n",
      "Loss: 75.8912, lp_mean: 75.8912, lp_var: 179851008.0000, lp_std: 134.1086\n",
      "Epoch [12/1000], BatchStep[3200/6642]\n",
      "Loss: 26298.1387, lp_mean: 26298.1387, lp_var: 19720598192128.0000, lp_std: 44407.8789\n",
      "Epoch [12/1000], BatchStep[3300/6642]\n",
      "Loss: 64240.6289, lp_mean: 64240.6289, lp_var: 114645956296704.0000, lp_std: 107072.8516\n",
      "Epoch [12/1000], BatchStep[3400/6642]\n",
      "Loss: 19796.5117, lp_mean: 19796.5117, lp_var: 10727047823360.0000, lp_std: 32752.1738\n",
      "Epoch [12/1000], BatchStep[3500/6642]\n",
      "Loss: 144418.8594, lp_mean: 144418.8594, lp_var: 594170473349120.0000, lp_std: 243756.1250\n",
      "Epoch [12/1000], BatchStep[3600/6642]\n",
      "Loss: 0.0003, lp_mean: 0.0003, lp_var: 0.0016, lp_std: 0.0004\n",
      "Epoch [12/1000], BatchStep[3700/6642]\n",
      "Loss: 25763.9434, lp_mean: 25763.9434, lp_var: 18994398494720.0000, lp_std: 43582.5625\n",
      "Epoch [12/1000], BatchStep[3800/6642]\n",
      "Loss: 15192.3477, lp_mean: 15192.3477, lp_var: 6635801542656.0000, lp_std: 25760.0508\n",
      "Epoch [12/1000], BatchStep[3900/6642]\n",
      "Loss: 1856.8691, lp_mean: 1856.8691, lp_var: 95551700992.0000, lp_std: 3091.1440\n",
      "Epoch [12/1000], BatchStep[4000/6642]\n",
      "Loss: 266.8396, lp_mean: 266.8396, lp_var: 1626276352.0000, lp_std: 403.2712\n",
      "Epoch [12/1000], BatchStep[4100/6642]\n",
      "Loss: 332953.8750, lp_mean: 332953.8750, lp_var: 3128694696574976.0000, lp_std: 559347.3750\n",
      "Epoch [12/1000], BatchStep[4200/6642]\n",
      "Loss: 11407.8730, lp_mean: 11407.8730, lp_var: 3692818857984.0000, lp_std: 19216.7090\n",
      "Epoch [12/1000], BatchStep[4300/6642]\n",
      "Loss: 3755.4722, lp_mean: 3755.4722, lp_var: 412811460608.0000, lp_std: 6425.0405\n",
      "Epoch [12/1000], BatchStep[4400/6642]\n",
      "Loss: 13.3957, lp_mean: 13.3957, lp_var: 4207562.0000, lp_std: 20.5123\n",
      "Epoch [12/1000], BatchStep[4500/6642]\n",
      "Loss: 45057.1953, lp_mean: 45057.1953, lp_var: 56804813832192.0000, lp_std: 75368.9688\n",
      "Epoch [12/1000], BatchStep[4600/6642]\n",
      "Loss: 5638.1089, lp_mean: 5638.1089, lp_var: 893950492672.0000, lp_std: 9454.8965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/1000], BatchStep[4700/6642]\n",
      "Loss: 1.8556, lp_mean: 1.8556, lp_var: 172400.4844, lp_std: 4.1521\n",
      "Epoch [12/1000], BatchStep[4800/6642]\n",
      "Loss: 11474.2412, lp_mean: 11474.2412, lp_var: 3688944107520.0000, lp_std: 19206.6250\n",
      "Epoch [12/1000], BatchStep[4900/6642]\n",
      "Loss: 149.2718, lp_mean: 149.2718, lp_var: 582052608.0000, lp_std: 241.2577\n",
      "Epoch [12/1000], BatchStep[5000/6642]\n",
      "Loss: 3907.5513, lp_mean: 3907.5513, lp_var: 426374004736.0000, lp_std: 6529.7319\n",
      "Epoch [12/1000], BatchStep[5100/6642]\n",
      "Loss: 4.7057, lp_mean: 4.7057, lp_var: 378571.6562, lp_std: 6.1528\n",
      "Epoch [12/1000], BatchStep[5200/6642]\n",
      "Loss: 1507.0973, lp_mean: 1507.0973, lp_var: 66356432896.0000, lp_std: 2575.9741\n",
      "Epoch [12/1000], BatchStep[5300/6642]\n",
      "Loss: 39237.6836, lp_mean: 39237.6836, lp_var: 44544934543360.0000, lp_std: 66741.9922\n",
      "Epoch [12/1000], BatchStep[5400/6642]\n",
      "Loss: 0.6677, lp_mean: 0.6677, lp_var: 10360.4600, lp_std: 1.0179\n",
      "Epoch [12/1000], BatchStep[5500/6642]\n",
      "Loss: 164540.5625, lp_mean: 164540.5625, lp_var: 743617886945280.0000, lp_std: 272693.5625\n",
      "Epoch [12/1000], BatchStep[5600/6642]\n",
      "Loss: 752.0746, lp_mean: 752.0746, lp_var: 15796757504.0000, lp_std: 1256.8514\n",
      "Epoch [12/1000], BatchStep[5700/6642]\n",
      "Loss: 163328.6875, lp_mean: 163328.6875, lp_var: 757486873214976.0000, lp_std: 275224.8125\n",
      "Epoch [12/1000], BatchStep[5800/6642]\n",
      "Loss: 1603.9950, lp_mean: 1603.9950, lp_var: 72936873984.0000, lp_std: 2700.6826\n",
      "Epoch [12/1000], BatchStep[5900/6642]\n",
      "Loss: 13773.9746, lp_mean: 13773.9746, lp_var: 5281302446080.0000, lp_std: 22981.0859\n",
      "Epoch [12/1000], BatchStep[6000/6642]\n",
      "Loss: 18.7007, lp_mean: 18.7007, lp_var: 8929621.0000, lp_std: 29.8825\n",
      "Epoch [12/1000], BatchStep[6100/6642]\n",
      "Loss: 0.0000, lp_mean: 0.0000, lp_var: 0.0000, lp_std: 0.0000\n",
      "Epoch [12/1000], BatchStep[6200/6642]\n",
      "Loss: 44221.1992, lp_mean: 44221.1992, lp_var: 56187282259968.0000, lp_std: 74958.1797\n",
      "Epoch [12/1000], BatchStep[6300/6642]\n",
      "Loss: 7883.3369, lp_mean: 7883.3369, lp_var: 1757728604160.0000, lp_std: 13257.9365\n",
      "Epoch [12/1000], BatchStep[6400/6642]\n",
      "Loss: 0.3132, lp_mean: 0.3132, lp_var: 2024.3811, lp_std: 0.4499\n",
      "Epoch [12/1000], BatchStep[6500/6642]\n",
      "Loss: 1632.7028, lp_mean: 1632.7028, lp_var: 74227826688.0000, lp_std: 2724.4783\n",
      "Epoch [12/1000], BatchStep[6600/6642]\n",
      "Loss: 17591.6953, lp_mean: 17591.6953, lp_var: 8317350445056.0000, lp_std: 28839.8184\n",
      "Epoch [12/1000]\n",
      "Train Loss : 579.262634, Test Loss: 9304646656.0000\n",
      "Epoch [13/1000], BatchStep[100/6642]\n",
      "Loss: 21340.9277, lp_mean: 21340.9277, lp_var: 13064590065664.0000, lp_std: 36144.9727\n",
      "Epoch [13/1000], BatchStep[200/6642]\n",
      "Loss: 37.2540, lp_mean: 37.2540, lp_var: 43603348.0000, lp_std: 66.0328\n",
      "Epoch [13/1000], BatchStep[300/6642]\n",
      "Loss: 22940.8730, lp_mean: 22940.8730, lp_var: 15357024665600.0000, lp_std: 39188.0430\n",
      "Epoch [13/1000], BatchStep[400/6642]\n",
      "Loss: 3089.7053, lp_mean: 3089.7053, lp_var: 268563759104.0000, lp_std: 5182.3140\n",
      "Epoch [13/1000], BatchStep[500/6642]\n",
      "Loss: 22.0298, lp_mean: 22.0298, lp_var: 15431256.0000, lp_std: 39.2826\n",
      "Epoch [13/1000], BatchStep[600/6642]\n",
      "Loss: 8512.3799, lp_mean: 8512.3799, lp_var: 1953699987456.0000, lp_std: 13977.4824\n",
      "Epoch [13/1000], BatchStep[700/6642]\n",
      "Loss: 324.1334, lp_mean: 324.1334, lp_var: 3015193856.0000, lp_std: 549.1078\n",
      "Epoch [13/1000], BatchStep[800/6642]\n",
      "Loss: 15.1192, lp_mean: 15.1192, lp_var: 7919731.0000, lp_std: 28.1420\n",
      "Epoch [13/1000], BatchStep[900/6642]\n",
      "Loss: 66047.2422, lp_mean: 66047.2422, lp_var: 133930183294976.0000, lp_std: 115728.2031\n",
      "Epoch [13/1000], BatchStep[1000/6642]\n",
      "Loss: 1447.6764, lp_mean: 1447.6764, lp_var: 63584022528.0000, lp_std: 2521.5874\n",
      "Epoch [13/1000], BatchStep[1100/6642]\n",
      "Loss: 34249.6133, lp_mean: 34249.6133, lp_var: 33105702289408.0000, lp_std: 57537.5547\n",
      "Epoch [13/1000], BatchStep[1200/6642]\n",
      "Loss: 120.1335, lp_mean: 120.1335, lp_var: 385726016.0000, lp_std: 196.3991\n",
      "Epoch [13/1000], BatchStep[1300/6642]\n",
      "Loss: 368.3653, lp_mean: 368.3653, lp_var: 3678027776.0000, lp_std: 606.4674\n",
      "Epoch [13/1000], BatchStep[1400/6642]\n",
      "Loss: 93711.1719, lp_mean: 93711.1719, lp_var: 247073161084928.0000, lp_std: 157185.6094\n",
      "Epoch [13/1000], BatchStep[1500/6642]\n",
      "Loss: 12474.2012, lp_mean: 12474.2012, lp_var: 4289533575168.0000, lp_std: 20711.1895\n",
      "Epoch [13/1000], BatchStep[1600/6642]\n",
      "Loss: 202355.3281, lp_mean: 202355.3281, lp_var: 1193228485263360.0000, lp_std: 345431.4062\n",
      "Epoch [13/1000], BatchStep[1700/6642]\n",
      "Loss: 14095.0273, lp_mean: 14095.0273, lp_var: 5565345431552.0000, lp_std: 23590.9844\n",
      "Epoch [13/1000], BatchStep[1800/6642]\n",
      "Loss: 324.3102, lp_mean: 324.3102, lp_var: 2088335232.0000, lp_std: 456.9831\n",
      "Epoch [13/1000], BatchStep[1900/6642]\n",
      "Loss: 0.5834, lp_mean: 0.5834, lp_var: 7850.5190, lp_std: 0.8860\n",
      "Epoch [13/1000], BatchStep[2000/6642]\n",
      "Loss: 513.9845, lp_mean: 513.9845, lp_var: 6548441600.0000, lp_std: 809.2244\n",
      "Epoch [13/1000], BatchStep[2100/6642]\n",
      "Loss: 156655.9375, lp_mean: 156655.9375, lp_var: 680943576678400.0000, lp_std: 260948.9688\n",
      "Epoch [13/1000], BatchStep[2200/6642]\n",
      "Loss: 162.5399, lp_mean: 162.5399, lp_var: 722909696.0000, lp_std: 268.8698\n",
      "Epoch [13/1000], BatchStep[2300/6642]\n",
      "Loss: 25694.7812, lp_mean: 25694.7812, lp_var: 18426787528704.0000, lp_std: 42926.4336\n",
      "Epoch [13/1000], BatchStep[2400/6642]\n",
      "Loss: 4729.9585, lp_mean: 4729.9585, lp_var: 636194848768.0000, lp_std: 7976.1826\n",
      "Epoch [13/1000], BatchStep[2500/6642]\n",
      "Loss: 3452.8132, lp_mean: 3452.8132, lp_var: 324051075072.0000, lp_std: 5692.5483\n",
      "Epoch [13/1000], BatchStep[2600/6642]\n",
      "Loss: 91867.1719, lp_mean: 91867.1719, lp_var: 239706268762112.0000, lp_std: 154824.5000\n",
      "Epoch [13/1000], BatchStep[2700/6642]\n",
      "Loss: 1989.8280, lp_mean: 1989.8280, lp_var: 110647296000.0000, lp_std: 3326.3689\n",
      "Epoch [13/1000], BatchStep[2800/6642]\n",
      "Loss: 23.0180, lp_mean: 23.0180, lp_var: 16309802.0000, lp_std: 40.3854\n",
      "Epoch [13/1000], BatchStep[2900/6642]\n",
      "Loss: 408.9752, lp_mean: 408.9752, lp_var: 4886479872.0000, lp_std: 699.0336\n",
      "Epoch [13/1000], BatchStep[3000/6642]\n",
      "Loss: 19633.2969, lp_mean: 19633.2969, lp_var: 10556793683968.0000, lp_std: 32491.2188\n",
      "Epoch [13/1000], BatchStep[3100/6642]\n",
      "Loss: 4643.4175, lp_mean: 4643.4175, lp_var: 608731725824.0000, lp_std: 7802.1265\n",
      "Epoch [13/1000], BatchStep[3200/6642]\n",
      "Loss: 305334.1250, lp_mean: 305334.1250, lp_var: 2532405864824832.0000, lp_std: 503230.1875\n",
      "Epoch [13/1000], BatchStep[3300/6642]\n",
      "Loss: 49.1637, lp_mean: 49.1637, lp_var: 69323168.0000, lp_std: 83.2605\n",
      "Epoch [13/1000], BatchStep[3400/6642]\n",
      "Loss: 2106.6067, lp_mean: 2106.6067, lp_var: 127013855232.0000, lp_std: 3563.9006\n",
      "Epoch [13/1000], BatchStep[3500/6642]\n",
      "Loss: 0.4748, lp_mean: 0.4748, lp_var: 6754.6284, lp_std: 0.8219\n",
      "Epoch [13/1000], BatchStep[3600/6642]\n",
      "Loss: 0.0041, lp_mean: 0.0041, lp_var: 0.1317, lp_std: 0.0036\n",
      "Epoch [13/1000], BatchStep[3700/6642]\n",
      "Loss: 376.9519, lp_mean: 376.9519, lp_var: 3854952960.0000, lp_std: 620.8827\n",
      "Epoch [13/1000], BatchStep[3800/6642]\n",
      "Loss: 1291.5021, lp_mean: 1291.5021, lp_var: 39991271424.0000, lp_std: 1999.7817\n",
      "Epoch [13/1000], BatchStep[3900/6642]\n",
      "Loss: 115873.2578, lp_mean: 115873.2578, lp_var: 362781962403840.0000, lp_std: 190468.3594\n",
      "Epoch [13/1000], BatchStep[4000/6642]\n",
      "Loss: 1.1638, lp_mean: 1.1638, lp_var: 19674.9297, lp_std: 1.4027\n",
      "Epoch [13/1000], BatchStep[4100/6642]\n",
      "Loss: 326.2198, lp_mean: 326.2198, lp_var: 2998378496.0000, lp_std: 547.5745\n",
      "Epoch [13/1000], BatchStep[4200/6642]\n",
      "Loss: 92336.7422, lp_mean: 92336.7422, lp_var: 233120641056768.0000, lp_std: 152682.8906\n",
      "Epoch [13/1000], BatchStep[4300/6642]\n",
      "Loss: 4.9202, lp_mean: 4.9202, lp_var: 828274.2500, lp_std: 9.1010\n",
      "Epoch [13/1000], BatchStep[4400/6642]\n",
      "Loss: 23183.0781, lp_mean: 23183.0781, lp_var: 15031825596416.0000, lp_std: 38770.8984\n",
      "Epoch [13/1000], BatchStep[4500/6642]\n",
      "Loss: 38271.0977, lp_mean: 38271.0977, lp_var: 41279564021760.0000, lp_std: 64249.1758\n",
      "Epoch [13/1000], BatchStep[4600/6642]\n",
      "Loss: 1561.3564, lp_mean: 1561.3564, lp_var: 69029117952.0000, lp_std: 2627.3394\n",
      "Epoch [13/1000], BatchStep[4700/6642]\n",
      "Loss: 11.3983, lp_mean: 11.3983, lp_var: 4317854.0000, lp_std: 20.7794\n",
      "Epoch [13/1000], BatchStep[4800/6642]\n",
      "Loss: 4299.3140, lp_mean: 4299.3140, lp_var: 523914215424.0000, lp_std: 7238.1924\n",
      "Epoch [13/1000], BatchStep[4900/6642]\n",
      "Loss: 160011.8281, lp_mean: 160011.8281, lp_var: 726692830117888.0000, lp_std: 269572.4062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/1000], BatchStep[5000/6642]\n",
      "Loss: 673254.8750, lp_mean: 673254.8750, lp_var: 12837631477940224.0000, lp_std: 1133032.7500\n",
      "Epoch [13/1000], BatchStep[5100/6642]\n",
      "Loss: 43707.8438, lp_mean: 43707.8438, lp_var: 54448181215232.0000, lp_std: 73789.0078\n",
      "Epoch [13/1000], BatchStep[5200/6642]\n",
      "Loss: 0.7083, lp_mean: 0.7083, lp_var: 6094.1431, lp_std: 0.7806\n",
      "Epoch [13/1000], BatchStep[5300/6642]\n",
      "Loss: 0.0104, lp_mean: 0.0104, lp_var: 6.4299, lp_std: 0.0254\n",
      "Epoch [13/1000], BatchStep[5400/6642]\n",
      "Loss: 971.7686, lp_mean: 971.7686, lp_var: 24708128768.0000, lp_std: 1571.8820\n",
      "Epoch [13/1000], BatchStep[5500/6642]\n",
      "Loss: 46106.1680, lp_mean: 46106.1680, lp_var: 59818308009984.0000, lp_std: 77342.2969\n",
      "Epoch [13/1000], BatchStep[5600/6642]\n",
      "Loss: 90.8568, lp_mean: 90.8568, lp_var: 248045072.0000, lp_std: 157.4945\n",
      "Epoch [13/1000], BatchStep[5700/6642]\n",
      "Loss: 475.3900, lp_mean: 475.3900, lp_var: 5670690816.0000, lp_std: 753.0399\n",
      "Epoch [13/1000], BatchStep[5800/6642]\n",
      "Loss: 1170.3671, lp_mean: 1170.3671, lp_var: 40116822016.0000, lp_std: 2002.9185\n",
      "Epoch [13/1000], BatchStep[5900/6642]\n",
      "Loss: 1097.8865, lp_mean: 1097.8865, lp_var: 35634466816.0000, lp_std: 1887.7094\n",
      "Epoch [13/1000], BatchStep[6000/6642]\n",
      "Loss: 888.6945, lp_mean: 888.6945, lp_var: 23453894656.0000, lp_std: 1531.4664\n",
      "Epoch [13/1000], BatchStep[6100/6642]\n",
      "Loss: 4764.4863, lp_mean: 4764.4863, lp_var: 610119909376.0000, lp_std: 7811.0171\n",
      "Epoch [13/1000], BatchStep[6200/6642]\n",
      "Loss: 456.2781, lp_mean: 456.2781, lp_var: 6547441152.0000, lp_std: 809.1626\n",
      "Epoch [13/1000], BatchStep[6300/6642]\n",
      "Loss: 3797.0891, lp_mean: 3797.0891, lp_var: 406166175744.0000, lp_std: 6373.1167\n",
      "Epoch [13/1000], BatchStep[6400/6642]\n",
      "Loss: 0.0000, lp_mean: 0.0000, lp_var: 0.0000, lp_std: 0.0000\n",
      "Epoch [13/1000], BatchStep[6500/6642]\n",
      "Loss: 48.8685, lp_mean: 48.8685, lp_var: 69002240.0000, lp_std: 83.0676\n",
      "Epoch [13/1000], BatchStep[6600/6642]\n",
      "Loss: 0.0058, lp_mean: 0.0058, lp_var: 0.1542, lp_std: 0.0039\n",
      "Epoch [13/1000]\n",
      "Train Loss : 544.904602, Test Loss: 9279478784.0000\n",
      "Epoch [14/1000], BatchStep[100/6642]\n",
      "Loss: 55.5235, lp_mean: 55.5235, lp_var: 100430008.0000, lp_std: 100.2148\n",
      "Epoch [14/1000], BatchStep[200/6642]\n",
      "Loss: 27684.6211, lp_mean: 27684.6211, lp_var: 21998642135040.0000, lp_std: 46902.7109\n",
      "Epoch [14/1000], BatchStep[300/6642]\n",
      "Loss: 1.5806, lp_mean: 1.5806, lp_var: 103255.7969, lp_std: 3.2133\n",
      "Epoch [14/1000], BatchStep[400/6642]\n",
      "Loss: 56598.5156, lp_mean: 56598.5156, lp_var: 89097435611136.0000, lp_std: 94391.4375\n",
      "Epoch [14/1000], BatchStep[500/6642]\n",
      "Loss: 1826.1803, lp_mean: 1826.1803, lp_var: 91449475072.0000, lp_std: 3024.0615\n",
      "Epoch [14/1000], BatchStep[600/6642]\n",
      "Loss: 231232.0938, lp_mean: 231232.0938, lp_var: 1499945118990336.0000, lp_std: 387291.2500\n",
      "Epoch [14/1000], BatchStep[700/6642]\n",
      "Loss: 7109.8887, lp_mean: 7109.8887, lp_var: 1392797810688.0000, lp_std: 11801.6855\n",
      "Epoch [14/1000], BatchStep[800/6642]\n",
      "Loss: 0.2866, lp_mean: 0.2866, lp_var: 2594.2747, lp_std: 0.5093\n",
      "Epoch [14/1000], BatchStep[900/6642]\n",
      "Loss: 455.9608, lp_mean: 455.9608, lp_var: 5821312000.0000, lp_std: 762.9752\n",
      "Epoch [14/1000], BatchStep[1000/6642]\n",
      "Loss: 10036.7002, lp_mean: 10036.7002, lp_var: 2766642872320.0000, lp_std: 16633.2285\n",
      "Epoch [14/1000], BatchStep[1100/6642]\n",
      "Loss: 12211.2637, lp_mean: 12211.2637, lp_var: 3845278400512.0000, lp_std: 19609.3828\n",
      "Epoch [14/1000], BatchStep[1200/6642]\n",
      "Loss: 5279.8389, lp_mean: 5279.8389, lp_var: 695678271488.0000, lp_std: 8340.7334\n",
      "Epoch [14/1000], BatchStep[1300/6642]\n",
      "Loss: 53861.9453, lp_mean: 53861.9453, lp_var: 86889713369088.0000, lp_std: 93214.6484\n",
      "Epoch [14/1000], BatchStep[1400/6642]\n",
      "Loss: 3861.2578, lp_mean: 3861.2578, lp_var: 415138217984.0000, lp_std: 6443.1221\n",
      "Epoch [14/1000], BatchStep[1500/6642]\n",
      "Loss: 964.2009, lp_mean: 964.2009, lp_var: 24389339136.0000, lp_std: 1561.7086\n",
      "Epoch [14/1000], BatchStep[1600/6642]\n",
      "Loss: 11755.6455, lp_mean: 11755.6455, lp_var: 3892251721728.0000, lp_std: 19728.7910\n",
      "Epoch [14/1000], BatchStep[1700/6642]\n",
      "Loss: 426.6638, lp_mean: 426.6638, lp_var: 4920024576.0000, lp_std: 701.4289\n",
      "Epoch [14/1000], BatchStep[1800/6642]\n",
      "Loss: 334.0722, lp_mean: 334.0722, lp_var: 3392528896.0000, lp_std: 582.4542\n",
      "Epoch [14/1000], BatchStep[1900/6642]\n",
      "Loss: 482.0105, lp_mean: 482.0105, lp_var: 6603489792.0000, lp_std: 812.6186\n",
      "Epoch [14/1000], BatchStep[2000/6642]\n",
      "Loss: 4.9215, lp_mean: 4.9215, lp_var: 949494.0000, lp_std: 9.7442\n",
      "Epoch [14/1000], BatchStep[2100/6642]\n",
      "Loss: 44997.5664, lp_mean: 44997.5664, lp_var: 54521015304192.0000, lp_std: 73838.3438\n",
      "Epoch [14/1000], BatchStep[2200/6642]\n",
      "Loss: 67170.3281, lp_mean: 67170.3281, lp_var: 123897156468736.0000, lp_std: 111309.0938\n",
      "Epoch [14/1000], BatchStep[2300/6642]\n",
      "Loss: 5.8769, lp_mean: 5.8769, lp_var: 946398.6250, lp_std: 9.7283\n",
      "Epoch [14/1000], BatchStep[2400/6642]\n",
      "Loss: 44514.3438, lp_mean: 44514.3438, lp_var: 54818030747648.0000, lp_std: 74039.1953\n",
      "Epoch [14/1000], BatchStep[2500/6642]\n",
      "Loss: 0.0000, lp_mean: 0.0000, lp_var: 0.0000, lp_std: 0.0000\n",
      "Epoch [14/1000], BatchStep[2600/6642]\n",
      "Loss: 14034.5430, lp_mean: 14034.5430, lp_var: 4963886956544.0000, lp_std: 22279.7832\n",
      "Epoch [14/1000], BatchStep[2700/6642]\n",
      "Loss: 15780.9121, lp_mean: 15780.9121, lp_var: 7026575933440.0000, lp_std: 26507.6895\n",
      "Epoch [14/1000], BatchStep[2800/6642]\n",
      "Loss: 1846.0187, lp_mean: 1846.0187, lp_var: 95806275584.0000, lp_std: 3095.2590\n",
      "Epoch [14/1000], BatchStep[2900/6642]\n",
      "Loss: 8061.4614, lp_mean: 8061.4614, lp_var: 1824024952832.0000, lp_std: 13505.6465\n",
      "Epoch [14/1000], BatchStep[3000/6642]\n",
      "Loss: 36212.5664, lp_mean: 36212.5664, lp_var: 37415389495296.0000, lp_std: 61168.1250\n",
      "Epoch [14/1000], BatchStep[3100/6642]\n",
      "Loss: 102.7273, lp_mean: 102.7273, lp_var: 294389856.0000, lp_std: 171.5779\n",
      "Epoch [14/1000], BatchStep[3200/6642]\n",
      "Loss: 1107.1517, lp_mean: 1107.1517, lp_var: 34742390784.0000, lp_std: 1863.9312\n",
      "Epoch [14/1000], BatchStep[3300/6642]\n",
      "Loss: 169457.2031, lp_mean: 169457.2031, lp_var: 803611869184000.0000, lp_std: 283480.4688\n",
      "Epoch [14/1000], BatchStep[3400/6642]\n",
      "Loss: 70.8377, lp_mean: 70.8377, lp_var: 162463696.0000, lp_std: 127.4613\n",
      "Epoch [14/1000], BatchStep[3500/6642]\n",
      "Loss: 905.3728, lp_mean: 905.3728, lp_var: 24159563776.0000, lp_std: 1554.3347\n",
      "Epoch [14/1000], BatchStep[3600/6642]\n",
      "Loss: 10.9823, lp_mean: 10.9823, lp_var: 2291731.2500, lp_std: 15.1385\n",
      "Epoch [14/1000], BatchStep[3700/6642]\n",
      "Loss: 59.5162, lp_mean: 59.5162, lp_var: 106763080.0000, lp_std: 103.3262\n",
      "Epoch [14/1000], BatchStep[3800/6642]\n",
      "Loss: 0.2206, lp_mean: 0.2206, lp_var: 1737.1067, lp_std: 0.4168\n",
      "Epoch [14/1000], BatchStep[3900/6642]\n",
      "Loss: 45283.8086, lp_mean: 45283.8086, lp_var: 57282918350848.0000, lp_std: 75685.4844\n",
      "Epoch [14/1000], BatchStep[4000/6642]\n",
      "Loss: 0.0003, lp_mean: 0.0003, lp_var: 0.0016, lp_std: 0.0004\n",
      "Epoch [14/1000], BatchStep[4100/6642]\n",
      "Loss: 270.6231, lp_mean: 270.6231, lp_var: 2171511040.0000, lp_std: 465.9948\n",
      "Epoch [14/1000], BatchStep[4200/6642]\n",
      "Loss: 0.1559, lp_mean: 0.1559, lp_var: 205.2843, lp_std: 0.1433\n",
      "Epoch [14/1000], BatchStep[4300/6642]\n",
      "Loss: 9844.0713, lp_mean: 9844.0713, lp_var: 2818241986560.0000, lp_std: 16787.6191\n",
      "Epoch [14/1000], BatchStep[4400/6642]\n",
      "Loss: 0.0186, lp_mean: 0.0186, lp_var: 10.8558, lp_std: 0.0329\n",
      "Epoch [14/1000], BatchStep[4500/6642]\n",
      "Loss: 0.0000, lp_mean: 0.0000, lp_var: 0.0000, lp_std: 0.0000\n",
      "Epoch [14/1000], BatchStep[4600/6642]\n",
      "Loss: 232.7287, lp_mean: 232.7287, lp_var: 1549192960.0000, lp_std: 393.5979\n",
      "Epoch [14/1000], BatchStep[4700/6642]\n",
      "Loss: 27567.9316, lp_mean: 27567.9316, lp_var: 21377815937024.0000, lp_std: 46236.1484\n",
      "Epoch [14/1000], BatchStep[4800/6642]\n",
      "Loss: 735293.7500, lp_mean: 735293.7500, lp_var: 15383391738265600.0000, lp_std: 1240298.0000\n",
      "Epoch [14/1000], BatchStep[4900/6642]\n",
      "Loss: 609.4771, lp_mean: 609.4771, lp_var: 11456113664.0000, lp_std: 1070.3323\n",
      "Epoch [14/1000], BatchStep[5000/6642]\n",
      "Loss: 75.6739, lp_mean: 75.6739, lp_var: 163920704.0000, lp_std: 128.0315\n",
      "Epoch [14/1000], BatchStep[5100/6642]\n",
      "Loss: 2971.3660, lp_mean: 2971.3660, lp_var: 242987073536.0000, lp_std: 4929.3716\n",
      "Epoch [14/1000], BatchStep[5200/6642]\n",
      "Loss: 0.0057, lp_mean: 0.0057, lp_var: 2.7755, lp_std: 0.0167\n",
      "Epoch [14/1000], BatchStep[5300/6642]\n",
      "Loss: 16.3480, lp_mean: 16.3480, lp_var: 7201516.5000, lp_std: 26.8356\n",
      "Epoch [14/1000], BatchStep[5400/6642]\n",
      "Loss: 30122.8750, lp_mean: 30122.8750, lp_var: 25795231744000.0000, lp_std: 50789.0078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/1000], BatchStep[5500/6642]\n",
      "Loss: 5112.5674, lp_mean: 5112.5674, lp_var: 783017050112.0000, lp_std: 8848.8252\n",
      "Epoch [14/1000], BatchStep[5600/6642]\n",
      "Loss: 160.7090, lp_mean: 160.7090, lp_var: 725743296.0000, lp_std: 269.3962\n",
      "Epoch [14/1000], BatchStep[5700/6642]\n",
      "Loss: 10275.0928, lp_mean: 10275.0928, lp_var: 2901519630336.0000, lp_std: 17033.8477\n",
      "Epoch [14/1000], BatchStep[5800/6642]\n",
      "Loss: 9563.0635, lp_mean: 9563.0635, lp_var: 2380090834944.0000, lp_std: 15427.5430\n",
      "Epoch [14/1000], BatchStep[5900/6642]\n",
      "Loss: 1958.0146, lp_mean: 1958.0146, lp_var: 112597450752.0000, lp_std: 3355.5544\n",
      "Epoch [14/1000], BatchStep[6000/6642]\n",
      "Loss: 366.9285, lp_mean: 366.9285, lp_var: 3832185088.0000, lp_std: 619.0464\n",
      "Epoch [14/1000], BatchStep[6100/6642]\n",
      "Loss: 189.4476, lp_mean: 189.4476, lp_var: 791262080.0000, lp_std: 281.2938\n",
      "Epoch [14/1000], BatchStep[6200/6642]\n",
      "Loss: 2.5895, lp_mean: 2.5895, lp_var: 168256.5000, lp_std: 4.1019\n",
      "Epoch [14/1000], BatchStep[6300/6642]\n",
      "Loss: 33.3402, lp_mean: 33.3402, lp_var: 28591838.0000, lp_std: 53.4713\n",
      "Epoch [14/1000], BatchStep[6400/6642]\n",
      "Loss: 1258.0214, lp_mean: 1258.0214, lp_var: 42761609216.0000, lp_std: 2067.8879\n",
      "Epoch [14/1000], BatchStep[6500/6642]\n",
      "Loss: 34.2279, lp_mean: 34.2279, lp_var: 38728296.0000, lp_std: 62.2321\n",
      "Epoch [14/1000], BatchStep[6600/6642]\n",
      "Loss: 3008.1890, lp_mean: 3008.1890, lp_var: 250463059968.0000, lp_std: 5004.6284\n",
      "Epoch [14/1000]\n",
      "Train Loss : 589.106567, Test Loss: 10538180608.0000\n",
      "Epoch [15/1000], BatchStep[100/6642]\n",
      "Loss: 12630.2842, lp_mean: 12630.2842, lp_var: 4455489077248.0000, lp_std: 21108.0293\n",
      "Epoch [15/1000], BatchStep[200/6642]\n",
      "Loss: 22591.6914, lp_mean: 22591.6914, lp_var: 14303692324864.0000, lp_std: 37820.2227\n",
      "Epoch [15/1000], BatchStep[300/6642]\n",
      "Loss: 7034.9678, lp_mean: 7034.9678, lp_var: 1391833776128.0000, lp_std: 11797.6006\n",
      "Epoch [15/1000], BatchStep[400/6642]\n",
      "Loss: 13.8690, lp_mean: 13.8690, lp_var: 5526111.5000, lp_std: 23.5077\n",
      "Epoch [15/1000], BatchStep[500/6642]\n",
      "Loss: 1643.6899, lp_mean: 1643.6899, lp_var: 84637999104.0000, lp_std: 2909.2610\n",
      "Epoch [15/1000], BatchStep[600/6642]\n",
      "Loss: 12043.3887, lp_mean: 12043.3887, lp_var: 4023070490624.0000, lp_std: 20057.5938\n",
      "Epoch [15/1000], BatchStep[700/6642]\n",
      "Loss: 1053.7537, lp_mean: 1053.7537, lp_var: 17208776704.0000, lp_std: 1311.8223\n",
      "Epoch [15/1000], BatchStep[800/6642]\n",
      "Loss: 1881.2004, lp_mean: 1881.2004, lp_var: 99247128576.0000, lp_std: 3150.3511\n",
      "Epoch [15/1000], BatchStep[900/6642]\n",
      "Loss: 182.0869, lp_mean: 182.0869, lp_var: 870679872.0000, lp_std: 295.0728\n",
      "Epoch [15/1000], BatchStep[1000/6642]\n",
      "Loss: 20157.0684, lp_mean: 20157.0684, lp_var: 11125494120448.0000, lp_std: 33354.9023\n",
      "Epoch [15/1000], BatchStep[1100/6642]\n",
      "Loss: 192.0303, lp_mean: 192.0303, lp_var: 1016372416.0000, lp_std: 318.8060\n",
      "Epoch [15/1000], BatchStep[1200/6642]\n",
      "Loss: 8532.1045, lp_mean: 8532.1045, lp_var: 2034329059328.0000, lp_std: 14262.9912\n",
      "Epoch [15/1000], BatchStep[1300/6642]\n",
      "Loss: 182.8347, lp_mean: 182.8347, lp_var: 898712704.0000, lp_std: 299.7854\n",
      "Epoch [15/1000], BatchStep[1400/6642]\n",
      "Loss: 2.3755, lp_mean: 2.3755, lp_var: 183253.4062, lp_std: 4.2808\n",
      "Epoch [15/1000], BatchStep[1500/6642]\n",
      "Loss: 2.1479, lp_mean: 2.1479, lp_var: 149631.3906, lp_std: 3.8682\n",
      "Epoch [15/1000], BatchStep[1600/6642]\n",
      "Loss: 89.9327, lp_mean: 89.9327, lp_var: 265975984.0000, lp_std: 163.0877\n",
      "Epoch [15/1000], BatchStep[1700/6642]\n",
      "Loss: 1056.0018, lp_mean: 1056.0018, lp_var: 31752282112.0000, lp_std: 1781.9169\n",
      "Epoch [15/1000], BatchStep[1800/6642]\n",
      "Loss: 0.0448, lp_mean: 0.0448, lp_var: 62.4576, lp_std: 0.0790\n",
      "Epoch [15/1000], BatchStep[1900/6642]\n",
      "Loss: 2119.0317, lp_mean: 2119.0317, lp_var: 122288758784.0000, lp_std: 3496.9810\n",
      "Epoch [15/1000], BatchStep[2000/6642]\n",
      "Loss: 32131.8164, lp_mean: 32131.8164, lp_var: 28225549893632.0000, lp_std: 53127.7227\n",
      "Epoch [15/1000], BatchStep[2100/6642]\n",
      "Loss: 103438.4062, lp_mean: 103438.4062, lp_var: 301989351129088.0000, lp_std: 173778.4062\n",
      "Epoch [15/1000], BatchStep[2200/6642]\n",
      "Loss: 5233.1626, lp_mean: 5233.1626, lp_var: 607838339072.0000, lp_std: 7796.3984\n",
      "Epoch [15/1000], BatchStep[2300/6642]\n",
      "Loss: 600.7294, lp_mean: 600.7294, lp_var: 10999563264.0000, lp_std: 1048.7880\n",
      "Epoch [15/1000], BatchStep[2400/6642]\n",
      "Loss: 103108.1172, lp_mean: 103108.1172, lp_var: 296502463299584.0000, lp_std: 172192.4688\n",
      "Epoch [15/1000], BatchStep[2500/6642]\n",
      "Loss: 12529.7334, lp_mean: 12529.7334, lp_var: 4408294244352.0000, lp_std: 20995.9375\n",
      "Epoch [15/1000], BatchStep[2600/6642]\n",
      "Loss: 5.9680, lp_mean: 5.9680, lp_var: 853038.6875, lp_std: 9.2360\n",
      "Epoch [15/1000], BatchStep[2700/6642]\n",
      "Loss: 62901.6289, lp_mean: 62901.6289, lp_var: 111115828723712.0000, lp_std: 105411.5000\n",
      "Epoch [15/1000], BatchStep[2800/6642]\n",
      "Loss: 294.4660, lp_mean: 294.4660, lp_var: 2419577856.0000, lp_std: 491.8920\n",
      "Epoch [15/1000], BatchStep[2900/6642]\n",
      "Loss: 1035.2056, lp_mean: 1035.2056, lp_var: 29881083904.0000, lp_std: 1728.6146\n",
      "Epoch [15/1000], BatchStep[3000/6642]\n",
      "Loss: 19134.6387, lp_mean: 19134.6387, lp_var: 11202057994240.0000, lp_std: 33469.4766\n",
      "Epoch [15/1000], BatchStep[3100/6642]\n",
      "Loss: 6992.5234, lp_mean: 6992.5234, lp_var: 1345348567040.0000, lp_std: 11598.9160\n",
      "Epoch [15/1000], BatchStep[3200/6642]\n",
      "Loss: 102.4276, lp_mean: 102.4276, lp_var: 296323392.0000, lp_std: 172.1405\n",
      "Epoch [15/1000], BatchStep[3300/6642]\n",
      "Loss: 0.0000, lp_mean: 0.0000, lp_var: 0.0000, lp_std: 0.0000\n",
      "Epoch [15/1000], BatchStep[3400/6642]\n",
      "Loss: 177.7249, lp_mean: 177.7249, lp_var: 818860032.0000, lp_std: 286.1573\n",
      "Epoch [15/1000], BatchStep[3500/6642]\n",
      "Loss: 197906.9531, lp_mean: 197906.9531, lp_var: 1112049778163712.0000, lp_std: 333474.0938\n",
      "Epoch [15/1000], BatchStep[3600/6642]\n",
      "Loss: 76731.6094, lp_mean: 76731.6094, lp_var: 165086178050048.0000, lp_std: 128485.8750\n",
      "Epoch [15/1000], BatchStep[3700/6642]\n",
      "Loss: 111401.1250, lp_mean: 111401.1250, lp_var: 342353286004736.0000, lp_std: 185027.9219\n",
      "Epoch [15/1000], BatchStep[3800/6642]\n",
      "Loss: 1348.4481, lp_mean: 1348.4481, lp_var: 42306174976.0000, lp_std: 2056.8467\n",
      "Epoch [15/1000], BatchStep[3900/6642]\n",
      "Loss: 18.8072, lp_mean: 18.8072, lp_var: 9166828.0000, lp_std: 30.2768\n",
      "Epoch [15/1000], BatchStep[4000/6642]\n",
      "Loss: 2004.1597, lp_mean: 2004.1597, lp_var: 106169819136.0000, lp_std: 3258.3711\n",
      "Epoch [15/1000], BatchStep[4100/6642]\n",
      "Loss: 825.8046, lp_mean: 825.8046, lp_var: 18996180992.0000, lp_std: 1378.2664\n",
      "Epoch [15/1000], BatchStep[4200/6642]\n",
      "Loss: 1196.5441, lp_mean: 1196.5441, lp_var: 40576200704.0000, lp_std: 2014.3536\n",
      "Epoch [15/1000], BatchStep[4300/6642]\n",
      "Loss: 49.3160, lp_mean: 49.3160, lp_var: 76950096.0000, lp_std: 87.7212\n",
      "Epoch [15/1000], BatchStep[4400/6642]\n",
      "Loss: 11613.2051, lp_mean: 11613.2051, lp_var: 3800018452480.0000, lp_std: 19493.6367\n",
      "Epoch [15/1000], BatchStep[4500/6642]\n",
      "Loss: 15.9508, lp_mean: 15.9508, lp_var: 5505931.0000, lp_std: 23.4647\n",
      "Epoch [15/1000], BatchStep[4600/6642]\n",
      "Loss: 544.4377, lp_mean: 544.4377, lp_var: 8618475520.0000, lp_std: 928.3575\n",
      "Epoch [15/1000], BatchStep[4700/6642]\n",
      "Loss: 1307.0508, lp_mean: 1307.0508, lp_var: 48979128320.0000, lp_std: 2213.1228\n",
      "Epoch [15/1000], BatchStep[4800/6642]\n",
      "Loss: 6312.1494, lp_mean: 6312.1494, lp_var: 1135905210368.0000, lp_std: 10657.8857\n",
      "Epoch [15/1000], BatchStep[4900/6642]\n",
      "Loss: 4108.5195, lp_mean: 4108.5195, lp_var: 484102275072.0000, lp_std: 6957.7456\n",
      "Epoch [15/1000], BatchStep[5000/6642]\n",
      "Loss: 77313.0078, lp_mean: 77313.0078, lp_var: 165960153563136.0000, lp_std: 128825.5156\n",
      "Epoch [15/1000], BatchStep[5100/6642]\n",
      "Loss: 67.6322, lp_mean: 67.6322, lp_var: 151092608.0000, lp_std: 122.9197\n",
      "Epoch [15/1000], BatchStep[5200/6642]\n",
      "Loss: 956.6514, lp_mean: 956.6514, lp_var: 29987393536.0000, lp_std: 1731.6870\n",
      "Epoch [15/1000], BatchStep[5300/6642]\n",
      "Loss: 2774.8088, lp_mean: 2774.8088, lp_var: 207457206272.0000, lp_std: 4554.7471\n",
      "Epoch [15/1000], BatchStep[5400/6642]\n",
      "Loss: 186.2132, lp_mean: 186.2132, lp_var: 1013624640.0000, lp_std: 318.3747\n",
      "Epoch [15/1000], BatchStep[5500/6642]\n",
      "Loss: 973.8801, lp_mean: 973.8801, lp_var: 27486779392.0000, lp_std: 1657.9138\n",
      "Epoch [15/1000], BatchStep[5600/6642]\n",
      "Loss: 149.9718, lp_mean: 149.9718, lp_var: 623508096.0000, lp_std: 249.7014\n",
      "Epoch [15/1000], BatchStep[5700/6642]\n",
      "Loss: 125.5290, lp_mean: 125.5290, lp_var: 436473376.0000, lp_std: 208.9194\n",
      "Epoch [15/1000], BatchStep[5800/6642]\n",
      "Loss: 45680.7383, lp_mean: 45680.7383, lp_var: 60245548204032.0000, lp_std: 77618.0078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/1000], BatchStep[5900/6642]\n",
      "Loss: 3335.5015, lp_mean: 3335.5015, lp_var: 319773966336.0000, lp_std: 5654.8560\n",
      "Epoch [15/1000], BatchStep[6000/6642]\n",
      "Loss: 166.6924, lp_mean: 166.6924, lp_var: 796579776.0000, lp_std: 282.2375\n",
      "Epoch [15/1000], BatchStep[6100/6642]\n",
      "Loss: 7262.3828, lp_mean: 7262.3828, lp_var: 1503607652352.0000, lp_std: 12262.1680\n",
      "Epoch [15/1000], BatchStep[6200/6642]\n",
      "Loss: 2.5482, lp_mean: 2.5482, lp_var: 204497.7656, lp_std: 4.5221\n",
      "Epoch [15/1000], BatchStep[6300/6642]\n",
      "Loss: 335.4832, lp_mean: 335.4832, lp_var: 3214754048.0000, lp_std: 566.9880\n",
      "Epoch [15/1000], BatchStep[6400/6642]\n",
      "Loss: 220117.4688, lp_mean: 220117.4688, lp_var: 1333279315722240.0000, lp_std: 365141.0000\n",
      "Epoch [15/1000], BatchStep[6500/6642]\n",
      "Loss: 139488.0469, lp_mean: 139488.0469, lp_var: 551861220278272.0000, lp_std: 234917.2500\n",
      "Epoch [15/1000], BatchStep[6600/6642]\n",
      "Loss: 18631.6641, lp_mean: 18631.6641, lp_var: 9727811518464.0000, lp_std: 31189.4375\n",
      "Epoch [15/1000]\n",
      "Train Loss : 585.785583, Test Loss: 9315766272.0000\n",
      "Epoch [16/1000], BatchStep[100/6642]\n",
      "Loss: 1399.9623, lp_mean: 1399.9623, lp_var: 55540772864.0000, lp_std: 2356.7090\n",
      "Epoch [16/1000], BatchStep[200/6642]\n",
      "Loss: 0.1619, lp_mean: 0.1619, lp_var: 542.9017, lp_std: 0.2330\n",
      "Epoch [16/1000], BatchStep[300/6642]\n",
      "Loss: 7278.6675, lp_mean: 7278.6675, lp_var: 1521793892352.0000, lp_std: 12336.1016\n",
      "Epoch [16/1000], BatchStep[400/6642]\n",
      "Loss: 18856.0527, lp_mean: 18856.0527, lp_var: 9647447605248.0000, lp_std: 31060.3379\n",
      "Epoch [16/1000], BatchStep[500/6642]\n",
      "Loss: 29012.9277, lp_mean: 29012.9277, lp_var: 24328550744064.0000, lp_std: 49323.9805\n",
      "Epoch [16/1000], BatchStep[600/6642]\n",
      "Loss: 36375.5156, lp_mean: 36375.5156, lp_var: 37461619113984.0000, lp_std: 61205.8984\n",
      "Epoch [16/1000], BatchStep[700/6642]\n",
      "Loss: 1689.4415, lp_mean: 1689.4415, lp_var: 53775953920.0000, lp_std: 2318.9644\n",
      "Epoch [16/1000], BatchStep[800/6642]\n",
      "Loss: 78847.2109, lp_mean: 78847.2109, lp_var: 170667135729664.0000, lp_std: 130639.6328\n",
      "Epoch [16/1000], BatchStep[900/6642]\n",
      "Loss: 0.0058, lp_mean: 0.0058, lp_var: 0.6747, lp_std: 0.0082\n",
      "Epoch [16/1000], BatchStep[1000/6642]\n",
      "Loss: 60.7980, lp_mean: 60.7980, lp_var: 94319616.0000, lp_std: 97.1183\n",
      "Epoch [16/1000], BatchStep[1100/6642]\n",
      "Loss: 80.5509, lp_mean: 80.5509, lp_var: 159824800.0000, lp_std: 126.4218\n",
      "Epoch [16/1000], BatchStep[1200/6642]\n",
      "Loss: 29570.9609, lp_mean: 29570.9609, lp_var: 23830034644992.0000, lp_std: 48816.0156\n",
      "Epoch [16/1000], BatchStep[1300/6642]\n",
      "Loss: 346.4283, lp_mean: 346.4283, lp_var: 3087289344.0000, lp_std: 555.6339\n",
      "Epoch [16/1000], BatchStep[1400/6642]\n",
      "Loss: 680.2953, lp_mean: 680.2953, lp_var: 13664686080.0000, lp_std: 1168.9606\n",
      "Epoch [16/1000], BatchStep[1500/6642]\n",
      "Loss: 90.5701, lp_mean: 90.5701, lp_var: 224974096.0000, lp_std: 149.9914\n",
      "Epoch [16/1000], BatchStep[1600/6642]\n",
      "Loss: 134.5042, lp_mean: 134.5042, lp_var: 466271776.0000, lp_std: 215.9333\n"
     ]
    }
   ],
   "source": [
    "# dataloader\n",
    "data_loader = data.DataLoader(dataset=dpe_dataset, batch_size=dpenet_batch_size, shuffle=True)\n",
    "test_loader = data.DataLoader(dataset=dpe_testset, batch_size=dpenet_batch_size)\n",
    "\n",
    "# tensorboardX summary\n",
    "summary_writer = SummaryWriter('log')\n",
    "dpenet = DPENet()\n",
    "dpenet.apply(weight_init)\n",
    "\n",
    "# To make graph in summary\n",
    "dummmy = torch.zeros(dpenet_batch_size,5)\n",
    "summary_writer.add_graph(dpenet,dummmy.cuda())\n",
    "\n",
    "# Optimizer\n",
    "#dpenet_optimizer = optim.Adam(dpenet.parameters(), lr=dpenet_learning_rate)\n",
    "#dpenet_optimizer = optim.Adam(dpenet.parameters(), dpenet_learning_rate, [dpenet_beta1, dpenet_beta2])\n",
    "dpenet_optimizer = optim.SGD(dpenet.parameters(), lr=dpenet_learning_rate, momentum=dpenet_momentum)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_train = 0\n",
    "    loss_train_lsit = []\n",
    "    for i, sample in enumerate(data_loader):\n",
    "        \n",
    "        # input : ['ego_vx', 'yaw_rate', 'sas_angle', 'long_acc', 'lat_acc']\n",
    "        # gt : ['ego_global_x', 'ego_global_y']\n",
    "        input_ = sample['in'].cuda()\n",
    "        gt_ = sample['gt']\n",
    "        output_ = dpenet(input_)\n",
    "        #print(output_)\n",
    "        # train dpenet\n",
    "        # reset the gradient for the optimizer\n",
    "        # zero_grad : 역전파 실행 전 변화도 0으로\n",
    "        dpenet_optimizer.zero_grad()\n",
    "        \n",
    "        # loss from lateral position\n",
    "        loss_lp_mean, loss_lp_var, loss_lp_std = lp_loss(output_,gt_)\n",
    "        loss_lp_mean = lambda_mean*loss_lp_mean\n",
    "        loss_lp_var = lambda_var*loss_lp_var\n",
    "        loss_lp_std = lambda_std*loss_lp_std\n",
    "        loss = loss_lp_mean + loss_lp_var + loss_lp_std\n",
    "        loss = Variable(loss_lp_mean,requires_grad=True)\n",
    "        # backpropagate the gradient\n",
    "        loss.backward()\n",
    "        # update the weights using the gradient with the optimizer\n",
    "        dpenet_optimizer.step()\n",
    "        \n",
    "        loss_train_lsit.append(loss/len(input_))\n",
    "        # print the log information\n",
    "        if (i+1) % log_step == 0:\n",
    "            print('Epoch [%d/%d], BatchStep[%d/%d]' % (epoch + 1, num_epochs, i + 1, len(data_loader)))\n",
    "            print('Loss: %.4f, lp_mean: %.4f, lp_var: %.4f, lp_std: %.4f' \n",
    "                  % (loss.data, loss_lp_mean.data, loss_lp_var.data, loss_lp_std.data))\n",
    "\n",
    "    loss_test = 0\n",
    "    loss_test_lsit = []\n",
    "    with torch.no_grad():\n",
    "        for i, sample_t in enumerate(test_loader):\n",
    "            input_t = sample_t['in'].cuda()\n",
    "            gt_t = sample_t['gt']\n",
    "            output_t = dpenet(input_t)\n",
    "            loss_lp_mean_t, loss_lp_var_t, loss_lp_std_t = lp_loss(output_t,gt_t)\n",
    "            loss_test_lsit.append(loss_lp_mean_t/len(input_t))\n",
    "            \n",
    "    loss_test = torch.mean(torch.tensor(loss_test_lsit,requires_grad=False))\n",
    "    loss_train = torch.mean(torch.tensor(loss_train_lsit,requires_grad=False))\n",
    "    \n",
    "    print('Epoch [%d/%d]' % (epoch + 1, num_epochs))\n",
    "    print('Train Loss : %4f, Test Loss: %.4f' % (loss_train.data, loss_test.data))\n",
    "    \n",
    "    # visualize the parameter\n",
    "    for name, param in dpenet.named_parameters():\n",
    "        #summary.add_histogram(name, param, epoch)\n",
    "        summary_writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch, bins='doane')\n",
    "    \n",
    "    # save summary\n",
    "    summary_writer.add_scalar('loss/train', loss_train.item(), epoch)\n",
    "    #summary_writer.add_scalar('loss/loss_lp_mean', loss_lp_mean.item(), epoch)\n",
    "    #summary_writer.add_scalar('loss/loss_lp_var', loss_lp_var.item(), epoch)\n",
    "    #summary_writer.add_scalar('loss/loss_lp_std', loss_lp_std.item(), epoch)\n",
    "    summary_writer.add_scalar('loss/test', loss_test.item(), epoch)\n",
    "    \n",
    "    summary_writer.flush()\n",
    "    # save the model parameters\n",
    "    #if epoch % 10 == 0:\n",
    "    model_path = os.path.join('./DPENet_model', 'DPENet-%d.pkl' % (epoch + 1))\n",
    "    torch.save(dpenet.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gt_.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_test.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_lp_mean, loss_lp_var, loss_lp_std = lp_loss(output_,gt_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_lp_mean.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = Variable(loss_lp_mean,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in dpenet.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(name, param.grad.sum())\n",
    "    else:\n",
    "        print(name, param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in dpenet.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_test/len(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.retain_grad()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_real_deep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_real_deep_wo_decoder\n",
    "Epoch [1/1000] Test Loss: 144666329088.0000\n",
    "Epoch [2/1000] Test Loss: 145073176576.0000\n",
    "Epoch [3/1000] Test Loss: 146446860288.0000\n",
    "Epoch [4/1000] Test Loss: 144554934272.0000\n",
    "Epoch [5/1000] Test Loss: 145652776960.0000\n",
    "Epoch [6/1000] Test Loss: 143921414144.0000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_dropout_wo_loss_scaling\n",
    "epoch [1/1000] Test Loss: 236042076160.0000\n",
    "epoch [2/1000] Test Loss: 238885634048.0000 \n",
    "Epoch [3/1000] Test Loss: 235835916288.0000\n",
    "Epoch [4/1000] Test Loss: 237683245056.0000\n",
    "Epoch [5/1000] Test Loss: 240484368384.0000\n",
    "Epoch [6/1000] Test Loss: 235287920640.0000\n",
    "Epoch [7/1000] Test Loss: 232444411904.0000\n",
    "Epoch [8/1000] Test Loss: 233658007552.0000\n",
    "Epoch [9/1000] Test Loss: 235095965696.0000\n",
    "Epoch [10/1000] Test Loss: 232347713536.0000\n",
    "Epoch [11/1000] Test Loss: 239848275968.0000\n",
    "Epoch [12/1000] Test Loss: 242579668992.0000\n",
    "Epoch [13/1000] Test Loss: 240169140224.0000\n",
    "Epoch [14/1000] Test Loss: 239437922304.0000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
